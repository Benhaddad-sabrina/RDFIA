<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>57a9e6dfe1b34e869d6f171dfdc40cf0</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown" id="VgWb11WT-07r">
<center><h1>1-cd: Convolutional Neural Networks (ConvNets)</h1></center>
<center><h2><a href="https://rdfia.github.io/">Course link</a></h2></center>
<h1 id="warning-">Warning :</h1>
<h1
id="do-file---save-a-copy-in-drive-before-you-start-modifying-the-notebook-otherwise-your-modifications-will-not-be-saved">Do
"File -&gt; Save a copy in Drive" before you start modifying the
notebook, otherwise your modifications will not be saved.</h1>
<p>Done by :</p>
<ul>
<li>BENHADDAD Sabrina - Group 3 - DAC</li>
<li>BENSIDHOUM Azzedine - Group 3 - DAC</li>
</ul>
</div>
<div class="cell code" data-execution_count="1"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ac5Js1iML3d_" data-outputId="778ccabb-d5bb-45a1-8bca-01c47c9e7ce1">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!git clone https://github.com/cdancette/deep-learning-polytech-tp6-7.git</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> wget https:<span class="op">//</span>github.com<span class="op">/</span>rdfia<span class="op">/</span>rdfia.github.io<span class="op">/</span>raw<span class="op">/</span>master<span class="op">/</span>code<span class="op">/</span><span class="dv">2</span><span class="op">-</span>cd<span class="op">/</span>utils.py</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--2023-11-07 12:42:30--  https://github.com/rdfia/rdfia.github.io/raw/master/code/2-cd/utils.py
Resolving github.com (github.com)... 140.82.113.4
Connecting to github.com (github.com)|140.82.113.4|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/code/2-cd/utils.py [following]
--2023-11-07 12:42:31--  https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/code/2-cd/utils.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2627 (2.6K) [text/plain]
Saving to: ‘utils.py’

utils.py            100%[===================&gt;]   2.57K  --.-KB/s    in 0s      

2023-11-07 12:42:31 (56.2 MB/s) - ‘utils.py’ saved [2627/2627]

</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="2" id="QXEas44B52kC">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run <span class="st">&#39;utils.py&#39;</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3" id="8I7lgAEJvPCh">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> argparse</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.parallel</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.backends.cudnn <span class="im">as</span> cudnn</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.datasets <span class="im">as</span> datasets</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> <span class="op">*</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>sns.set_theme()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>PRINT_INTERVAL <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>PATH<span class="op">=</span><span class="st">&quot;datasets&quot;</span></span></code></pre></div>
</div>
<section id="mnist" class="cell markdown" id="34REXQ2ZIwmH">
<h3>MNIST</h3>
</section>
<div class="cell code" data-execution_count="4" id="31_R9xZEIxZD">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class defines the structure of the neural network</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We first define the convolution and pooling layers as a features extractor</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">1</span>, <span class="dv">6</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We then define fully connected layers as a classifier</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">400</span>, <span class="dv">120</span>),</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>),</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reminder: The softmax is included in the loss, do not put it here</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method called when we apply the network to an input batch</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        bsize <span class="op">=</span> <span class="bu">input</span>.size(<span class="dv">0</span>) <span class="co"># batch size</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.features(<span class="bu">input</span>) <span class="co"># output of the conv layers</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.view(bsize, <span class="op">-</span><span class="dv">1</span>) <span class="co"># we flatten the 2D feature maps into one 1D vector for each input</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.classifier(output) <span class="co"># we compute the output of the fc layers</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.MNIST(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor()</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.MNIST(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor()</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epoch(data, model, criterion, optimizer<span class="op">=</span><span class="va">None</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co">    Make a pass (called epoch in English) on the data `data` with the</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="co">     model `model`. Evaluates `criterion` as loss.</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="co">     If `optimizer` is given, perform a training epoch using</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="co">     the given optimizer, otherwise, perform an evaluation epoch (no backward)</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a><span class="co">     of the model.</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># indicates whether the model is in eval or train mode (some layers behave differently in train and eval)</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> model.train()</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># objects to store metric averages</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>    avg_top1_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>    avg_top5_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>    avg_batch_time <span class="op">=</span> AverageMeter()</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we iterate on the batches</span></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    tic <span class="op">=</span> time.time()</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (<span class="bu">input</span>, target) <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>            <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.cuda()</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>            target <span class="op">=</span> target.cuda()</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward if we are training</span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute metrics</span></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>        prec1, prec5 <span class="op">=</span> accuracy(output, target, topk<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">5</span>))</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>        batch_time <span class="op">=</span> time.time() <span class="op">-</span> tic</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>        tic <span class="op">=</span> time.time()</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>        avg_loss.update(loss.item())</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>        avg_top1_acc.update(prec1.item())</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>        avg_top5_acc.update(prec5.item())</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>        avg_batch_time.update(batch_time)</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>            loss_plot.update(avg_loss.val)</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print info</span></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> PRINT_INTERVAL <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&#39;[</span><span class="sc">{0:s}</span><span class="st"> Batch </span><span class="sc">{1:03d}</span><span class="st">/</span><span class="sc">{2:03d}</span><span class="st">]</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Time </span><span class="sc">{batch_time.val:.3f}</span><span class="st">s (</span><span class="sc">{batch_time.avg:.3f}</span><span class="st">s)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Loss </span><span class="sc">{loss.val:.4f}</span><span class="st"> (</span><span class="sc">{loss.avg:.4f}</span><span class="st">)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Prec@1 </span><span class="sc">{top1.val:5.1f}</span><span class="st"> (</span><span class="sc">{top1.avg:5.1f}</span><span class="st">)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Prec@5 </span><span class="sc">{top5.val:5.1f}</span><span class="st"> (</span><span class="sc">{top5.avg:5.1f}</span><span class="st">)&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;EVAL&quot;</span> <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">&quot;TRAIN&quot;</span>, i, <span class="bu">len</span>(data), batch_time<span class="op">=</span>avg_batch_time, loss<span class="op">=</span>avg_loss,</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>                   top1<span class="op">=</span>avg_top1_acc, top5<span class="op">=</span>avg_top5_acc))</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> optimizer:</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>                loss_plot.plot()</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print summary</span></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">===============&gt; Total time </span><span class="sc">{batch_time:d}</span><span class="st">s</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg loss </span><span class="sc">{loss.avg:.4f}</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg Prec@1 </span><span class="sc">{top1.avg:5.2f}</span><span class="st"> %</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg Prec@5 </span><span class="sc">{top5.avg:5.2f}</span><span class="st"> %</span><span class="ch">\n</span><span class="st">&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>           batch_time<span class="op">=</span><span class="bu">int</span>(avg_batch_time.<span class="bu">sum</span>), loss<span class="op">=</span>avg_loss,</span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>           top1<span class="op">=</span>avg_top1_acc, top5<span class="op">=</span>avg_top5_acc))</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_top1_acc, avg_top5_acc, avg_loss</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(batch_size<span class="op">=</span><span class="dv">128</span>, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ex :</span></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   {&quot;batch_size&quot;: 128, &quot;epochs&quot;: 5, &quot;lr&quot;: 0.1}</span></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define model, loss, optim</span></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNet()</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr)</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>        cudnn.benchmark <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> criterion.cuda()</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the data</span></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>    train, test <span class="op">=</span> get_dataset(batch_size, cuda)</span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init plots</span></span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>    plot <span class="op">=</span> AccLossPlot()</span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a>    loss_plot <span class="op">=</span> TrainLossPlot()</span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We iterate on the epochs</span></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;=================</span><span class="ch">\n</span><span class="st">=== EPOCH &quot;</span><span class="op">+</span><span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>)<span class="op">+</span><span class="st">&quot; =====</span><span class="ch">\n</span><span class="st">=================</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train phase</span></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>        top1_acc, avg_top5_acc, loss <span class="op">=</span> epoch(train, model, criterion, optimizer, cuda)</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Test phase</span></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a>        top1_acc_test, top5_acc_test, loss_test <span class="op">=</span> epoch(test, model, criterion, cuda<span class="op">=</span>cuda)</span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot</span></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="5"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="nDxWsHFaI5lm" data-outputId="94fd3175-bc52-45bf-fdec-f471fc39f117">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to datasets/MNIST/raw/train-images-idx3-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 9912422/9912422 [00:00&lt;00:00, 89365352.94it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting datasets/MNIST/raw/train-images-idx3-ubyte.gz to datasets/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to datasets/MNIST/raw/train-labels-idx1-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 28881/28881 [00:00&lt;00:00, 87970728.99it/s]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting datasets/MNIST/raw/train-labels-idx1-ubyte.gz to datasets/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to datasets/MNIST/raw/t10k-images-idx3-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
100%|██████████| 1648877/1648877 [00:00&lt;00:00, 31636908.16it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to datasets/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 4542/4542 [00:00&lt;00:00, 21477484.52it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/MNIST/raw

=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/469]	Time 2.400s (2.400s)	Loss 2.3170 (2.3170)	Prec@1   9.4 (  9.4)	Prec@5  43.0 ( 43.0)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/68b597d2f8cec2dd3817db24f53b21e42ff772e2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/469]	Time 0.059s (0.040s)	Loss 0.3231 (1.1884)	Prec@1  91.4 ( 68.5)	Prec@5  99.2 ( 90.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4686298b32a02ba87d4388de5b9776b775bd385f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 400/469]	Time 0.029s (0.029s)	Loss 0.1323 (0.7123)	Prec@1  95.3 ( 81.0)	Prec@5 100.0 ( 95.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b1986ccba576850accfb7cf40321654615771a91.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 12s	Avg loss 0.6348	Avg Prec@1 82.96 %	Avg Prec@5 95.73 %

[EVAL Batch 000/079]	Time 0.091s (0.091s)	Loss 0.0954 (0.0954)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)

===============&gt; Total time 1s	Avg loss 0.1420	Avg Prec@1 95.96 %	Avg Prec@5 99.97 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1d647ed9f9bf9ae6daa03c2975aeb5fa1f534928.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/469]	Time 0.095s (0.095s)	Loss 0.1188 (0.1188)	Prec@1  96.9 ( 96.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e32c433c8f209a1142ba3e112e5e2bde1cfb0995.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/469]	Time 0.003s (0.017s)	Loss 0.1118 (0.1322)	Prec@1  96.1 ( 96.2)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/57bed9010fd86003965a62f9bda0f16f14159254.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 400/469]	Time 0.016s (0.019s)	Loss 0.0525 (0.1157)	Prec@1 100.0 ( 96.6)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/716104fcadd70a574fa4b8aa2c3e490c21c960fe.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 8s	Avg loss 0.1137	Avg Prec@1 96.66 %	Avg Prec@5 99.90 %

[EVAL Batch 000/079]	Time 0.097s (0.097s)	Loss 0.0318 (0.0318)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)

===============&gt; Total time 1s	Avg loss 0.0741	Avg Prec@1 97.89 %	Avg Prec@5 100.00 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/394467aa39061af193f8a9edad8d2649a725d413.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/469]	Time 0.158s (0.158s)	Loss 0.1805 (0.1805)	Prec@1  94.5 ( 94.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3525ba8461f9f5ab17aa487fd2ea043a251d491a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/469]	Time 0.026s (0.021s)	Loss 0.0424 (0.0807)	Prec@1  98.4 ( 97.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/34623a1361bba425d286d99b2efe7a95dbdc2e33.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 400/469]	Time 0.003s (0.021s)	Loss 0.0424 (0.0802)	Prec@1  99.2 ( 97.6)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0af1b1f7ed781954a31e386eea8df8894db00a45.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.0785	Avg Prec@1 97.66 %	Avg Prec@5 99.95 %

[EVAL Batch 000/079]	Time 0.093s (0.093s)	Loss 0.0238 (0.0238)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)

===============&gt; Total time 1s	Avg loss 0.0563	Avg Prec@1 98.32 %	Avg Prec@5 100.00 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cb3c58835a1eb9a5365840504ca18bb0484176ac.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/469]	Time 0.118s (0.118s)	Loss 0.0658 (0.0658)	Prec@1  98.4 ( 98.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/80a942befcd6bbce6ba753c575d7058974aea0db.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/469]	Time 0.003s (0.019s)	Loss 0.0574 (0.0662)	Prec@1  97.7 ( 98.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/01b6a6e84ef17cb09793318bf2cef5da7b4527f9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 400/469]	Time 0.017s (0.019s)	Loss 0.0299 (0.0624)	Prec@1  99.2 ( 98.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/936df166d62f2c38254965fcaf82c3b90894a8c3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.0621	Avg Prec@1 98.16 %	Avg Prec@5 99.97 %

[EVAL Batch 000/079]	Time 0.094s (0.094s)	Loss 0.0285 (0.0285)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)

===============&gt; Total time 1s	Avg loss 0.0524	Avg Prec@1 98.53 %	Avg Prec@5 99.99 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e91e39f7c919ead7ff526b4ddfb7b47dd2984ecb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/469]	Time 0.114s (0.114s)	Loss 0.0283 (0.0283)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6a614aad15dff4d64d64c50526e55155cac9d347.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/469]	Time 0.013s (0.017s)	Loss 0.1724 (0.0555)	Prec@1  96.1 ( 98.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/65f778321e7964994014ac24bf02eb5ac24193d8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 400/469]	Time 0.005s (0.017s)	Loss 0.0655 (0.0537)	Prec@1  98.4 ( 98.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8d69323ca52b81aab61daac17ccf0c47f755e521.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 8s	Avg loss 0.0525	Avg Prec@1 98.45 %	Avg Prec@5 99.97 %

[EVAL Batch 000/079]	Time 0.144s (0.144s)	Loss 0.0250 (0.0250)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)

===============&gt; Total time 1s	Avg loss 0.0455	Avg Prec@1 98.66 %	Avg Prec@5 100.00 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f0c2aae48fc05b8d4f6de3f57ec3df208f973bd7.png" /></p>
</div>
</div>
<section id="cifar10" class="cell markdown" id="FBDfDFqtJGjk">
<h3>CIFAR10</h3>
</section>
<div class="cell code" data-execution_count="6" id="3xVkUxvwy6a0">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class defines the structure of the neural network</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We first define the convolution and pooling layers as a features extractor</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We then define fully connected layers as a classifier</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">1000</span>),</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1000</span>, <span class="dv">10</span>),</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reminder: The softmax is included in the loss, do not put it here</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method called when we apply the network to an input batch</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>        bsize <span class="op">=</span> <span class="bu">input</span>.size(<span class="dv">0</span>) <span class="co"># batch size</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.features(<span class="bu">input</span>) <span class="co"># output of the conv layers</span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.view(bsize, <span class="op">-</span><span class="dv">1</span>) <span class="co"># we flatten the 2D feature maps into one 1D vector for each input</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.classifier(output) <span class="co"># we compute the output of the fc layers</span></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor()</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor()</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epoch(data, model, criterion, optimizer<span class="op">=</span><span class="va">None</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a><span class="co">    Make a pass (called epoch in English) on the data `data` with the</span></span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a><span class="co">     model `model`. Evaluates `criterion` as loss.</span></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a><span class="co">     If `optimizer` is given, perform a training epoch using</span></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="co">     the given optimizer, otherwise, perform an evaluation epoch (no backward)</span></span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a><span class="co">     of the model.</span></span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># indicates whether the model is in eval or train mode (some layers behave differently in train and eval)</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> model.train()</span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># objects to store metric averages</span></span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a>    avg_top1_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>    avg_top5_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a>    avg_batch_time <span class="op">=</span> AverageMeter()</span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we iterate on the batches</span></span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a>    tic <span class="op">=</span> time.time()</span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (<span class="bu">input</span>, target) <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a>            <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.cuda()</span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a>            target <span class="op">=</span> target.cuda()</span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward</span></span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb36-91"><a href="#cb36-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward if we are training</span></span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-98"><a href="#cb36-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute metrics</span></span>
<span id="cb36-99"><a href="#cb36-99" aria-hidden="true" tabindex="-1"></a>        prec1, prec5 <span class="op">=</span> accuracy(output, target, topk<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">5</span>))</span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a>        batch_time <span class="op">=</span> time.time() <span class="op">-</span> tic</span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a>        tic <span class="op">=</span> time.time()</span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update</span></span>
<span id="cb36-104"><a href="#cb36-104" aria-hidden="true" tabindex="-1"></a>        avg_loss.update(loss.item())</span>
<span id="cb36-105"><a href="#cb36-105" aria-hidden="true" tabindex="-1"></a>        avg_top1_acc.update(prec1.item())</span>
<span id="cb36-106"><a href="#cb36-106" aria-hidden="true" tabindex="-1"></a>        avg_top5_acc.update(prec5.item())</span>
<span id="cb36-107"><a href="#cb36-107" aria-hidden="true" tabindex="-1"></a>        avg_batch_time.update(batch_time)</span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb36-109"><a href="#cb36-109" aria-hidden="true" tabindex="-1"></a>            loss_plot.update(avg_loss.val)</span>
<span id="cb36-110"><a href="#cb36-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print info</span></span>
<span id="cb36-111"><a href="#cb36-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> PRINT_INTERVAL <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb36-112"><a href="#cb36-112" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&#39;[</span><span class="sc">{0:s}</span><span class="st"> Batch </span><span class="sc">{1:03d}</span><span class="st">/</span><span class="sc">{2:03d}</span><span class="st">]</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-113"><a href="#cb36-113" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Time </span><span class="sc">{batch_time.val:.3f}</span><span class="st">s (</span><span class="sc">{batch_time.avg:.3f}</span><span class="st">s)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-114"><a href="#cb36-114" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Loss </span><span class="sc">{loss.val:.4f}</span><span class="st"> (</span><span class="sc">{loss.avg:.4f}</span><span class="st">)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-115"><a href="#cb36-115" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Prec@1 </span><span class="sc">{top1.val:5.1f}</span><span class="st"> (</span><span class="sc">{top1.avg:5.1f}</span><span class="st">)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-116"><a href="#cb36-116" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Prec@5 </span><span class="sc">{top5.val:5.1f}</span><span class="st"> (</span><span class="sc">{top5.avg:5.1f}</span><span class="st">)&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb36-117"><a href="#cb36-117" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;EVAL&quot;</span> <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">&quot;TRAIN&quot;</span>, i, <span class="bu">len</span>(data), batch_time<span class="op">=</span>avg_batch_time, loss<span class="op">=</span>avg_loss,</span>
<span id="cb36-118"><a href="#cb36-118" aria-hidden="true" tabindex="-1"></a>                   top1<span class="op">=</span>avg_top1_acc, top5<span class="op">=</span>avg_top5_acc))</span>
<span id="cb36-119"><a href="#cb36-119" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> optimizer:</span>
<span id="cb36-120"><a href="#cb36-120" aria-hidden="true" tabindex="-1"></a>                loss_plot.plot()</span>
<span id="cb36-121"><a href="#cb36-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-122"><a href="#cb36-122" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print summary</span></span>
<span id="cb36-123"><a href="#cb36-123" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">===============&gt; Total time </span><span class="sc">{batch_time:d}</span><span class="st">s</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-124"><a href="#cb36-124" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg loss </span><span class="sc">{loss.avg:.4f}</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-125"><a href="#cb36-125" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg Prec@1 </span><span class="sc">{top1.avg:5.2f}</span><span class="st"> %</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb36-126"><a href="#cb36-126" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg Prec@5 </span><span class="sc">{top5.avg:5.2f}</span><span class="st"> %</span><span class="ch">\n</span><span class="st">&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb36-127"><a href="#cb36-127" aria-hidden="true" tabindex="-1"></a>           batch_time<span class="op">=</span><span class="bu">int</span>(avg_batch_time.<span class="bu">sum</span>), loss<span class="op">=</span>avg_loss,</span>
<span id="cb36-128"><a href="#cb36-128" aria-hidden="true" tabindex="-1"></a>           top1<span class="op">=</span>avg_top1_acc, top5<span class="op">=</span>avg_top5_acc))</span>
<span id="cb36-129"><a href="#cb36-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-130"><a href="#cb36-130" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_top1_acc, avg_top5_acc, avg_loss</span>
<span id="cb36-131"><a href="#cb36-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-132"><a href="#cb36-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-133"><a href="#cb36-133" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(batch_size<span class="op">=</span><span class="dv">128</span>, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb36-134"><a href="#cb36-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-135"><a href="#cb36-135" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ex :</span></span>
<span id="cb36-136"><a href="#cb36-136" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   {&quot;batch_size&quot;: 128, &quot;epochs&quot;: 5, &quot;lr&quot;: 0.1}</span></span>
<span id="cb36-137"><a href="#cb36-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-138"><a href="#cb36-138" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define model, loss, optim</span></span>
<span id="cb36-139"><a href="#cb36-139" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNet()</span>
<span id="cb36-140"><a href="#cb36-140" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb36-141"><a href="#cb36-141" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr)</span>
<span id="cb36-142"><a href="#cb36-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-143"><a href="#cb36-143" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb36-144"><a href="#cb36-144" aria-hidden="true" tabindex="-1"></a>        cudnn.benchmark <span class="op">=</span> <span class="va">True</span></span>
<span id="cb36-145"><a href="#cb36-145" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb36-146"><a href="#cb36-146" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> criterion.cuda()</span>
<span id="cb36-147"><a href="#cb36-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-148"><a href="#cb36-148" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the data</span></span>
<span id="cb36-149"><a href="#cb36-149" aria-hidden="true" tabindex="-1"></a>    train, test <span class="op">=</span> get_dataset(batch_size, cuda)</span>
<span id="cb36-150"><a href="#cb36-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-151"><a href="#cb36-151" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init plots</span></span>
<span id="cb36-152"><a href="#cb36-152" aria-hidden="true" tabindex="-1"></a>    plot <span class="op">=</span> AccLossPlot()</span>
<span id="cb36-153"><a href="#cb36-153" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb36-154"><a href="#cb36-154" aria-hidden="true" tabindex="-1"></a>    loss_plot <span class="op">=</span> TrainLossPlot()</span>
<span id="cb36-155"><a href="#cb36-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-156"><a href="#cb36-156" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We iterate on the epochs</span></span>
<span id="cb36-157"><a href="#cb36-157" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb36-158"><a href="#cb36-158" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">&quot;=================</span><span class="ch">\n</span><span class="st">=== EPOCH &quot;</span><span class="op">+</span><span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>)<span class="op">+</span><span class="st">&quot; =====</span><span class="ch">\n</span><span class="st">=================</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb36-159"><a href="#cb36-159" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Train phase</span></span>
<span id="cb36-160"><a href="#cb36-160" aria-hidden="true" tabindex="-1"></a>      top1_acc, avg_top5_acc, loss <span class="op">=</span> epoch(train, model, criterion, optimizer, cuda)</span>
<span id="cb36-161"><a href="#cb36-161" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Test phase</span></span>
<span id="cb36-162"><a href="#cb36-162" aria-hidden="true" tabindex="-1"></a>      top1_acc_test, top5_acc_test, loss_test <span class="op">=</span> epoch(test, model, criterion, cuda<span class="op">=</span>cuda)</span>
<span id="cb36-163"><a href="#cb36-163" aria-hidden="true" tabindex="-1"></a>      <span class="co"># plot</span></span>
<span id="cb36-164"><a href="#cb36-164" aria-hidden="true" tabindex="-1"></a>      plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="7"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="uXoiJsO0PI5C" data-outputId="fc697bbb-dad3-4ccc-c71c-2cd6fdd7d8b1">
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">40</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 170498071/170498071 [00:01&lt;00:00, 101156560.55it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting datasets/cifar-10-python.tar.gz to datasets
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.608s (0.608s)	Loss 2.3035 (2.3035)	Prec@1   9.4 (  9.4)	Prec@5  47.7 ( 47.7)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6c6b5c6a09dbfe06ce894365eaabe82e4bb97bc0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.021s (0.028s)	Loss 2.2966 (2.2682)	Prec@1  14.1 ( 14.4)	Prec@5  47.7 ( 59.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/691180744fc78540070d0a105019eb0e57019f57.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 2.2211	Avg Prec@1 17.09 %	Avg Prec@5 63.35 %

[EVAL Batch 000/079]	Time 0.104s (0.104s)	Loss 1.8887 (1.8887)	Prec@1  35.9 ( 35.9)	Prec@5  86.7 ( 86.7)

===============&gt; Total time 1s	Avg loss 1.9359	Avg Prec@1 30.95 %	Avg Prec@5 80.70 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0f9477d8580cd58411c8d25e688ac957adf85081.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.123s (0.123s)	Loss 2.0280 (2.0280)	Prec@1  24.2 ( 24.2)	Prec@5  77.3 ( 77.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8e5cb5d6483c71f29ac24b59671f7636ff4977bb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.023s)	Loss 1.7253 (1.9441)	Prec@1  43.8 ( 30.0)	Prec@5  89.1 ( 80.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e7b22e7efecafc9bc8eee8173cc2b8c28c006e3b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 1.8398	Avg Prec@1 33.69 %	Avg Prec@5 83.84 %

[EVAL Batch 000/079]	Time 0.177s (0.177s)	Loss 1.5419 (1.5419)	Prec@1  46.9 ( 46.9)	Prec@5  89.1 ( 89.1)

===============&gt; Total time 2s	Avg loss 1.6160	Avg Prec@1 39.74 %	Avg Prec@5 90.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/31ba4c9a26ef2e8707094fbc213fb2f1be8f464b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.140s (0.140s)	Loss 1.6580 (1.6580)	Prec@1  38.3 ( 38.3)	Prec@5  90.6 ( 90.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/47a0a2e3e6963d9bed75b51c8e69cf3cd03e4ca5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.024s)	Loss 1.5077 (1.5476)	Prec@1  46.1 ( 44.3)	Prec@5  92.2 ( 90.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bbfd5a4ecf4d16bb15d5d076c54f834a7c990226.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 1.4969	Avg Prec@1 46.17 %	Avg Prec@5 91.51 %

[EVAL Batch 000/079]	Time 0.107s (0.107s)	Loss 1.3328 (1.3328)	Prec@1  52.3 ( 52.3)	Prec@5  91.4 ( 91.4)

===============&gt; Total time 1s	Avg loss 1.4701	Avg Prec@1 46.29 %	Avg Prec@5 92.86 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/563dd6783b60c952853f52548f2909a21f32a1f8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.132s (0.132s)	Loss 1.4993 (1.4993)	Prec@1  49.2 ( 49.2)	Prec@5  91.4 ( 91.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/65ef61cee44cbd7a0cc8c7634bdb7512c054b0fc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.029s (0.023s)	Loss 1.3019 (1.3447)	Prec@1  57.8 ( 52.4)	Prec@5  94.5 ( 93.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0b14e0d70dbd76ed73e55dbab3256fc81f4faadc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 1.3110	Avg Prec@1 53.46 %	Avg Prec@5 93.87 %

[EVAL Batch 000/079]	Time 0.108s (0.108s)	Loss 1.1613 (1.1613)	Prec@1  60.2 ( 60.2)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 1.2779	Avg Prec@1 53.38 %	Avg Prec@5 94.98 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/78e7a478a520e09643fbeb7a75f1098462472c33.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.129s (0.129s)	Loss 1.2140 (1.2140)	Prec@1  56.2 ( 56.2)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7ff03fe2abd49991f0ef1d548defcf48b8c34e77.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.023s)	Loss 0.9758 (1.1974)	Prec@1  65.6 ( 57.2)	Prec@5  99.2 ( 95.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/880d24493d14babe249407d85c3db97a43767ab2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 1.1680	Avg Prec@1 58.52 %	Avg Prec@5 95.54 %

[EVAL Batch 000/079]	Time 0.106s (0.106s)	Loss 1.0284 (1.0284)	Prec@1  67.2 ( 67.2)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 1s	Avg loss 1.1270	Avg Prec@1 59.97 %	Avg Prec@5 95.98 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/23cf9ee4c305b1fe22201733f40a5692613d711b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.127s (0.127s)	Loss 0.9202 (0.9202)	Prec@1  68.0 ( 68.0)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8fa3a26c249904a920c3099403246c7f8434eeb1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.034s (0.023s)	Loss 1.0528 (1.0568)	Prec@1  61.7 ( 62.5)	Prec@5  99.2 ( 96.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/00b1c07e1a17165f3e109f2843daf26e15663b5c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 1.0413	Avg Prec@1 63.20 %	Avg Prec@5 96.52 %

[EVAL Batch 000/079]	Time 0.123s (0.123s)	Loss 1.0779 (1.0779)	Prec@1  60.9 ( 60.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 1s	Avg loss 1.1409	Avg Prec@1 59.70 %	Avg Prec@5 96.07 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dbb78b3876861758c61e92c1a7e03de81692320f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.134s (0.134s)	Loss 1.0722 (1.0722)	Prec@1  57.8 ( 57.8)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c3e0550d39df55fe80d02041608fca6ae1a4bce7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.023s)	Loss 0.7916 (0.9456)	Prec@1  72.7 ( 66.8)	Prec@5  97.7 ( 97.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6786a5bdbfa5580d59da21d99c3ed4ed78735a89.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.9378	Avg Prec@1 66.96 %	Avg Prec@5 97.25 %

[EVAL Batch 000/079]	Time 0.117s (0.117s)	Loss 1.0623 (1.0623)	Prec@1  65.6 ( 65.6)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 1.1066	Avg Prec@1 60.27 %	Avg Prec@5 96.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4ba954cf409f56cd312f28dba6f6b8bf813c0657.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.136s (0.136s)	Loss 0.9782 (0.9782)	Prec@1  64.8 ( 64.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b6094109495f6e88360e2240c1c9f63331e6e816.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.024s)	Loss 0.7261 (0.8293)	Prec@1  72.7 ( 71.0)	Prec@5  99.2 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3b357139424b1f84f787d58d081071b825f099cd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.8335	Avg Prec@1 70.67 %	Avg Prec@5 97.89 %

[EVAL Batch 000/079]	Time 0.115s (0.115s)	Loss 0.9563 (0.9563)	Prec@1  73.4 ( 73.4)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 1s	Avg loss 0.9835	Avg Prec@1 65.15 %	Avg Prec@5 97.08 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b60c155ed540c3cd2996651b493a957f850c2247.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.145s (0.145s)	Loss 0.8002 (0.8002)	Prec@1  70.3 ( 70.3)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1bbd46654444b8290093bb779b08976e15b9689e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.023s)	Loss 0.7534 (0.7322)	Prec@1  75.8 ( 74.4)	Prec@5  99.2 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/80eeac7eeafb260a11390d4e0d7324e992828ffe.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.7381	Avg Prec@1 74.17 %	Avg Prec@5 98.45 %

[EVAL Batch 000/079]	Time 0.145s (0.145s)	Loss 0.9646 (0.9646)	Prec@1  74.2 ( 74.2)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 1s	Avg loss 0.9834	Avg Prec@1 65.87 %	Avg Prec@5 96.97 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ff5def562ce43267ece3c429e4c0307381aa2d5d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.140s (0.140s)	Loss 0.7191 (0.7191)	Prec@1  73.4 ( 73.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/045a6e62b563ed729efa09738e6f67534d67b91b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.019s (0.023s)	Loss 0.4900 (0.6387)	Prec@1  85.2 ( 77.6)	Prec@5  98.4 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2c60de0b5174b6e42c433aac85de43f7686cec14.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.6477	Avg Prec@1 77.27 %	Avg Prec@5 98.93 %

[EVAL Batch 000/079]	Time 0.158s (0.158s)	Loss 1.1590 (1.1590)	Prec@1  69.5 ( 69.5)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 1.0927	Avg Prec@1 63.73 %	Avg Prec@5 97.17 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ffe7fe070d88dc98d4a11580b4b7c6d5fac13620.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.147s (0.147s)	Loss 0.6501 (0.6501)	Prec@1  77.3 ( 77.3)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a0888ca2d60a9ba0ca9c59ee33eb89952a41f1bb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.024s)	Loss 0.6489 (0.5401)	Prec@1  76.6 ( 81.0)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6114b50ab8d4e2e0c525bf25bb67fcf2d4ca6e63.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.5525	Avg Prec@1 80.53 %	Avg Prec@5 99.32 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.9606 (0.9606)	Prec@1  71.9 ( 71.9)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 0.9908	Avg Prec@1 67.58 %	Avg Prec@5 97.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/eb059ba47e3fc615c73f2dbaf120599bbb5d514a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.152s (0.152s)	Loss 0.5721 (0.5721)	Prec@1  82.8 ( 82.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e8d3fc95b1dc76de355def2c6a807759dd530806.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.034s (0.023s)	Loss 0.6739 (0.4349)	Prec@1  76.6 ( 84.9)	Prec@5  98.4 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f08c4b6088cd12580f8ff9e083824fc43376be5b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 8s	Avg loss 0.4529	Avg Prec@1 84.16 %	Avg Prec@5 99.53 %

[EVAL Batch 000/079]	Time 0.124s (0.124s)	Loss 1.1990 (1.1990)	Prec@1  65.6 ( 65.6)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.0085	Avg Prec@1 68.26 %	Avg Prec@5 97.36 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/beea61c792593b90c26a42808965bb1a974da799.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.152s (0.152s)	Loss 0.3177 (0.3177)	Prec@1  91.4 ( 91.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/79d36448784307457813a1c8a839331f731b0d6a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.048s (0.023s)	Loss 0.3658 (0.3328)	Prec@1  86.7 ( 88.6)	Prec@5 100.0 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8c68288631ab9ee29ea1fd33e9ea1d504d0698e9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 8s	Avg loss 0.3594	Avg Prec@1 87.48 %	Avg Prec@5 99.74 %

[EVAL Batch 000/079]	Time 0.108s (0.108s)	Loss 1.1667 (1.1667)	Prec@1  71.9 ( 71.9)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 1.0774	Avg Prec@1 68.56 %	Avg Prec@5 96.80 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4fb4551784e57ff5bde887d36a63e4e8d1d8bb62.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.176s (0.176s)	Loss 0.2905 (0.2905)	Prec@1  89.1 ( 89.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8120232399e13ed2462578bf33fc9f6c46af2668.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.018s (0.024s)	Loss 0.4239 (0.2573)	Prec@1  86.7 ( 91.3)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b382857b1604f1378494ce885978ca088c4e1aab.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.2837	Avg Prec@1 90.23 %	Avg Prec@5 99.87 %

[EVAL Batch 000/079]	Time 0.127s (0.127s)	Loss 1.3618 (1.3618)	Prec@1  64.8 ( 64.8)	Prec@5  94.5 ( 94.5)

===============&gt; Total time 1s	Avg loss 1.2738	Avg Prec@1 66.05 %	Avg Prec@5 96.78 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b055ac244f4f951c89cc2d5838eccb52aa355d57.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.210s (0.210s)	Loss 0.2618 (0.2618)	Prec@1  88.3 ( 88.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f1057b372b77067fb3a432d1dac3d99430676b32.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.038s (0.025s)	Loss 0.1008 (0.1734)	Prec@1  97.7 ( 94.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dc6065d67aeea88346fc19fa632d7e732cbe3e53.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.1997	Avg Prec@1 92.95 %	Avg Prec@5 99.96 %

[EVAL Batch 000/079]	Time 0.129s (0.129s)	Loss 1.6360 (1.6360)	Prec@1  66.4 ( 66.4)	Prec@5  94.5 ( 94.5)

===============&gt; Total time 1s	Avg loss 1.3347	Avg Prec@1 68.03 %	Avg Prec@5 97.05 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4941d72cfb34d2200dad3795cf8fd682a16fc6ce.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.213s (0.213s)	Loss 0.2419 (0.2419)	Prec@1  89.8 ( 89.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7a72416972d0de3438ca9cdf1cc4d40912e427b4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.028s)	Loss 0.1046 (0.1201)	Prec@1  96.9 ( 95.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cbc30be9545bfd875087985491a68bcc8d46f584.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.1456	Avg Prec@1 95.02 %	Avg Prec@5 99.98 %

[EVAL Batch 000/079]	Time 0.122s (0.122s)	Loss 1.3621 (1.3621)	Prec@1  72.7 ( 72.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 1s	Avg loss 1.2946	Avg Prec@1 68.75 %	Avg Prec@5 97.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c399de1af090f69cc8a5147647d9adb2e0ee99b4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.148s (0.148s)	Loss 0.1308 (0.1308)	Prec@1  97.7 ( 97.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4f44f3b498a9996881e6b1a40df3c844994176c6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.029s)	Loss 0.1002 (0.0944)	Prec@1  95.3 ( 97.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9922c3de934fc7e4f375aac9504e52b34b2b27e4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.1131	Avg Prec@1 96.28 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.118s (0.118s)	Loss 1.7747 (1.7747)	Prec@1  66.4 ( 66.4)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 1s	Avg loss 1.5552	Avg Prec@1 68.51 %	Avg Prec@5 96.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b14d2b1c6acaadbb12736fc29b5106e700651550.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.151s (0.151s)	Loss 0.0641 (0.0641)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d0fa4c24091ece5bc8d5fa93b41e12fe6b0b5eac.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.028s)	Loss 0.0608 (0.0708)	Prec@1  98.4 ( 97.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c64ea96608a4bc42c5f029c26b4db1dd21e889f4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0835	Avg Prec@1 97.33 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.122s (0.122s)	Loss 2.0746 (2.0746)	Prec@1  59.4 ( 59.4)	Prec@5  94.5 ( 94.5)

===============&gt; Total time 1s	Avg loss 1.7987	Avg Prec@1 66.00 %	Avg Prec@5 96.55 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5d17f879f0c33ae7baebc65eece2cc68b8cc613b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.162s (0.162s)	Loss 0.1229 (0.1229)	Prec@1  94.5 ( 94.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5a61e7dac380fb695be7d2f4cb1bfaab13e07c76.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.029s)	Loss 0.0234 (0.0515)	Prec@1 100.0 ( 98.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0a56c4b6c55b1dc5710efa00c7b310738fe7892e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0599	Avg Prec@1 98.11 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.126s (0.126s)	Loss 1.7433 (1.7433)	Prec@1  71.1 ( 71.1)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 1s	Avg loss 1.7328	Avg Prec@1 68.84 %	Avg Prec@5 96.92 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/043a0bc4ecce51e6265de970a03c89e553a5c499.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.157s (0.157s)	Loss 0.0963 (0.0963)	Prec@1  96.1 ( 96.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c0dc2a6950952c6771b9c432b3f366165f7ee0ae.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.035s (0.029s)	Loss 0.0275 (0.0269)	Prec@1  99.2 ( 99.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/72f28d98d8a72a07d0db2cc566c13e485d516967.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0382	Avg Prec@1 98.86 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.127s (0.127s)	Loss 1.6077 (1.6077)	Prec@1  75.0 ( 75.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 1s	Avg loss 1.8076	Avg Prec@1 68.52 %	Avg Prec@5 97.04 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b0245a3ba411bfb76c7de1d60f81c3ada7f19b65.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.169s (0.169s)	Loss 0.0451 (0.0451)	Prec@1  98.4 ( 98.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/92dccc9900aa99f77bb2e79de7cdc9b55eac8dd9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.031s)	Loss 0.0279 (0.0212)	Prec@1  99.2 ( 99.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/332fc4be6ca63ed0af70b6454ce5fed8d40f3612.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.1125	Avg Prec@1 97.26 %	Avg Prec@5 99.79 %

[EVAL Batch 000/079]	Time 0.134s (0.134s)	Loss 1.5632 (1.5632)	Prec@1  68.0 ( 68.0)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 1s	Avg loss 1.3957	Avg Prec@1 66.59 %	Avg Prec@5 96.20 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9f2b0e1be1b946c996d9c4ed64d23251cff9a98d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.160s (0.160s)	Loss 0.1955 (0.1955)	Prec@1  94.5 ( 94.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/eb55d35210722e572274aef2dce3054dfa6309cc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.029s)	Loss 0.0378 (0.0723)	Prec@1  98.4 ( 97.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5f6550cf12972ce2c87ade726d655a29b36d1b50.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0767	Avg Prec@1 97.52 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.134s (0.134s)	Loss 1.7904 (1.7904)	Prec@1  77.3 ( 77.3)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 1s	Avg loss 1.7313	Avg Prec@1 69.18 %	Avg Prec@5 96.81 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ac54f1612f60ea565a4f87c6137e572bce5138a4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.161s (0.161s)	Loss 0.0716 (0.0716)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/28cc5ba30fbdcb318a440ea9be94a9b1884cd7b3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.008s (0.029s)	Loss 0.0106 (0.0288)	Prec@1 100.0 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/30164dffc355d6b65f1ad46351edf9efdb765060.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0302	Avg Prec@1 99.09 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.131s (0.131s)	Loss 2.0440 (2.0440)	Prec@1  74.2 ( 74.2)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 1.9231	Avg Prec@1 69.40 %	Avg Prec@5 96.89 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c817313b517dfe15a6deeb2fa9272d3f0ca81ae4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.152s (0.152s)	Loss 0.0117 (0.0117)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/20c148c324248b3b828c99a4224940bbf99fa2d5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.029s)	Loss 0.0144 (0.0192)	Prec@1  99.2 ( 99.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f36ce9cf806daf4b559d31febd13a61bd883ca6e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0170	Avg Prec@1 99.54 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.140s (0.140s)	Loss 2.1155 (2.1155)	Prec@1  75.8 ( 75.8)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 1s	Avg loss 1.9996	Avg Prec@1 70.18 %	Avg Prec@5 97.04 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8f729b708b7b7da343ed6e589e501dbc86dd9dc6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.170s (0.170s)	Loss 0.0079 (0.0079)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b17596fbb2d25230b8f4ec282df8c0d4efccc7d4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.029s)	Loss 0.0126 (0.0055)	Prec@1  99.2 ( 99.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c8ab64f6e1034d024c58dd7a8ae1c36cd9cc1a63.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0042	Avg Prec@1 99.94 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.130s (0.130s)	Loss 2.3176 (2.3176)	Prec@1  75.0 ( 75.0)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.0636	Avg Prec@1 71.17 %	Avg Prec@5 97.25 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d17244b1a7fc4de95c0e1eccba106d60bfec218f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.172s (0.172s)	Loss 0.0009 (0.0009)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e57a9847b08a5223b0d81f5681343408cb3f7c46.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.061s (0.028s)	Loss 0.0004 (0.0008)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a020bbb2d71460bcb60dc40630af5836a632f16d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0008	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.133s (0.133s)	Loss 2.3082 (2.3082)	Prec@1  75.0 ( 75.0)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.1081	Avg Prec@1 71.52 %	Avg Prec@5 97.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/85c8ab4aa9e8ccf96dadc4965952b08cf4638d47.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.190s (0.190s)	Loss 0.0003 (0.0003)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/77ec6da154387dd392d1db523a277c025adffd73.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.021s (0.035s)	Loss 0.0002 (0.0005)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2673eb7ee76764de0c47f65dacd16309709c3094.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 11s	Avg loss 0.0004	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.131s (0.131s)	Loss 2.3655 (2.3655)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.1644	Avg Prec@1 71.65 %	Avg Prec@5 97.31 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/beccfffdf103e216be113e505bfbf7bf7bde9064.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.159s (0.159s)	Loss 0.0002 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3616a0e49cb436c4ec157a8315efd97fa373b1e3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.029s)	Loss 0.0005 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46e65ef2a0e5668522b14e9496aabec7fb27f561.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0003	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.137s (0.137s)	Loss 2.4021 (2.4021)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.1977	Avg Prec@1 71.65 %	Avg Prec@5 97.28 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/930c3536a544f230d9e28f712ee57e1233cffdac.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.195s (0.195s)	Loss 0.0003 (0.0003)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3fa507111ac73679a0ea2fc5a9b07598695fc9ff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.020s (0.027s)	Loss 0.0001 (0.0003)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fcc8f573446f1a03129c919f72cffd43d55df57e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0003	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.146s (0.146s)	Loss 2.4206 (2.4206)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.2260	Avg Prec@1 71.67 %	Avg Prec@5 97.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9a3cfe027412bb7071d5eb9e61e525a8de6be7fc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.175s (0.175s)	Loss 0.0002 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/76a1f4eb99c8952a02c0478866601fabe9fc4753.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.032s (0.025s)	Loss 0.0001 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/443830ae67c1298b3b15404383bd5e6b421e2ab2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0002	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.130s (0.130s)	Loss 2.4550 (2.4550)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.2540	Avg Prec@1 71.76 %	Avg Prec@5 97.28 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7274d418b91304e9cdb6442702b19914101b33c7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 31 =====
=================

[TRAIN Batch 000/391]	Time 0.176s (0.176s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9db8fded50ff0ccf89c58ba3b2f7b52d87487dff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.043s (0.025s)	Loss 0.0002 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/120e282589c9c5bebcd1315c8d6a5ac192c2975d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0002	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.134s (0.134s)	Loss 2.4742 (2.4742)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.2760	Avg Prec@1 71.73 %	Avg Prec@5 97.30 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5643a5c02e32397b59c607ab678327ace33795c6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 32 =====
=================

[TRAIN Batch 000/391]	Time 0.167s (0.167s)	Loss 0.0003 (0.0003)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bf1042cea802e92811181754f526edaeb1dd5f9c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.023s)	Loss 0.0003 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a2380d57ae1757ea7541547f56525f30c5ec41d7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0002	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.128s (0.128s)	Loss 2.4981 (2.4981)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.2981	Avg Prec@1 71.78 %	Avg Prec@5 97.28 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6271e1c2d8f86e123018ce45a97fbbe927d69244.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 33 =====
=================

[TRAIN Batch 000/391]	Time 0.142s (0.142s)	Loss 0.0002 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/783b8bb1dbafe04196c47b880bceb099a9960792.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.024s)	Loss 0.0001 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ab8d7ff3d85cab3fbad4ee8afee443a92677f0ef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0002	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.138s (0.138s)	Loss 2.5181 (2.5181)	Prec@1  75.8 ( 75.8)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.3171	Avg Prec@1 71.86 %	Avg Prec@5 97.26 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fafb96a6099da515fcbee62cf6926b481e087d9a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 34 =====
=================

[TRAIN Batch 000/391]	Time 0.190s (0.190s)	Loss 0.0002 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/31c624445f39ca3f10e90920ff09edd1593cc621.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.029s (0.024s)	Loss 0.0001 (0.0002)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e76544ed163f6c8fcae9c11be8ba0ff1dc6a594e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0002	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.137s (0.137s)	Loss 2.5369 (2.5369)	Prec@1  75.8 ( 75.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 1s	Avg loss 2.3333	Avg Prec@1 71.84 %	Avg Prec@5 97.28 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/742793d5f49a7539affdc2427adc7612f8f5c1f5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 35 =====
=================

[TRAIN Batch 000/391]	Time 0.180s (0.180s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2d5b7acb3e04edaa8f8e7a7c6e7446d1959b1a55.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.033s (0.023s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aa1cfb9a86a569591e6b123719adbed81313e80b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0001	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.148s (0.148s)	Loss 2.5577 (2.5577)	Prec@1  75.8 ( 75.8)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.3524	Avg Prec@1 71.88 %	Avg Prec@5 97.25 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9121dbdc6733ba906061e1fac5bd55fb7069b48d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 36 =====
=================

[TRAIN Batch 000/391]	Time 0.179s (0.179s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3170b3ee67c1a95c731bc12521f87bf308b01c2f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.020s (0.023s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2aee52c0633b93e4f69b5b9b12e64c4479ea1fe4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.0001	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.141s (0.141s)	Loss 2.5785 (2.5785)	Prec@1  75.8 ( 75.8)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.3665	Avg Prec@1 71.98 %	Avg Prec@5 97.26 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b508053c50d9352e49ad8aa981a0465f9c1cde5c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 37 =====
=================

[TRAIN Batch 000/391]	Time 0.205s (0.205s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c964a1449c9c285fceaa306a61d632cfeddde92d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.018s (0.023s)	Loss 0.0002 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f24ed460859e2eaef420fc5284abf4c3c3be4ab7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 9s	Avg loss 0.0001	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.133s (0.133s)	Loss 2.5873 (2.5873)	Prec@1  75.8 ( 75.8)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.3780	Avg Prec@1 71.89 %	Avg Prec@5 97.26 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8f1ec2cb7ad7ee7b0fa42cc3646b46e8b1a77743.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 38 =====
=================

[TRAIN Batch 000/391]	Time 0.132s (0.132s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7d26fca00b302505e005b9c68f9bf717f09778be.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.031s (0.025s)	Loss 0.0002 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4841af4326a079d47d9ba9cb8d17eefd239095aa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0001	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.131s (0.131s)	Loss 2.6035 (2.6035)	Prec@1  75.8 ( 75.8)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.3924	Avg Prec@1 71.90 %	Avg Prec@5 97.27 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a5c2ccfe24a3f213c42490e4fa9f9abcc09553ec.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 39 =====
=================

[TRAIN Batch 000/391]	Time 0.198s (0.198s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1b8180c4a3a902c41e7beb8345843bd5fd220dc1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.021s (0.024s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8d8c460c50634ff23d79de730c6685b9065777ea.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0001	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.151s (0.151s)	Loss 2.6158 (2.6158)	Prec@1  76.6 ( 76.6)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.4030	Avg Prec@1 71.91 %	Avg Prec@5 97.27 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/547fe0288f0acf4a74a20656204e6261c99d40bf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 40 =====
=================

[TRAIN Batch 000/391]	Time 0.179s (0.179s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2d5e5bd77c044b4406531681932feee515ec7517.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.018s (0.024s)	Loss 0.0001 (0.0001)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cc7e8a6aeee73cf3756b3e640c0cf4d30c60e792.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 10s	Avg loss 0.0001	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.141s (0.141s)	Loss 2.6330 (2.6330)	Prec@1  76.6 ( 76.6)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 1s	Avg loss 2.4161	Avg Prec@1 71.92 %	Avg Prec@5 97.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c36f1aff63eed12068b5084dce5d415dbffc1ce1.png" /></p>
</div>
</div>
<section
id="experimentations--the-effect-of-the-learning-rate-and-the-batch-size"
class="cell markdown" id="E-4GaSAT_qxQ">
<h3>Experimentations : the effect of the learning rate and the batch
size</h3>
<h4 id="the-learning-rate">The learning rate</h4>
</section>
<div class="cell code" data-execution_count="8" id="KE_5PoqkLxl-">
<div class="sourceCode" id="cb161"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class defines the structure of the neural network</span></span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We first define the convolution and pooling layers as a features extractor</span></span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb161-10"><a href="#cb161-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb161-11"><a href="#cb161-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb161-12"><a href="#cb161-12" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb161-13"><a href="#cb161-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb161-14"><a href="#cb161-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb161-15"><a href="#cb161-15" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb161-16"><a href="#cb161-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb161-17"><a href="#cb161-17" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb161-18"><a href="#cb161-18" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb161-19"><a href="#cb161-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb161-20"><a href="#cb161-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We then define fully connected layers as a classifier</span></span>
<span id="cb161-21"><a href="#cb161-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb161-22"><a href="#cb161-22" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">1000</span>),</span>
<span id="cb161-23"><a href="#cb161-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb161-24"><a href="#cb161-24" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1000</span>, <span class="dv">10</span>),</span>
<span id="cb161-25"><a href="#cb161-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reminder: The softmax is included in the loss, do not put it here</span></span>
<span id="cb161-26"><a href="#cb161-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb161-27"><a href="#cb161-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-28"><a href="#cb161-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method called when we apply the network to an input batch</span></span>
<span id="cb161-29"><a href="#cb161-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb161-30"><a href="#cb161-30" aria-hidden="true" tabindex="-1"></a>        bsize <span class="op">=</span> <span class="bu">input</span>.size(<span class="dv">0</span>)  <span class="co"># batch size</span></span>
<span id="cb161-31"><a href="#cb161-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.features(<span class="bu">input</span>)  <span class="co"># output of the conv layers</span></span>
<span id="cb161-32"><a href="#cb161-32" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.view(bsize, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># we flatten the 2D feature maps into one 1D vector for each input</span></span>
<span id="cb161-33"><a href="#cb161-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.classifier(output)  <span class="co"># we compute the output of the fc layers</span></span>
<span id="cb161-34"><a href="#cb161-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb161-35"><a href="#cb161-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-36"><a href="#cb161-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-37"><a href="#cb161-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb161-38"><a href="#cb161-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb161-39"><a href="#cb161-39" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb161-40"><a href="#cb161-40" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb161-41"><a href="#cb161-41" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb161-42"><a href="#cb161-42" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb161-43"><a href="#cb161-43" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb161-44"><a href="#cb161-44" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor()</span>
<span id="cb161-45"><a href="#cb161-45" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb161-46"><a href="#cb161-46" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb161-47"><a href="#cb161-47" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb161-48"><a href="#cb161-48" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor()</span>
<span id="cb161-49"><a href="#cb161-49" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb161-50"><a href="#cb161-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-51"><a href="#cb161-51" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb161-52"><a href="#cb161-52" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb161-53"><a href="#cb161-53" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb161-54"><a href="#cb161-54" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb161-55"><a href="#cb161-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-56"><a href="#cb161-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span>
<span id="cb161-57"><a href="#cb161-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-58"><a href="#cb161-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-59"><a href="#cb161-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epoch(data, model, criterion, optimizer<span class="op">=</span><span class="va">None</span>, cuda<span class="op">=</span><span class="va">False</span>, plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb161-60"><a href="#cb161-60" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb161-61"><a href="#cb161-61" aria-hidden="true" tabindex="-1"></a><span class="co">    Make a pass (called epoch in English) on the data `data` with the</span></span>
<span id="cb161-62"><a href="#cb161-62" aria-hidden="true" tabindex="-1"></a><span class="co">     model `model`. Evaluates `criterion` as loss.</span></span>
<span id="cb161-63"><a href="#cb161-63" aria-hidden="true" tabindex="-1"></a><span class="co">     If `optimizer` is given, perform a training epoch using</span></span>
<span id="cb161-64"><a href="#cb161-64" aria-hidden="true" tabindex="-1"></a><span class="co">     the given optimizer, otherwise, perform an evaluation epoch (no backward)</span></span>
<span id="cb161-65"><a href="#cb161-65" aria-hidden="true" tabindex="-1"></a><span class="co">     of the model.</span></span>
<span id="cb161-66"><a href="#cb161-66" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb161-67"><a href="#cb161-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-68"><a href="#cb161-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># indicates whether the model is in eval or train mode (some layers behave differently in train and eval)</span></span>
<span id="cb161-69"><a href="#cb161-69" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> model.train()</span>
<span id="cb161-70"><a href="#cb161-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-71"><a href="#cb161-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># objects to store metric averages</span></span>
<span id="cb161-72"><a href="#cb161-72" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb161-73"><a href="#cb161-73" aria-hidden="true" tabindex="-1"></a>    avg_top1_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb161-74"><a href="#cb161-74" aria-hidden="true" tabindex="-1"></a>    avg_top5_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb161-75"><a href="#cb161-75" aria-hidden="true" tabindex="-1"></a>    avg_batch_time <span class="op">=</span> AverageMeter()</span>
<span id="cb161-76"><a href="#cb161-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb161-77"><a href="#cb161-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-78"><a href="#cb161-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we iterate on the batches</span></span>
<span id="cb161-79"><a href="#cb161-79" aria-hidden="true" tabindex="-1"></a>    tic <span class="op">=</span> time.time()</span>
<span id="cb161-80"><a href="#cb161-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (<span class="bu">input</span>, target) <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb161-81"><a href="#cb161-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cuda:  <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb161-82"><a href="#cb161-82" aria-hidden="true" tabindex="-1"></a>            <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.cuda()</span>
<span id="cb161-83"><a href="#cb161-83" aria-hidden="true" tabindex="-1"></a>            target <span class="op">=</span> target.cuda()</span>
<span id="cb161-84"><a href="#cb161-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-85"><a href="#cb161-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward</span></span>
<span id="cb161-86"><a href="#cb161-86" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb161-87"><a href="#cb161-87" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb161-88"><a href="#cb161-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-89"><a href="#cb161-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward if we are training</span></span>
<span id="cb161-90"><a href="#cb161-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb161-91"><a href="#cb161-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb161-92"><a href="#cb161-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb161-93"><a href="#cb161-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb161-94"><a href="#cb161-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-95"><a href="#cb161-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute metrics</span></span>
<span id="cb161-96"><a href="#cb161-96" aria-hidden="true" tabindex="-1"></a>        prec1, prec5 <span class="op">=</span> accuracy(output, target, topk<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">5</span>))</span>
<span id="cb161-97"><a href="#cb161-97" aria-hidden="true" tabindex="-1"></a>        batch_time <span class="op">=</span> time.time() <span class="op">-</span> tic</span>
<span id="cb161-98"><a href="#cb161-98" aria-hidden="true" tabindex="-1"></a>        tic <span class="op">=</span> time.time()</span>
<span id="cb161-99"><a href="#cb161-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-100"><a href="#cb161-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update</span></span>
<span id="cb161-101"><a href="#cb161-101" aria-hidden="true" tabindex="-1"></a>        avg_loss.update(loss.item())</span>
<span id="cb161-102"><a href="#cb161-102" aria-hidden="true" tabindex="-1"></a>        avg_top1_acc.update(prec1.item())</span>
<span id="cb161-103"><a href="#cb161-103" aria-hidden="true" tabindex="-1"></a>        avg_top5_acc.update(prec5.item())</span>
<span id="cb161-104"><a href="#cb161-104" aria-hidden="true" tabindex="-1"></a>        avg_batch_time.update(batch_time)</span>
<span id="cb161-105"><a href="#cb161-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer <span class="kw">and</span> plot:</span>
<span id="cb161-106"><a href="#cb161-106" aria-hidden="true" tabindex="-1"></a>            loss_plot.update(avg_loss.val)</span>
<span id="cb161-107"><a href="#cb161-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print info</span></span>
<span id="cb161-108"><a href="#cb161-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if i % PRINT_INTERVAL == 0:</span></span>
<span id="cb161-109"><a href="#cb161-109" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     print(&#39;[{0:s} Batch {1:03d}/{2:03d}]\t&#39;</span></span>
<span id="cb161-110"><a href="#cb161-110" aria-hidden="true" tabindex="-1"></a>        <span class="co">#           &#39;Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\t&#39;</span></span>
<span id="cb161-111"><a href="#cb161-111" aria-hidden="true" tabindex="-1"></a>        <span class="co">#           &#39;Loss {loss.val:.4f} ({loss.avg:.4f})\t&#39;</span></span>
<span id="cb161-112"><a href="#cb161-112" aria-hidden="true" tabindex="-1"></a>        <span class="co">#           &#39;Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\t&#39;</span></span>
<span id="cb161-113"><a href="#cb161-113" aria-hidden="true" tabindex="-1"></a>        <span class="co">#           &#39;Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})&#39;.format(</span></span>
<span id="cb161-114"><a href="#cb161-114" aria-hidden="true" tabindex="-1"></a>        <span class="co">#            &quot;EVAL&quot; if optimizer is None else &quot;TRAIN&quot;, i, len(data), batch_time=avg_batch_time, loss=avg_loss,</span></span>
<span id="cb161-115"><a href="#cb161-115" aria-hidden="true" tabindex="-1"></a>        <span class="co">#            top1=avg_top1_acc, top5=avg_top5_acc))</span></span>
<span id="cb161-116"><a href="#cb161-116" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     if optimizer and plot:</span></span>
<span id="cb161-117"><a href="#cb161-117" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         loss_plot.plot()</span></span>
<span id="cb161-118"><a href="#cb161-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-119"><a href="#cb161-119" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print summary</span></span>
<span id="cb161-120"><a href="#cb161-120" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(&#39;\n===============&gt; Total time {batch_time:d}s\t&#39;</span></span>
<span id="cb161-121"><a href="#cb161-121" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       &#39;Avg loss {loss.avg:.4f}\t&#39;</span></span>
<span id="cb161-122"><a href="#cb161-122" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       &#39;Avg Prec@1 {top1.avg:5.2f} %\t&#39;</span></span>
<span id="cb161-123"><a href="#cb161-123" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       &#39;Avg Prec@5 {top5.avg:5.2f} %\n&#39;.format(</span></span>
<span id="cb161-124"><a href="#cb161-124" aria-hidden="true" tabindex="-1"></a>    <span class="co">#        batch_time=int(avg_batch_time.sum), loss=avg_loss,</span></span>
<span id="cb161-125"><a href="#cb161-125" aria-hidden="true" tabindex="-1"></a>    <span class="co">#        top1=avg_top1_acc, top5=avg_top5_acc))</span></span>
<span id="cb161-126"><a href="#cb161-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-127"><a href="#cb161-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_top1_acc, avg_top5_acc, avg_loss</span>
<span id="cb161-128"><a href="#cb161-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-129"><a href="#cb161-129" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AccLossPlot_bis(<span class="bu">object</span>):</span>
<span id="cb161-130"><a href="#cb161-130" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb161-131"><a href="#cb161-131" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_train <span class="op">=</span> []</span>
<span id="cb161-132"><a href="#cb161-132" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_test <span class="op">=</span> []</span>
<span id="cb161-133"><a href="#cb161-133" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.acc_train <span class="op">=</span> []</span>
<span id="cb161-134"><a href="#cb161-134" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.acc_test <span class="op">=</span> []</span>
<span id="cb161-135"><a href="#cb161-135" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fig <span class="op">=</span> plt.figure()</span>
<span id="cb161-136"><a href="#cb161-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-137"><a href="#cb161-137" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_sans_plot(<span class="va">self</span>, loss_train, loss_test, acc_train, acc_test):</span>
<span id="cb161-138"><a href="#cb161-138" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_train.append(loss_train)</span>
<span id="cb161-139"><a href="#cb161-139" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_test.append(loss_test)</span>
<span id="cb161-140"><a href="#cb161-140" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.acc_train.append(acc_train)</span>
<span id="cb161-141"><a href="#cb161-141" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.acc_test.append(acc_test)</span>
<span id="cb161-142"><a href="#cb161-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-143"><a href="#cb161-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-144"><a href="#cb161-144" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(batch_size<span class="op">=</span><span class="dv">128</span>,lr<span class="op">=</span><span class="fl">0.1</span>,epochs<span class="op">=</span><span class="dv">5</span>,cuda<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb161-145"><a href="#cb161-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ex :</span></span>
<span id="cb161-146"><a href="#cb161-146" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   {&quot;batch_size&quot;: 128, &quot;epochs&quot;: 5, &quot;lr&quot;: 0.1}</span></span>
<span id="cb161-147"><a href="#cb161-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-148"><a href="#cb161-148" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define model, loss, optim</span></span>
<span id="cb161-149"><a href="#cb161-149" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNet()</span>
<span id="cb161-150"><a href="#cb161-150" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb161-151"><a href="#cb161-151" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr)</span>
<span id="cb161-152"><a href="#cb161-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-153"><a href="#cb161-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-154"><a href="#cb161-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cuda:  <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb161-155"><a href="#cb161-155" aria-hidden="true" tabindex="-1"></a>        cudnn.benchmark <span class="op">=</span> <span class="va">True</span></span>
<span id="cb161-156"><a href="#cb161-156" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb161-157"><a href="#cb161-157" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> criterion.cuda()</span>
<span id="cb161-158"><a href="#cb161-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-159"><a href="#cb161-159" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the data</span></span>
<span id="cb161-160"><a href="#cb161-160" aria-hidden="true" tabindex="-1"></a>    train, test <span class="op">=</span> get_dataset(batch_size, cuda)</span>
<span id="cb161-161"><a href="#cb161-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-162"><a href="#cb161-162" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init plots</span></span>
<span id="cb161-163"><a href="#cb161-163" aria-hidden="true" tabindex="-1"></a>    plot <span class="op">=</span> AccLossPlot_bis()</span>
<span id="cb161-164"><a href="#cb161-164" aria-hidden="true" tabindex="-1"></a>    <span class="co"># global loss_plot</span></span>
<span id="cb161-165"><a href="#cb161-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss_plot = TrainLossPlot()</span></span>
<span id="cb161-166"><a href="#cb161-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-167"><a href="#cb161-167" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We iterate on the epochs</span></span>
<span id="cb161-168"><a href="#cb161-168" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb161-169"><a href="#cb161-169" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;=================\n==== EPOCH &quot;+str(i+1)+&quot; =====\n=================\n&quot;)</span></span>
<span id="cb161-170"><a href="#cb161-170" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train phase</span></span>
<span id="cb161-171"><a href="#cb161-171" aria-hidden="true" tabindex="-1"></a>        top1_acc, avg_top5_acc, loss <span class="op">=</span> epoch(train, model, criterion, optimizer, cuda)</span>
<span id="cb161-172"><a href="#cb161-172" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Test phase</span></span>
<span id="cb161-173"><a href="#cb161-173" aria-hidden="true" tabindex="-1"></a>        top1_acc_test, top5_acc_test, loss_test <span class="op">=</span> epoch(test, model, criterion, cuda<span class="op">=</span>cuda)</span>
<span id="cb161-174"><a href="#cb161-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-175"><a href="#cb161-175" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot</span></span>
<span id="cb161-176"><a href="#cb161-176" aria-hidden="true" tabindex="-1"></a>        plot.update_sans_plot(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)</span>
<span id="cb161-177"><a href="#cb161-177" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot.plot(save_name)</span></span>
<span id="cb161-178"><a href="#cb161-178" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plot</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="9"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:303}"
id="UYGmKJ-hAKlx" data-outputId="d668b6aa-465d-4d62-8eeb-b0fe2c13484b">
<div class="sourceCode" id="cb162"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Experimentations  : the learning rate</span></span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>dic_lr <span class="op">=</span> {}</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lr <span class="kw">in</span> [<span class="dv">0</span>, <span class="fl">0.01</span>,<span class="fl">0.1</span>, <span class="fl">0.5</span>]:</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>   dic_lr[lr]<span class="op">=</span> main(<span class="dv">128</span>, lr,epochs <span class="op">=</span> <span class="dv">10</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:49&lt;00:00, 10.99s/it]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:51&lt;00:00, 11.10s/it]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:47&lt;00:00, 10.77s/it]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:50&lt;00:00, 11.01s/it]
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="10"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Vv5hN1uUGPbd" data-outputId="3113ba38-1a32-4358-bacf-9f3b3553e0b5">
<div class="sourceCode" id="cb175"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lr, result <span class="kw">in</span> dic_lr.items():</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(result.loss_test)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423, 2.3026428977145423]
[2.2811631130266794, 2.041857452332219, 1.9445053686069538, 1.9152453428582301, 1.950498947614356, 1.7250687230991413, 1.6543763242190397, 1.61670574357238, 1.5235125822356985, 1.4681698264954965]
[2.0259660301329214, 1.5896472432945348, 1.4121667916261698, 1.3262453848802591, 1.2676551930512054, 1.1320578418200529, 1.034792535667178, 0.9730202812182752, 1.0342380034772656, 1.0087692903566965]
[2.3035796865632263, 2.2989246875424927, 1.7878087034708336, 1.7424838256232347, 1.6600377951996237, 1.348710613914683, 1.4034113189842128, 1.4432338083846659, 1.3137885814980617, 1.2739698826512205]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="11"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:469}"
id="TX8dRBe01hP9" data-outputId="a77ac330-1313-4cb9-c377-43841e7edb2d">
<div class="sourceCode" id="cb177"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">15</span>,<span class="dv">10</span>))</span>
<span id="cb177-2"><a href="#cb177-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-3"><a href="#cb177-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lr, result <span class="kw">in</span> dic_lr.items():</span>
<span id="cb177-4"><a href="#cb177-4" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>,<span class="dv">0</span>].plot(result.acc_train, label<span class="op">=</span><span class="ss">f&quot;lr = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb177-5"><a href="#cb177-5" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>,<span class="dv">1</span>].plot(result.acc_test, label<span class="op">=</span><span class="ss">f&quot;lr = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb177-6"><a href="#cb177-6" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>,<span class="dv">0</span>].plot(result.loss_train, label<span class="op">=</span><span class="ss">f&quot;lr = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb177-7"><a href="#cb177-7" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>,<span class="dv">1</span>].plot(result.loss_test, label<span class="op">=</span><span class="ss">f&quot;lr = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb177-8"><a href="#cb177-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-9"><a href="#cb177-9" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>,<span class="dv">0</span>].set_title(<span class="st">&quot;Train Accuracy  with batch = 128&quot;</span>)</span>
<span id="cb177-10"><a href="#cb177-10" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>,<span class="dv">1</span>].set_title(<span class="st">&quot;Test Accuracy with batch = 128 &quot;</span>)</span>
<span id="cb177-11"><a href="#cb177-11" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>,<span class="dv">0</span>].set_title(<span class="st">&quot;Train loss with batch = 128 &quot;</span>)</span>
<span id="cb177-12"><a href="#cb177-12" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>,<span class="dv">1</span>].set_title(<span class="st">&quot;Test loss with batch = 128 &quot;</span>)</span>
<span id="cb177-13"><a href="#cb177-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-14"><a href="#cb177-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb177-15"><a href="#cb177-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/eb865c5005414b00f2c3a47fd38a05abf69ddb16.png" /></p>
</div>
</div>
<div class="cell markdown" id="WCYFFA115bn6">
<p>The batch size</p>
</div>
<div class="cell code" data-execution_count="12"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:303}"
id="1AhHEt6v2xpb" data-outputId="10edc5d5-285f-43d7-f8f7-97f8884ac1f2">
<div class="sourceCode" id="cb178"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Experimentations  : the batch size</span></span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a>dic_bs <span class="op">=</span> {}</span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bs <span class="kw">in</span> [<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>]:</span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a>   dic_bs[bs]<span class="op">=</span> main(bs, <span class="fl">0.1</span>,epochs <span class="op">=</span> <span class="dv">10</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [05:13&lt;00:00, 31.31s/it]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:50&lt;00:00, 11.08s/it]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:34&lt;00:00,  9.46s/it]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 10/10 [01:33&lt;00:00,  9.32s/it]
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="13"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:469}"
id="h-j2QJf9uL8u" data-outputId="4e656da2-d475-48dd-b56d-4e92f8486b56">
<div class="sourceCode" id="cb191"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">15</span>,<span class="dv">10</span>))</span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bs, result <span class="kw">in</span> dic_bs.items():</span>
<span id="cb191-4"><a href="#cb191-4" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>,<span class="dv">0</span>].plot(result.acc_train, label<span class="op">=</span><span class="ss">f&quot;batch size  = </span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb191-5"><a href="#cb191-5" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>,<span class="dv">1</span>].plot(result.acc_test, label<span class="op">=</span><span class="ss">f&quot;batch size  =  </span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb191-6"><a href="#cb191-6" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>,<span class="dv">0</span>].plot(result.loss_train, label<span class="op">=</span><span class="ss">f&quot;lbatch size  =</span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb191-7"><a href="#cb191-7" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>,<span class="dv">1</span>].plot(result.loss_test, label<span class="op">=</span><span class="ss">f&quot;batch size  =</span><span class="sc">{</span>bs<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb191-8"><a href="#cb191-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-9"><a href="#cb191-9" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>,<span class="dv">0</span>].set_title(<span class="st">&quot;Train Accuracy  with lr = 0.1&quot;</span>)</span>
<span id="cb191-10"><a href="#cb191-10" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>,<span class="dv">1</span>].set_title(<span class="st">&quot;Test Accuracy with lr = 0.1 &quot;</span>)</span>
<span id="cb191-11"><a href="#cb191-11" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>,<span class="dv">0</span>].set_title(<span class="st">&quot;Train loss with lr = 0.1 &quot;</span>)</span>
<span id="cb191-12"><a href="#cb191-12" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>,<span class="dv">1</span>].set_title(<span class="st">&quot;Test loss with lr = 0.1 &quot;</span>)</span>
<span id="cb191-13"><a href="#cb191-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-14"><a href="#cb191-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb191-15"><a href="#cb191-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/79879455335db21959f7010e55f74e7c0a1d90bf.png" /></p>
</div>
</div>
<section id="section-3--results-improvements" class="cell markdown"
id="LdQOnE6zdmbO">
<h3>Section 3 : Results improvements</h3>
<h4 id="1-standardization-of-examples">1. Standardization of
examples</h4>
<p>We will now see several classic techniques to improve the performance
of our model. In the end of session report:</p>
<p>method successfully implemented :</p>
<ul>
<li>one sentence to explain its principle</li>
<li>why it improves learning</li>
</ul>
</section>
<div class="cell code" data-execution_count="14" id="h2_DsJO_0eU4">
<div class="sourceCode" id="cb192"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb192-7"><a href="#cb192-7" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb192-8"><a href="#cb192-8" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb192-9"><a href="#cb192-9" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>] ,</span>
<span id="cb192-10"><a href="#cb192-10" aria-hidden="true" tabindex="-1"></a>                         std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])</span>
<span id="cb192-11"><a href="#cb192-11" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb192-12"><a href="#cb192-12" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb192-13"><a href="#cb192-13" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb192-14"><a href="#cb192-14" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb192-15"><a href="#cb192-15" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>] ,</span>
<span id="cb192-16"><a href="#cb192-16" aria-hidden="true" tabindex="-1"></a>                        std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])</span>
<span id="cb192-17"><a href="#cb192-17" aria-hidden="true" tabindex="-1"></a>        ]))</span>
<span id="cb192-18"><a href="#cb192-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-19"><a href="#cb192-19" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb192-20"><a href="#cb192-20" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb192-21"><a href="#cb192-21" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb192-22"><a href="#cb192-22" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb192-23"><a href="#cb192-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-24"><a href="#cb192-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="15" id="Fj_Rh0Vjdyuv">
<div class="sourceCode" id="cb193"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epoch(data, model, criterion, optimizer<span class="op">=</span><span class="va">None</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Make a pass (called epoch in English) on the data `data` with the</span></span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a><span class="co">     model `model`. Evaluates `criterion` as loss.</span></span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a><span class="co">     If `optimizer` is given, perform a training epoch using</span></span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a><span class="co">     the given optimizer, otherwise, perform an evaluation epoch (no backward)</span></span>
<span id="cb193-7"><a href="#cb193-7" aria-hidden="true" tabindex="-1"></a><span class="co">     of the model.</span></span>
<span id="cb193-8"><a href="#cb193-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb193-9"><a href="#cb193-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-10"><a href="#cb193-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># indicates whether the model is in eval or train mode (some layers behave differently in train and eval)</span></span>
<span id="cb193-11"><a href="#cb193-11" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> model.train()</span>
<span id="cb193-12"><a href="#cb193-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-13"><a href="#cb193-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># objects to store metric averages</span></span>
<span id="cb193-14"><a href="#cb193-14" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb193-15"><a href="#cb193-15" aria-hidden="true" tabindex="-1"></a>    avg_top1_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb193-16"><a href="#cb193-16" aria-hidden="true" tabindex="-1"></a>    avg_top5_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb193-17"><a href="#cb193-17" aria-hidden="true" tabindex="-1"></a>    avg_batch_time <span class="op">=</span> AverageMeter()</span>
<span id="cb193-18"><a href="#cb193-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb193-19"><a href="#cb193-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-20"><a href="#cb193-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we iterate on the batches</span></span>
<span id="cb193-21"><a href="#cb193-21" aria-hidden="true" tabindex="-1"></a>    tic <span class="op">=</span> time.time()</span>
<span id="cb193-22"><a href="#cb193-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (<span class="bu">input</span>, target) <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb193-23"><a href="#cb193-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-24"><a href="#cb193-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb193-25"><a href="#cb193-25" aria-hidden="true" tabindex="-1"></a>            <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.cuda()</span>
<span id="cb193-26"><a href="#cb193-26" aria-hidden="true" tabindex="-1"></a>            target <span class="op">=</span> target.cuda()</span>
<span id="cb193-27"><a href="#cb193-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-28"><a href="#cb193-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward</span></span>
<span id="cb193-29"><a href="#cb193-29" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb193-30"><a href="#cb193-30" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb193-31"><a href="#cb193-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-32"><a href="#cb193-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward if we are training</span></span>
<span id="cb193-33"><a href="#cb193-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb193-34"><a href="#cb193-34" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb193-35"><a href="#cb193-35" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb193-36"><a href="#cb193-36" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb193-37"><a href="#cb193-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-38"><a href="#cb193-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute metrics</span></span>
<span id="cb193-39"><a href="#cb193-39" aria-hidden="true" tabindex="-1"></a>        prec1, prec5 <span class="op">=</span> accuracy(output, target, topk<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">5</span>))</span>
<span id="cb193-40"><a href="#cb193-40" aria-hidden="true" tabindex="-1"></a>        batch_time <span class="op">=</span> time.time() <span class="op">-</span> tic</span>
<span id="cb193-41"><a href="#cb193-41" aria-hidden="true" tabindex="-1"></a>        tic <span class="op">=</span> time.time()</span>
<span id="cb193-42"><a href="#cb193-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-43"><a href="#cb193-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update</span></span>
<span id="cb193-44"><a href="#cb193-44" aria-hidden="true" tabindex="-1"></a>        avg_loss.update(loss.item())</span>
<span id="cb193-45"><a href="#cb193-45" aria-hidden="true" tabindex="-1"></a>        avg_top1_acc.update(prec1.item())</span>
<span id="cb193-46"><a href="#cb193-46" aria-hidden="true" tabindex="-1"></a>        avg_top5_acc.update(prec5.item())</span>
<span id="cb193-47"><a href="#cb193-47" aria-hidden="true" tabindex="-1"></a>        avg_batch_time.update(batch_time)</span>
<span id="cb193-48"><a href="#cb193-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer:</span>
<span id="cb193-49"><a href="#cb193-49" aria-hidden="true" tabindex="-1"></a>            loss_plot.update(avg_loss.val)</span>
<span id="cb193-50"><a href="#cb193-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print info</span></span>
<span id="cb193-51"><a href="#cb193-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> PRINT_INTERVAL <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb193-52"><a href="#cb193-52" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&#39;[</span><span class="sc">{0:s}</span><span class="st"> Batch </span><span class="sc">{1:03d}</span><span class="st">/</span><span class="sc">{2:03d}</span><span class="st">]</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-53"><a href="#cb193-53" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Time </span><span class="sc">{batch_time.val:.3f}</span><span class="st">s (</span><span class="sc">{batch_time.avg:.3f}</span><span class="st">s)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-54"><a href="#cb193-54" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Loss </span><span class="sc">{loss.val:.4f}</span><span class="st"> (</span><span class="sc">{loss.avg:.4f}</span><span class="st">)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-55"><a href="#cb193-55" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Prec@1 </span><span class="sc">{top1.val:5.1f}</span><span class="st"> (</span><span class="sc">{top1.avg:5.1f}</span><span class="st">)</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-56"><a href="#cb193-56" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&#39;Prec@5 </span><span class="sc">{top5.val:5.1f}</span><span class="st"> (</span><span class="sc">{top5.avg:5.1f}</span><span class="st">)&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb193-57"><a href="#cb193-57" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;EVAL&quot;</span> <span class="cf">if</span> optimizer <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">&quot;TRAIN&quot;</span>, i, <span class="bu">len</span>(data), batch_time<span class="op">=</span>avg_batch_time, loss<span class="op">=</span>avg_loss,</span>
<span id="cb193-58"><a href="#cb193-58" aria-hidden="true" tabindex="-1"></a>                   top1<span class="op">=</span>avg_top1_acc, top5<span class="op">=</span>avg_top5_acc))</span>
<span id="cb193-59"><a href="#cb193-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> optimizer:</span>
<span id="cb193-60"><a href="#cb193-60" aria-hidden="true" tabindex="-1"></a>                loss_plot.plot()</span>
<span id="cb193-61"><a href="#cb193-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-62"><a href="#cb193-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print summary</span></span>
<span id="cb193-63"><a href="#cb193-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">===============&gt; Total time </span><span class="sc">{batch_time:d}</span><span class="st">s</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-64"><a href="#cb193-64" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg loss </span><span class="sc">{loss.avg:.4f}</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-65"><a href="#cb193-65" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg Prec@1 </span><span class="sc">{top1.avg:5.2f}</span><span class="st"> %</span><span class="ch">\t</span><span class="st">&#39;</span></span>
<span id="cb193-66"><a href="#cb193-66" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;Avg Prec@5 </span><span class="sc">{top5.avg:5.2f}</span><span class="st"> %</span><span class="ch">\n</span><span class="st">&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb193-67"><a href="#cb193-67" aria-hidden="true" tabindex="-1"></a>           batch_time<span class="op">=</span><span class="bu">int</span>(avg_batch_time.<span class="bu">sum</span>), loss<span class="op">=</span>avg_loss,</span>
<span id="cb193-68"><a href="#cb193-68" aria-hidden="true" tabindex="-1"></a>           top1<span class="op">=</span>avg_top1_acc, top5<span class="op">=</span>avg_top5_acc))</span>
<span id="cb193-69"><a href="#cb193-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-70"><a href="#cb193-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_top1_acc, avg_top5_acc, avg_loss</span>
<span id="cb193-71"><a href="#cb193-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-72"><a href="#cb193-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-73"><a href="#cb193-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(batch_size<span class="op">=</span><span class="dv">128</span>, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb193-74"><a href="#cb193-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-75"><a href="#cb193-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ex :</span></span>
<span id="cb193-76"><a href="#cb193-76" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   {&quot;batch_size&quot;: 128, &quot;epochs&quot;: 5, &quot;lr&quot;: 0.1}</span></span>
<span id="cb193-77"><a href="#cb193-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-78"><a href="#cb193-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define model, loss, optim</span></span>
<span id="cb193-79"><a href="#cb193-79" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNet()</span>
<span id="cb193-80"><a href="#cb193-80" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb193-81"><a href="#cb193-81" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr)</span>
<span id="cb193-82"><a href="#cb193-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-83"><a href="#cb193-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb193-84"><a href="#cb193-84" aria-hidden="true" tabindex="-1"></a>        cudnn.benchmark <span class="op">=</span> <span class="va">True</span></span>
<span id="cb193-85"><a href="#cb193-85" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb193-86"><a href="#cb193-86" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> criterion.cuda()</span>
<span id="cb193-87"><a href="#cb193-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-88"><a href="#cb193-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the data</span></span>
<span id="cb193-89"><a href="#cb193-89" aria-hidden="true" tabindex="-1"></a>    train, test <span class="op">=</span> get_dataset(batch_size, cuda)</span>
<span id="cb193-90"><a href="#cb193-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-91"><a href="#cb193-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init plots</span></span>
<span id="cb193-92"><a href="#cb193-92" aria-hidden="true" tabindex="-1"></a>    plot <span class="op">=</span> AccLossPlot()</span>
<span id="cb193-93"><a href="#cb193-93" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb193-94"><a href="#cb193-94" aria-hidden="true" tabindex="-1"></a>    loss_plot <span class="op">=</span> TrainLossPlot()</span>
<span id="cb193-95"><a href="#cb193-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-96"><a href="#cb193-96" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We iterate on the epochs</span></span>
<span id="cb193-97"><a href="#cb193-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb193-98"><a href="#cb193-98" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;=================</span><span class="ch">\n</span><span class="st">=== EPOCH &quot;</span><span class="op">+</span><span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>)<span class="op">+</span><span class="st">&quot; =====</span><span class="ch">\n</span><span class="st">=================</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb193-99"><a href="#cb193-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train phase</span></span>
<span id="cb193-100"><a href="#cb193-100" aria-hidden="true" tabindex="-1"></a>        top1_acc, avg_top5_acc, loss <span class="op">=</span> epoch(train, model, criterion, optimizer, cuda)</span>
<span id="cb193-101"><a href="#cb193-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Test phase</span></span>
<span id="cb193-102"><a href="#cb193-102" aria-hidden="true" tabindex="-1"></a>        top1_acc_test, top5_acc_test, loss_test <span class="op">=</span> epoch(test, model, criterion, cuda<span class="op">=</span>cuda)</span>
<span id="cb193-103"><a href="#cb193-103" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot</span></span>
<span id="cb193-104"><a href="#cb193-104" aria-hidden="true" tabindex="-1"></a>        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="16"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="8hqBsZuNe_AD" data-outputId="7b1c7489-e510-48c5-e3ea-a77601cda545">
<div class="sourceCode" id="cb194"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.05</span>, epochs<span class="op">=</span><span class="dv">30</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.154s (0.154s)	Loss 2.2969 (2.2969)	Prec@1  10.9 ( 10.9)	Prec@5  56.2 ( 56.2)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1562c0a425d1af570be11babbc694179fdbfec4e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.115s (0.037s)	Loss 1.6608 (1.9983)	Prec@1  45.3 ( 27.0)	Prec@5  91.4 ( 77.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/735a80730681bcc9967bfaf353a5a91ab0674cb0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 1.8313	Avg Prec@1 33.50 %	Avg Prec@5 82.77 %

[EVAL Batch 000/079]	Time 0.133s (0.133s)	Loss 1.7525 (1.7525)	Prec@1  39.8 ( 39.8)	Prec@5  85.9 ( 85.9)

===============&gt; Total time 2s	Avg loss 1.7076	Avg Prec@1 40.52 %	Avg Prec@5 87.84 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9e65b6670ed79ceccb6ebc1683a3fb9f76bc0416.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.143s (0.143s)	Loss 1.5702 (1.5702)	Prec@1  39.8 ( 39.8)	Prec@5  93.0 ( 93.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4cd0fa0c7b7a554f357ffd0f6d5d5ece76427001.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.039s)	Loss 1.4605 (1.4596)	Prec@1  42.2 ( 47.6)	Prec@5  90.6 ( 92.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/69b587730bae98d60ecdf2d044b52d41ef877cc4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 1.3961	Avg Prec@1 50.11 %	Avg Prec@5 93.11 %

[EVAL Batch 000/079]	Time 0.202s (0.202s)	Loss 1.2303 (1.2303)	Prec@1  57.8 ( 57.8)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 3s	Avg loss 1.2932	Avg Prec@1 53.30 %	Avg Prec@5 94.61 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fee83ef091a647af42b1a96b2e24ba276987696a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.178s (0.178s)	Loss 1.2518 (1.2518)	Prec@1  52.3 ( 52.3)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/32f7cea3d77c1540465c2684b01d425375f3227c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.020s (0.035s)	Loss 1.3168 (1.2272)	Prec@1  56.2 ( 56.5)	Prec@5  91.4 ( 94.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/af499162a68aee5195ead137b225f05e832c4e4f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 1.1899	Avg Prec@1 57.93 %	Avg Prec@5 95.33 %

[EVAL Batch 000/079]	Time 0.143s (0.143s)	Loss 0.9379 (0.9379)	Prec@1  68.0 ( 68.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.0904	Avg Prec@1 60.87 %	Avg Prec@5 96.36 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2d77764fb58b7351afaa0a397a4b200b3e04b7c1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.166s (0.166s)	Loss 0.9716 (0.9716)	Prec@1  64.1 ( 64.1)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ed0d1637deabfbb44c378f8dd563e8c63fe698d2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.062s (0.040s)	Loss 1.0329 (1.0421)	Prec@1  58.6 ( 63.3)	Prec@5  95.3 ( 96.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/786cf480431f8b7274b916985858a7f1dc48d97d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 1.0215	Avg Prec@1 64.18 %	Avg Prec@5 96.72 %

[EVAL Batch 000/079]	Time 0.141s (0.141s)	Loss 1.0699 (1.0699)	Prec@1  62.5 ( 62.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 1.1596	Avg Prec@1 59.09 %	Avg Prec@5 96.35 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f12a82d830818df63b4d4051785c2dc65ec2b415.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.215s (0.215s)	Loss 1.0792 (1.0792)	Prec@1  63.3 ( 63.3)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d032485439800e3214b972c14df09af74bf753d3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.037s)	Loss 0.9730 (0.9053)	Prec@1  64.8 ( 68.2)	Prec@5  97.7 ( 97.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/11fbc7f51c1d9d0ded74e0db08c564bf5965c6c3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.8992	Avg Prec@1 68.57 %	Avg Prec@5 97.45 %

[EVAL Batch 000/079]	Time 0.158s (0.158s)	Loss 0.8233 (0.8233)	Prec@1  70.3 ( 70.3)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.9535	Avg Prec@1 66.99 %	Avg Prec@5 97.25 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c99dd2386d7cbce638880a51d6926032a4add601.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.182s (0.182s)	Loss 0.7111 (0.7111)	Prec@1  71.9 ( 71.9)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/13ed3d6f0c676328d685638fa25e1cc6efcba711.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.118s (0.038s)	Loss 0.9640 (0.7940)	Prec@1  68.0 ( 72.5)	Prec@5  96.1 ( 98.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/60e6b8b8ad439acb35eb8d560fdb2953b2d6e9e8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.7857	Avg Prec@1 72.68 %	Avg Prec@5 98.15 %

[EVAL Batch 000/079]	Time 0.147s (0.147s)	Loss 0.7384 (0.7384)	Prec@1  75.8 ( 75.8)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.8837	Avg Prec@1 69.49 %	Avg Prec@5 97.42 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/52b5e7893736dddeed986ce81eeea1b295bc38f6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.161s (0.161s)	Loss 0.7456 (0.7456)	Prec@1  73.4 ( 73.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1d5ad836ec19e02b43168a48389e85be31136fde.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.021s (0.040s)	Loss 0.6636 (0.6956)	Prec@1  72.7 ( 75.7)	Prec@5 100.0 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f0807b9fd64df37209321d17e626fceb8ae8923e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.6914	Avg Prec@1 75.92 %	Avg Prec@5 98.59 %

[EVAL Batch 000/079]	Time 0.172s (0.172s)	Loss 0.7459 (0.7459)	Prec@1  71.1 ( 71.1)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.8661	Avg Prec@1 70.18 %	Avg Prec@5 97.68 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5dcf915be3d6a179f1ad50d947c36924c534dad3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.170s (0.170s)	Loss 0.6427 (0.6427)	Prec@1  77.3 ( 77.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/97e74b578bb520c301561958a1fff530e3e10e1a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.034s)	Loss 0.6310 (0.6134)	Prec@1  78.1 ( 78.5)	Prec@5 100.0 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c47574cf0f6b55c58eb75b6b94267a8b8fb1f454.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.6092	Avg Prec@1 78.88 %	Avg Prec@5 98.97 %

[EVAL Batch 000/079]	Time 0.149s (0.149s)	Loss 0.6737 (0.6737)	Prec@1  75.0 ( 75.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.8700	Avg Prec@1 70.67 %	Avg Prec@5 97.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/256a699ff31d7c50120b8f800a2682630a88f8de.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.172s (0.172s)	Loss 0.5170 (0.5170)	Prec@1  83.6 ( 83.6)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2681e66f13b541f1d5a4bff4cc734f3dab46bc5a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.039s)	Loss 0.6053 (0.5229)	Prec@1  76.6 ( 81.9)	Prec@5  99.2 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/790625a9e4824f2f0446170257aec831d09c4cf2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.5274	Avg Prec@1 81.69 %	Avg Prec@5 99.22 %

[EVAL Batch 000/079]	Time 0.143s (0.143s)	Loss 0.7460 (0.7460)	Prec@1  75.0 ( 75.0)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.8404	Avg Prec@1 72.56 %	Avg Prec@5 97.79 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a4da3a0b8c2a8dc28e6e947c278a0e75e8ecf8ee.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.174s (0.174s)	Loss 0.3332 (0.3332)	Prec@1  91.4 ( 91.4)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a91af39fab01afb5d688a73390f095c6e384749d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.040s)	Loss 0.4025 (0.4365)	Prec@1  87.5 ( 84.7)	Prec@5  99.2 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1f96ddd869c04b7ca4c9f45f27125439cb9f2c26.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 15s	Avg loss 0.4477	Avg Prec@1 84.35 %	Avg Prec@5 99.51 %

[EVAL Batch 000/079]	Time 0.265s (0.265s)	Loss 0.8549 (0.8549)	Prec@1  73.4 ( 73.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.8750	Avg Prec@1 72.66 %	Avg Prec@5 97.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ffbaf9e44a4df47e985d4d34ec6c0fd2022fa6e1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.170s (0.170s)	Loss 0.4317 (0.4317)	Prec@1  87.5 ( 87.5)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a3c3c87e194a9a865075e8c3ee4383681697d98e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.026s (0.035s)	Loss 0.2606 (0.3614)	Prec@1  92.2 ( 87.3)	Prec@5 100.0 ( 99.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/564a403a0219fcde238c758c4525949dc5b5fcfa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.3761	Avg Prec@1 86.79 %	Avg Prec@5 99.66 %

[EVAL Batch 000/079]	Time 0.154s (0.154s)	Loss 0.7846 (0.7846)	Prec@1  78.9 ( 78.9)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.8683	Avg Prec@1 73.04 %	Avg Prec@5 97.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4384d20be357652d6ef9d514114fa4f6d2eb6779.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.186s (0.186s)	Loss 0.3556 (0.3556)	Prec@1  88.3 ( 88.3)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cc5fb0d09dea1fd4e30d2204684bbf87fdde8740.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.050s (0.040s)	Loss 0.3536 (0.2887)	Prec@1  85.2 ( 90.2)	Prec@5 100.0 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b0b56a4f467871afad34cc01339b22aa7efc4dad.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 16s	Avg loss 0.3014	Avg Prec@1 89.62 %	Avg Prec@5 99.81 %

[EVAL Batch 000/079]	Time 0.200s (0.200s)	Loss 0.8157 (0.8157)	Prec@1  75.0 ( 75.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.9093	Avg Prec@1 72.91 %	Avg Prec@5 97.68 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8ed18556de1f97ae70ce85ceb0b7f55daf895bb7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.180s (0.180s)	Loss 0.2000 (0.2000)	Prec@1  93.0 ( 93.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d645ddbcbc4febef208931df0902a0b26adbef54.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.063s (0.034s)	Loss 0.2757 (0.2118)	Prec@1  89.8 ( 92.9)	Prec@5  99.2 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6fd99e4c484e10db1812cacf33b8faea42c74347.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.2325	Avg Prec@1 92.02 %	Avg Prec@5 99.89 %

[EVAL Batch 000/079]	Time 0.168s (0.168s)	Loss 0.8486 (0.8486)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.9473	Avg Prec@1 73.88 %	Avg Prec@5 97.93 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0807f786cd94b87a28e07558464b78839ce56a70.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.182s (0.182s)	Loss 0.1343 (0.1343)	Prec@1  96.9 ( 96.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2f83c8a061797e10dbccbd2274b947bbea398773.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.040s)	Loss 0.1614 (0.1470)	Prec@1  93.0 ( 95.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b95361baa8f7671c8d528015d53a4844df7d0ea6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.1664	Avg Prec@1 94.40 %	Avg Prec@5 99.95 %

[EVAL Batch 000/079]	Time 0.153s (0.153s)	Loss 0.9580 (0.9580)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 1.0427	Avg Prec@1 73.61 %	Avg Prec@5 97.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9bf65a35a484006393f11be1c609fb682de2cdcc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.282s (0.282s)	Loss 0.1372 (0.1372)	Prec@1  95.3 ( 95.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/749b95a2ef55dba6f02155878a86dc387b81187f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.035s (0.035s)	Loss 0.1597 (0.1087)	Prec@1  93.8 ( 96.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2d17d0dcbd22cd1d4292555461a328312870d791.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.1210	Avg Prec@1 96.07 %	Avg Prec@5 99.98 %

[EVAL Batch 000/079]	Time 0.160s (0.160s)	Loss 0.9685 (0.9685)	Prec@1  76.6 ( 76.6)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 1.2762	Avg Prec@1 71.65 %	Avg Prec@5 97.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/91eff8f752ea0fb8f2ada27d5c60f4b53ae9404d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.186s (0.186s)	Loss 0.1205 (0.1205)	Prec@1  96.1 ( 96.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/28934530f10b79d12d22347ae2789d6cdee63540.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.089s (0.038s)	Loss 0.0892 (0.0724)	Prec@1  98.4 ( 97.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1bf499b351e16462bdff6d35967feae8289b2b88.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.1022	Avg Prec@1 97.03 %	Avg Prec@5 99.94 %

[EVAL Batch 000/079]	Time 0.149s (0.149s)	Loss 0.8785 (0.8785)	Prec@1  76.6 ( 76.6)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 1.2156	Avg Prec@1 73.47 %	Avg Prec@5 97.67 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/68aec7aafb9b1132f55c87ef22e898d4ab034f77.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.206s (0.206s)	Loss 0.0536 (0.0536)	Prec@1  97.7 ( 97.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/64571d4ac2e15abe77d54a76a15ea086f13dfb8d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.023s (0.040s)	Loss 0.0583 (0.0368)	Prec@1  98.4 ( 99.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ff68a42e91ebf6e31bf4fc330cf0c3bcee7f65b0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0503	Avg Prec@1 98.59 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.232s (0.232s)	Loss 1.3452 (1.3452)	Prec@1  74.2 ( 74.2)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 3s	Avg loss 1.3730	Avg Prec@1 72.70 %	Avg Prec@5 97.78 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0b8721d479aa3ca4e696f38cc946126fccf738ca.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.196s (0.196s)	Loss 0.1167 (0.1167)	Prec@1  94.5 ( 94.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3bfe14e87f306ae17c3ab1c177734807eaa1b97a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.054s (0.036s)	Loss 0.0171 (0.0222)	Prec@1 100.0 ( 99.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/27985bef4e162d62649c2f0f5ca40445222d41a3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0236	Avg Prec@1 99.50 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.167s (0.167s)	Loss 1.2209 (1.2209)	Prec@1  74.2 ( 74.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.5227	Avg Prec@1 73.40 %	Avg Prec@5 97.67 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b403979debf267fa97623b8eef683af97c30fd6b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.186s (0.186s)	Loss 0.0217 (0.0217)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/70544ae6a68636b6e0501c02fba52c03338cc281.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.028s (0.040s)	Loss 0.0058 (0.0209)	Prec@1 100.0 ( 99.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/717daf76751acba27c16f13b89a4d4cd39119ad3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0161	Avg Prec@1 99.66 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.156s (0.156s)	Loss 1.2845 (1.2845)	Prec@1  78.1 ( 78.1)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.4481	Avg Prec@1 75.95 %	Avg Prec@5 98.10 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/956994e6b60608966261386e286bbddba8b9d664.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.255s (0.255s)	Loss 0.0032 (0.0032)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/844c7c11eeda18d07f2d54b8d5387af06e46128b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.056s (0.036s)	Loss 0.0035 (0.0041)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bca7114f27cf764ad53aea4a5389b340468e0359.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0038	Avg Prec@1 99.97 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 1.2716 (1.2716)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.5141	Avg Prec@1 76.20 %	Avg Prec@5 98.07 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5bda1f6391b82f983d430628cd1d14ef987c15b8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.197s (0.197s)	Loss 0.0025 (0.0025)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e2f58d6ee4f4415994f3fdc9c42ff2427de6cf86.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.058s (0.041s)	Loss 0.0033 (0.0027)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ead7aca6975136a080e029cd6d8783d6a84ec210.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0025	Avg Prec@1 99.97 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.180s (0.180s)	Loss 1.3196 (1.3196)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.5955	Avg Prec@1 76.07 %	Avg Prec@5 97.98 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/897eed852f87c9cff477752b13ef6628cc5670d4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.268s (0.268s)	Loss 0.0034 (0.0034)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a67ba12ddc99485755d012c0cf6645faadde514e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.019s (0.041s)	Loss 0.0011 (0.0015)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bc823703d96e19a4cd747c5a54c02236176226f4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 15s	Avg loss 0.0014	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.165s (0.165s)	Loss 1.4427 (1.4427)	Prec@1  78.9 ( 78.9)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.6278	Avg Prec@1 76.28 %	Avg Prec@5 98.04 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46d1fe46bddc4c2f39cbe6abec7de5b0224e8613.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.204s (0.204s)	Loss 0.0007 (0.0007)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/525d4a9393c2ac42d3bd0663362b7236c5c84b75.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.051s (0.035s)	Loss 0.0010 (0.0014)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/48c218253b88c7ccada1db3ae19a006d119636ee.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0012	Avg Prec@1 99.99 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.161s (0.161s)	Loss 1.4828 (1.4828)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.6648	Avg Prec@1 76.36 %	Avg Prec@5 98.04 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/27b8e0b69918dcdddd5e069799cfd438f835e600.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.217s (0.217s)	Loss 0.0008 (0.0008)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ae1d93d825f89759ae9c0cc2cfc7bced8432e937.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.040s)	Loss 0.0007 (0.0009)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8eaff205701315943fb02e95ff7c8fdeda1d938f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0008	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.181s (0.181s)	Loss 1.5017 (1.5017)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 1.7008	Avg Prec@1 76.32 %	Avg Prec@5 98.09 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7a1e21a1916a48c7a541a95a84ba98a7c9af089e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.179s (0.179s)	Loss 0.0006 (0.0006)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b3844b1b4561c76ad74f4de7066ecbc4e5321058.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.035s)	Loss 0.0008 (0.0006)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9c6e79783a50db1deb9463071b36def909ed89fb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0006	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.162s (0.162s)	Loss 1.5536 (1.5536)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.7316	Avg Prec@1 76.16 %	Avg Prec@5 98.13 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/37f5a6cd15dacabc94011e7b48056c74742cabe7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.214s (0.214s)	Loss 0.0004 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b948ab5c907ab5fe986d529642af61afe3a74b2b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.038s (0.040s)	Loss 0.0006 (0.0006)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/683c4fd109d751f0e3d2dd7d847be097c25bd2aa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0006	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.158s (0.158s)	Loss 1.5696 (1.5696)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.7571	Avg Prec@1 76.25 %	Avg Prec@5 98.06 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b2b052f755a10e9fdf0761b685132cb7505db391.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.257s (0.257s)	Loss 0.0005 (0.0005)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2393aea394971149cd64d214ac115c15ab7ced96.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.039s)	Loss 0.0004 (0.0005)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0a5833f5f78945850fddd26df85cef0b2deeeb74.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 15s	Avg loss 0.0005	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.234s (0.234s)	Loss 1.5992 (1.5992)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.7790	Avg Prec@1 76.05 %	Avg Prec@5 98.09 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/024695088f72dee9c6d732bbcacf00da47946799.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.228s (0.228s)	Loss 0.0004 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5b3a2253b280a8fbf37c918eb56e2753864933e6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.044s (0.034s)	Loss 0.0007 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/051aba6db955ac75aca1f1eea8914c82b8e6c6b9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0004	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.171s (0.171s)	Loss 1.6185 (1.6185)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.7976	Avg Prec@1 76.05 %	Avg Prec@5 98.07 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9a699b60070ec9fc0dfafe374cf025e98abb65d3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.193s (0.193s)	Loss 0.0005 (0.0005)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0677adaa2cc7d41895b40ceb5f73e616c585b3fd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.041s)	Loss 0.0004 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9f044024e92845f0a9b754ae3c9ed9fefc28ecb0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0004	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.166s (0.166s)	Loss 1.6338 (1.6338)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 1.8159	Avg Prec@1 76.14 %	Avg Prec@5 98.08 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2fdc3a66a582b430170557541f6a1857c580f378.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.242s (0.242s)	Loss 0.0004 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/90d50ad06f50c0b32e634e1658a00220a253ef86.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.036s)	Loss 0.0003 (0.0004)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/56c014823de831195261a34f5e119663355584b1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 14s	Avg loss 0.0004	Avg Prec@1 100.00 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.175s (0.175s)	Loss 1.6575 (1.6575)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.8342	Avg Prec@1 76.10 %	Avg Prec@5 98.05 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/28f906e2c49bd7b61214c022bc4250f6db948b86.png" /></p>
</div>
</div>
<div class="cell markdown" id="bSYIwyquzd3g">
<p>Note: All experiments are based on lr=0.1, batch_size = 128 and epoch
= 40.</p>
<p>Improvement 1 : Standardization of examples</p>
<p>Goal : better condition the learning.</p>
<p>Better learning --&gt; better performance</p>
<p>Technic : calculate the mean value and the standard deviation of each
channel over the all train</p>
<p>For RGB : 6 values (3 for mean, 3 for std)</p>
</div>
<section id="21-looking-at-zca-normalization" class="cell markdown"
id="_j02KVrOv1RY">
<h3>21. Looking at ZCA normalization</h3>
</section>
<div class="cell code" data-execution_count="17" id="UFhI7lXPlxBr">
<div class="sourceCode" id="cb286"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb286-1"><a href="#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb286-2"><a href="#cb286-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb286-3"><a href="#cb286-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb286-4"><a href="#cb286-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ZCAWhitening:</span>
<span id="cb286-5"><a href="#cb286-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, epsilon<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb286-6"><a href="#cb286-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon</span>
<span id="cb286-7"><a href="#cb286-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean <span class="op">=</span> <span class="va">None</span></span>
<span id="cb286-8"><a href="#cb286-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.zca_matrix <span class="op">=</span> <span class="va">None</span></span>
<span id="cb286-9"><a href="#cb286-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb286-10"><a href="#cb286-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, images):</span>
<span id="cb286-11"><a href="#cb286-11" aria-hidden="true" tabindex="-1"></a>        flat_images <span class="op">=</span> images.view(images.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb286-12"><a href="#cb286-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean <span class="op">=</span> flat_images.mean(dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># we recalculate the mean for simplicity of shapes</span></span>
<span id="cb286-13"><a href="#cb286-13" aria-hidden="true" tabindex="-1"></a>        flat_images <span class="op">-=</span> <span class="va">self</span>.mean</span>
<span id="cb286-14"><a href="#cb286-14" aria-hidden="true" tabindex="-1"></a>        cov_matrix <span class="op">=</span> torch.mm(flat_images.T, flat_images) <span class="op">/</span> flat_images.size(<span class="dv">0</span>)</span>
<span id="cb286-15"><a href="#cb286-15" aria-hidden="true" tabindex="-1"></a>        u, s, _ <span class="op">=</span> torch.svd(cov_matrix)</span>
<span id="cb286-16"><a href="#cb286-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.zca_matrix <span class="op">=</span> torch.mm(u, torch.mm(torch.diag(<span class="fl">1.0</span> <span class="op">/</span> torch.sqrt(s <span class="op">+</span> <span class="va">self</span>.epsilon)), u.T))</span>
<span id="cb286-17"><a href="#cb286-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb286-18"><a href="#cb286-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> transform(<span class="va">self</span>, images):</span>
<span id="cb286-19"><a href="#cb286-19" aria-hidden="true" tabindex="-1"></a>        flat_images <span class="op">=</span> images.view(images.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb286-20"><a href="#cb286-20" aria-hidden="true" tabindex="-1"></a>        whitened_images <span class="op">=</span> torch.mm(flat_images <span class="op">-</span> <span class="va">self</span>.mean[<span class="va">None</span>, :], <span class="va">self</span>.zca_matrix.T)</span>
<span id="cb286-21"><a href="#cb286-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> whitened_images.view(images.size()).<span class="bu">float</span>()</span>
<span id="cb286-22"><a href="#cb286-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb286-23"><a href="#cb286-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, image):</span>
<span id="cb286-24"><a href="#cb286-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.zca_matrix <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb286-25"><a href="#cb286-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">&quot;ZCAWhitening transform needs to be fitted before applied!&quot;</span>)</span>
<span id="cb286-26"><a href="#cb286-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.transform(image.unsqueeze(<span class="dv">0</span>)).squeeze(<span class="dv">0</span>) <span class="co"># unsqueeze because it treats one image at a time</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="18" id="xLtUGNStmA8L">
<div class="sourceCode" id="cb287"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb287-1"><a href="#cb287-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb287-2"><a href="#cb287-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load datasets without any transform first to compute ZCA</span></span>
<span id="cb287-3"><a href="#cb287-3" aria-hidden="true" tabindex="-1"></a>    train_dataset_raw <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb287-4"><a href="#cb287-4" aria-hidden="true" tabindex="-1"></a>    val_dataset_raw <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb287-5"><a href="#cb287-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-6"><a href="#cb287-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert dataset to tensor</span></span>
<span id="cb287-7"><a href="#cb287-7" aria-hidden="true" tabindex="-1"></a>    train_images <span class="op">=</span> torch.stack([transforms.ToTensor()(img) <span class="cf">for</span> img, _ <span class="kw">in</span> train_dataset_raw])</span>
<span id="cb287-8"><a href="#cb287-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-9"><a href="#cb287-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit ZCA on training set</span></span>
<span id="cb287-10"><a href="#cb287-10" aria-hidden="true" tabindex="-1"></a>    zca <span class="op">=</span> ZCAWhitening()</span>
<span id="cb287-11"><a href="#cb287-11" aria-hidden="true" tabindex="-1"></a>    zca.fit(train_images)</span>
<span id="cb287-12"><a href="#cb287-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-13"><a href="#cb287-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the ZCA transform and others</span></span>
<span id="cb287-14"><a href="#cb287-14" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb287-15"><a href="#cb287-15" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(),</span>
<span id="cb287-16"><a href="#cb287-16" aria-hidden="true" tabindex="-1"></a>        zca</span>
<span id="cb287-17"><a href="#cb287-17" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb287-18"><a href="#cb287-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-19"><a href="#cb287-19" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb287-20"><a href="#cb287-20" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb287-21"><a href="#cb287-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-22"><a href="#cb287-22" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb287-23"><a href="#cb287-23" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb287-24"><a href="#cb287-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-25"><a href="#cb287-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="19"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="6oX8UumIc7Qu" data-outputId="da23ed49-a101-4bea-8fe1-b075a1866602">
<div class="sourceCode" id="cb288"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb288-1"><a href="#cb288-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.05</span>, epochs<span class="op">=</span><span class="dv">30</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.673s (0.673s)	Loss 2.3065 (2.3065)	Prec@1   9.4 (  9.4)	Prec@5  40.6 ( 40.6)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dae88e25faacc0158115ca43b7dd5fe14bba2b93.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.323s (0.212s)	Loss 2.0395 (2.1443)	Prec@1  21.9 ( 19.8)	Prec@5  78.1 ( 69.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/62d32d70d18d86a14b597e6ae4ed2fac8ef42497.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 2.0258	Avg Prec@1 25.18 %	Avg Prec@5 75.84 %

[EVAL Batch 000/079]	Time 0.467s (0.467s)	Loss 1.7743 (1.7743)	Prec@1  39.8 ( 39.8)	Prec@5  85.9 ( 85.9)

===============&gt; Total time 15s	Avg loss 1.7663	Avg Prec@1 33.34 %	Avg Prec@5 86.92 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a604b1bdaced3846cbb760e059f3401d957673be.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.709s (0.709s)	Loss 1.6940 (1.6940)	Prec@1  28.9 ( 28.9)	Prec@5  89.8 ( 89.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1f9c39439afd3f81fec7689df7d77b5925e18fd6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.205s)	Loss 1.6513 (1.7154)	Prec@1  43.0 ( 38.3)	Prec@5  86.7 ( 87.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b2aaa951603e1e56fc62ed5f9139058321699cdd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 79s	Avg loss 1.6205	Avg Prec@1 41.83 %	Avg Prec@5 89.33 %

[EVAL Batch 000/079]	Time 0.451s (0.451s)	Loss 1.5121 (1.5121)	Prec@1  43.8 ( 43.8)	Prec@5  92.2 ( 92.2)

===============&gt; Total time 15s	Avg loss 1.4675	Avg Prec@1 46.55 %	Avg Prec@5 92.41 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/92f3d31dafc12d7316956643c1911b3489180697.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.546s (0.546s)	Loss 1.4100 (1.4100)	Prec@1  43.0 ( 43.0)	Prec@5  93.8 ( 93.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dd12ed6093f68c99368b39327f91c5d960a4b305.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.024s (0.210s)	Loss 1.1771 (1.3568)	Prec@1  57.0 ( 51.9)	Prec@5  95.3 ( 93.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b1ee4d55e6c8bb400b15338f9c77616d80e7d239.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 1.2755	Avg Prec@1 54.99 %	Avg Prec@5 94.25 %

[EVAL Batch 000/079]	Time 0.807s (0.807s)	Loss 1.3581 (1.3581)	Prec@1  51.6 ( 51.6)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 16s	Avg loss 1.2829	Avg Prec@1 55.64 %	Avg Prec@5 94.13 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/34fc16c539a5ee68bb5618432f6fab4733c338ac.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.530s (0.530s)	Loss 1.1967 (1.1967)	Prec@1  62.5 ( 62.5)	Prec@5  93.0 ( 93.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3dab9aa45abd2abf3c6b916aaf30a15c9828ca0f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.389s (0.206s)	Loss 0.9851 (1.0525)	Prec@1  62.5 ( 63.3)	Prec@5  98.4 ( 96.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aa578073ccaffb115b7b0331595d0acebc9b5749.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 1.0203	Avg Prec@1 64.39 %	Avg Prec@5 96.43 %

[EVAL Batch 000/079]	Time 0.479s (0.479s)	Loss 1.0288 (1.0288)	Prec@1  63.3 ( 63.3)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 17s	Avg loss 1.0493	Avg Prec@1 63.52 %	Avg Prec@5 96.12 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec5bf93f3c8baeecc995ffef0ed9bef05b32809d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.702s (0.702s)	Loss 0.8516 (0.8516)	Prec@1  65.6 ( 65.6)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a6ce824ab24badf2f3ab46c9a60d475c37bb275d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.374s (0.230s)	Loss 0.7513 (0.8814)	Prec@1  68.0 ( 69.0)	Prec@5  99.2 ( 97.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f6a069c2742f5821ff6b97bb7b8b0b354667fdc7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 85s	Avg loss 0.8678	Avg Prec@1 69.58 %	Avg Prec@5 97.69 %

[EVAL Batch 000/079]	Time 0.467s (0.467s)	Loss 0.8320 (0.8320)	Prec@1  73.4 ( 73.4)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 16s	Avg loss 0.8939	Avg Prec@1 69.39 %	Avg Prec@5 97.59 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bba189ad481ff91103e4eb3e7d388f73023096eb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.497s (0.497s)	Loss 0.8336 (0.8336)	Prec@1  71.9 ( 71.9)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/697e6e4d2eece61865937ee285d3a8d550d6bfe2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.208s)	Loss 0.7166 (0.7599)	Prec@1  75.0 ( 73.4)	Prec@5  99.2 ( 98.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/05c425bfc4295f24bd1a29de55898fb67696718e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 0.7562	Avg Prec@1 73.64 %	Avg Prec@5 98.19 %

[EVAL Batch 000/079]	Time 0.483s (0.483s)	Loss 0.8093 (0.8093)	Prec@1  72.7 ( 72.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 15s	Avg loss 0.8241	Avg Prec@1 71.39 %	Avg Prec@5 98.02 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/529716515af4423170c1e28cc5a5173455a6fb14.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.553s (0.553s)	Loss 0.5294 (0.5294)	Prec@1  82.8 ( 82.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/08e312b6982b7253d4a8599d58de3d93d5a3272b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.211s)	Loss 0.5764 (0.6762)	Prec@1  78.9 ( 76.7)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/35373a583325f0b66641be29ad0bb8f6edc1ddd5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.6661	Avg Prec@1 76.94 %	Avg Prec@5 98.70 %

[EVAL Batch 000/079]	Time 0.497s (0.497s)	Loss 0.8855 (0.8855)	Prec@1  72.7 ( 72.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 15s	Avg loss 0.8280	Avg Prec@1 72.23 %	Avg Prec@5 97.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/25aa61dc201d52ffb3656574352290eb93e9bd2d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.514s (0.514s)	Loss 0.4638 (0.4638)	Prec@1  86.7 ( 86.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a4756e1b7c66b410b24b12997e783e35f4c10be7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.235s (0.209s)	Loss 0.7558 (0.5938)	Prec@1  75.0 ( 79.2)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/92c5500fa6b46f28e7a2e025375ed2ffc9d0a072.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 0.5911	Avg Prec@1 79.30 %	Avg Prec@5 99.02 %

[EVAL Batch 000/079]	Time 0.495s (0.495s)	Loss 0.7960 (0.7960)	Prec@1  72.7 ( 72.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 16s	Avg loss 0.8170	Avg Prec@1 72.29 %	Avg Prec@5 98.12 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/124af88c49de97a515d7f1a2290e64c825c551c3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.530s (0.530s)	Loss 0.7405 (0.7405)	Prec@1  72.7 ( 72.7)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a7d4484e3703363c7471a68a066db03aa54abd6b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.268s (0.215s)	Loss 0.5169 (0.5112)	Prec@1  79.7 ( 82.0)	Prec@5  99.2 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/700747ee79d30266788141ae3f66a3e288678836.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.5204	Avg Prec@1 81.93 %	Avg Prec@5 99.21 %

[EVAL Batch 000/079]	Time 0.524s (0.524s)	Loss 0.7374 (0.7374)	Prec@1  78.9 ( 78.9)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 15s	Avg loss 0.7709	Avg Prec@1 73.91 %	Avg Prec@5 98.41 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/22025fd981ec1549142514a88775e98b867b31f0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.507s (0.507s)	Loss 0.5018 (0.5018)	Prec@1  85.2 ( 85.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f520f7865d9f1b2990d726d8b44d965b156d020e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.186s (0.213s)	Loss 0.4051 (0.4400)	Prec@1  81.2 ( 84.9)	Prec@5  99.2 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4433d2555de79154bfe662afbcdade7b0b35c236.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.4474	Avg Prec@1 84.49 %	Avg Prec@5 99.50 %

[EVAL Batch 000/079]	Time 0.458s (0.458s)	Loss 0.7109 (0.7109)	Prec@1  77.3 ( 77.3)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 15s	Avg loss 0.7583	Avg Prec@1 74.96 %	Avg Prec@5 98.54 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5a2ec05dfe561fef9b8c1d3d6f79f604ea71e277.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.508s (0.508s)	Loss 0.4284 (0.4284)	Prec@1  82.0 ( 82.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a757abb1dd6167a891fa9439032e162dcd5d194c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.205s)	Loss 0.4067 (0.3679)	Prec@1  82.8 ( 87.3)	Prec@5  98.4 ( 99.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3fc05ef8dbacb081e0fb4373e9c88404de031918.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 79s	Avg loss 0.3815	Avg Prec@1 86.76 %	Avg Prec@5 99.63 %

[EVAL Batch 000/079]	Time 0.532s (0.532s)	Loss 0.7403 (0.7403)	Prec@1  75.8 ( 75.8)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 16s	Avg loss 0.7750	Avg Prec@1 75.12 %	Avg Prec@5 98.43 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ed25220302632742d21adc5d6ec8166beb76317a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.536s (0.536s)	Loss 0.2691 (0.2691)	Prec@1  89.1 ( 89.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d6a05c41572f3817ea7aad53402e23630176260a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.090s (0.206s)	Loss 0.3257 (0.3012)	Prec@1  86.7 ( 89.8)	Prec@5 100.0 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d5209f94825edb872a3eea0ab5a04ea83e5426fa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 0.3146	Avg Prec@1 89.14 %	Avg Prec@5 99.78 %

[EVAL Batch 000/079]	Time 0.551s (0.551s)	Loss 0.8652 (0.8652)	Prec@1  73.4 ( 73.4)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 16s	Avg loss 0.8303	Avg Prec@1 74.68 %	Avg Prec@5 98.43 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/78ba1c721b8f312138609ae655419c33fe690c4b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.849s (0.849s)	Loss 0.3146 (0.3146)	Prec@1  88.3 ( 88.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/63d8188e96571f6c40b0a87f4f4d573100e419f0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.224s (0.220s)	Loss 0.2820 (0.2494)	Prec@1  88.3 ( 91.4)	Prec@5  99.2 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/422ae1875c263d37dacd21b746a093ee0feff0e8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 82s	Avg loss 0.2592	Avg Prec@1 91.05 %	Avg Prec@5 99.86 %

[EVAL Batch 000/079]	Time 0.540s (0.540s)	Loss 0.6453 (0.6453)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 15s	Avg loss 0.8150	Avg Prec@1 75.86 %	Avg Prec@5 98.43 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/68dd1012004d891970cc67a5a9cca3b6b45c6e7f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.515s (0.515s)	Loss 0.2013 (0.2013)	Prec@1  95.3 ( 95.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2ffe2ebcda58cc62afa67b65c8030149e697dfa2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.227s (0.205s)	Loss 0.2669 (0.1787)	Prec@1  90.6 ( 94.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e92e14daa787f50585f0408e2c18d098cca929b7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 0.1977	Avg Prec@1 93.37 %	Avg Prec@5 99.93 %

[EVAL Batch 000/079]	Time 0.688s (0.688s)	Loss 0.9342 (0.9342)	Prec@1  76.6 ( 76.6)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 16s	Avg loss 0.9937	Avg Prec@1 74.48 %	Avg Prec@5 97.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8453e1ed4de87464b14ccf318e789cc23d99892f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.537s (0.537s)	Loss 0.1877 (0.1877)	Prec@1  91.4 ( 91.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dbd2ad29ccc02aa97fbc0e48832f5e02effd083f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.348s (0.204s)	Loss 0.1571 (0.1384)	Prec@1  94.5 ( 95.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f22bee492e9c0fb9cdc25a66a04f5d83a9ef8ead.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.1574	Avg Prec@1 94.79 %	Avg Prec@5 99.96 %

[EVAL Batch 000/079]	Time 0.519s (0.519s)	Loss 0.8467 (0.8467)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 16s	Avg loss 0.8549	Avg Prec@1 77.50 %	Avg Prec@5 98.59 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/94b2df0314cb0454d25a39be0d56a8823174f2af.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.640s (0.640s)	Loss 0.1244 (0.1244)	Prec@1  96.9 ( 96.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/89434b899b6eb5a6272754d5f003a8ac249d8eb9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.174s (0.207s)	Loss 0.1004 (0.1057)	Prec@1  97.7 ( 97.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/971d9eb1ec7dfd5d1a1b8777e4bd0bddf2060f05.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 0.1123	Avg Prec@1 96.60 %	Avg Prec@5 99.97 %

[EVAL Batch 000/079]	Time 0.510s (0.510s)	Loss 0.9616 (0.9616)	Prec@1  81.2 ( 81.2)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 15s	Avg loss 0.9766	Avg Prec@1 76.51 %	Avg Prec@5 98.66 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c14e156d252fd10fe1b7efd577d7d7c81f9608ea.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.500s (0.500s)	Loss 0.1145 (0.1145)	Prec@1  96.9 ( 96.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d593f93dd6130f969f0ef6abce6b5bee1206a56d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.208s)	Loss 0.0458 (0.0557)	Prec@1  99.2 ( 98.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8aca727473a8f50774ae81e8e4c4d79f5b85218c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 0.0635	Avg Prec@1 98.29 %	Avg Prec@5 99.99 %

[EVAL Batch 000/079]	Time 0.718s (0.718s)	Loss 0.8774 (0.8774)	Prec@1  81.2 ( 81.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 17s	Avg loss 1.0446	Avg Prec@1 76.85 %	Avg Prec@5 98.55 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/db85da5e1a378e5e8e5e0e5f94556e77249e236a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.556s (0.556s)	Loss 0.0457 (0.0457)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/688490cdcbe4cfe69fc483d925d5a8bbdad50cc2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.142s (0.206s)	Loss 0.0359 (0.0542)	Prec@1  99.2 ( 98.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/10513519e7fde6c0c22dae0b633689258e45900d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 80s	Avg loss 1.3418	Avg Prec@1 81.77 %	Avg Prec@5 94.64 %

[EVAL Batch 000/079]	Time 0.498s (0.498s)	Loss 1.8166 (1.8166)	Prec@1  34.4 ( 34.4)	Prec@5  83.6 ( 83.6)

===============&gt; Total time 16s	Avg loss 1.6722	Avg Prec@1 41.56 %	Avg Prec@5 88.81 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a5671a455846a3fc20bf09d090d00c68ce4d8f87.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.913s (0.913s)	Loss 1.6926 (1.6926)	Prec@1  38.3 ( 38.3)	Prec@5  89.1 ( 89.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dbf1b37704eca93b002466dc10b4df768e063ef4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.370s (0.209s)	Loss 1.0102 (1.1912)	Prec@1  67.2 ( 59.3)	Prec@5  97.7 ( 95.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/37cfb9ae6a563edcd8587620c81b287399461f60.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 82s	Avg loss 1.0476	Avg Prec@1 64.10 %	Avg Prec@5 96.15 %

[EVAL Batch 000/079]	Time 0.544s (0.544s)	Loss 0.8733 (0.8733)	Prec@1  69.5 ( 69.5)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 15s	Avg loss 0.9342	Avg Prec@1 67.69 %	Avg Prec@5 97.38 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5a642a6ea605c2f9da953a88908c8bb0a56523a4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.564s (0.564s)	Loss 0.8215 (0.8215)	Prec@1  71.9 ( 71.9)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dbcf1e5841ca8179a02641c6e6841b587b56701f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.218s)	Loss 0.8480 (0.7214)	Prec@1  73.4 ( 75.2)	Prec@5  96.9 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/957112e7372caf88eeb90a5b90dd03a0abd38e09.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 84s	Avg loss 0.6933	Avg Prec@1 76.00 %	Avg Prec@5 98.57 %

[EVAL Batch 000/079]	Time 0.535s (0.535s)	Loss 0.8091 (0.8091)	Prec@1  75.0 ( 75.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 16s	Avg loss 0.7946	Avg Prec@1 73.34 %	Avg Prec@5 97.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fb2083e02db1478b3b43196fbcfc438758933801.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.556s (0.556s)	Loss 0.5299 (0.5299)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4198c6b5d2701ab2fbdedb0d33696c5483dd1c2e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.354s (0.219s)	Loss 0.6217 (0.5594)	Prec@1  78.1 ( 80.5)	Prec@5 100.0 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f135c9383a96a368d014b7097f451f3cf64b646c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 84s	Avg loss 0.5391	Avg Prec@1 81.11 %	Avg Prec@5 99.20 %

[EVAL Batch 000/079]	Time 0.525s (0.525s)	Loss 0.8028 (0.8028)	Prec@1  75.0 ( 75.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 16s	Avg loss 0.8222	Avg Prec@1 72.92 %	Avg Prec@5 97.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e241ee75a5669df409b03156579e2eb133f0367b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.518s (0.518s)	Loss 0.4401 (0.4401)	Prec@1  85.2 ( 85.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a7653d0b663d651c87a53ca50a3ae645afd6ca96.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.215s)	Loss 0.2613 (0.4129)	Prec@1  90.6 ( 85.6)	Prec@5 100.0 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ad31b71d2d3d171b695874e6da855466f293e219.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 85s	Avg loss 0.4230	Avg Prec@1 85.19 %	Avg Prec@5 99.55 %

[EVAL Batch 000/079]	Time 0.483s (0.483s)	Loss 0.7485 (0.7485)	Prec@1  77.3 ( 77.3)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 15s	Avg loss 0.8148	Avg Prec@1 73.66 %	Avg Prec@5 98.12 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e00dcfe16bf6f484f4979ba1556a0547aef479db.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.587s (0.587s)	Loss 0.3529 (0.3529)	Prec@1  85.9 ( 85.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/be5acbd909edb4576d80d6b6ea23d843fcea1d47.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.157s (0.215s)	Loss 0.3263 (0.3223)	Prec@1  89.1 ( 88.8)	Prec@5 100.0 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bf6a051c3b2704b50627631aee3b01468175800e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 83s	Avg loss 0.3349	Avg Prec@1 88.32 %	Avg Prec@5 99.76 %

[EVAL Batch 000/079]	Time 0.514s (0.514s)	Loss 0.8081 (0.8081)	Prec@1  76.6 ( 76.6)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 15s	Avg loss 0.8212	Avg Prec@1 75.31 %	Avg Prec@5 98.09 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/78179cf29346db6c575999f03a194107e182a044.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.548s (0.548s)	Loss 0.2204 (0.2204)	Prec@1  93.8 ( 93.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d9e7b19943f034b462c95c74ccf42b4cadb7a2f2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.330s (0.209s)	Loss 0.2740 (0.2418)	Prec@1  89.8 ( 91.8)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aad2b6eaae9c3e2863f4ec8d0489d37b6fc1e9ce.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.2523	Avg Prec@1 91.35 %	Avg Prec@5 99.88 %

[EVAL Batch 000/079]	Time 0.643s (0.643s)	Loss 1.0319 (1.0319)	Prec@1  78.1 ( 78.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 17s	Avg loss 1.0295	Avg Prec@1 72.88 %	Avg Prec@5 98.02 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/778713673a420230ae1ee7fd84a11315ba002083.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.532s (0.532s)	Loss 0.2984 (0.2984)	Prec@1  87.5 ( 87.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cf6ff63871a05a5c5a083d6c0e95cbc53ed6cddf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.155s (0.207s)	Loss 0.2334 (0.1756)	Prec@1  91.4 ( 94.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/55213a9fe61162e18e5bac1faee975c193194443.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.1886	Avg Prec@1 93.60 %	Avg Prec@5 99.96 %

[EVAL Batch 000/079]	Time 0.515s (0.515s)	Loss 0.8951 (0.8951)	Prec@1  75.8 ( 75.8)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 16s	Avg loss 0.9449	Avg Prec@1 76.04 %	Avg Prec@5 98.19 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b946c4df32f092267e86cde2a647e003899f65d9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.696s (0.696s)	Loss 0.1568 (0.1568)	Prec@1  96.9 ( 96.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/14ccdd89013291eb95eaecd05b6824db40999354.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.152s (0.211s)	Loss 0.1267 (0.1216)	Prec@1  95.3 ( 96.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/eadc1e2cd22796ca73df3f54894f4c7c0ec799ef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.1626	Avg Prec@1 94.87 %	Avg Prec@5 99.91 %

[EVAL Batch 000/079]	Time 0.533s (0.533s)	Loss 0.8084 (0.8084)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 16s	Avg loss 0.9857	Avg Prec@1 76.26 %	Avg Prec@5 98.23 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/40a68710596302f2ffa3e8f25ab073978c4ca793.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.582s (0.582s)	Loss 0.1094 (0.1094)	Prec@1  96.9 ( 96.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/023c94eb15a1fbdb6d8a77ee89569875408e96ef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.369s (0.226s)	Loss 0.0952 (0.0850)	Prec@1  96.9 ( 97.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7ac5a1496ba88c0489a15df07ecadffb4dad0fdb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 85s	Avg loss 0.0899	Avg Prec@1 97.32 %	Avg Prec@5 99.99 %

[EVAL Batch 000/079]	Time 0.514s (0.514s)	Loss 0.9680 (0.9680)	Prec@1  77.3 ( 77.3)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 16s	Avg loss 1.0604	Avg Prec@1 76.32 %	Avg Prec@5 98.35 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/43a70495834a0becc76f28e2b0ffb28c3a4bc070.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.594s (0.594s)	Loss 0.0424 (0.0424)	Prec@1  99.2 ( 99.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0d6ed1ce8f8e93cc9cbb048adfdc453cf536b7f6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.026s (0.214s)	Loss 0.0516 (0.0440)	Prec@1  97.7 ( 98.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/919682fa11bcaf749ffde5eee99a8ed0cd7d5c7f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 82s	Avg loss 0.0488	Avg Prec@1 98.68 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.509s (0.509s)	Loss 0.9682 (0.9682)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 15s	Avg loss 1.1889	Avg Prec@1 76.70 %	Avg Prec@5 97.79 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c13c0f803fa33f3aa54435b94cdea6e7cdc0f880.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.574s (0.574s)	Loss 0.0657 (0.0657)	Prec@1  97.7 ( 97.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/db75f4dead495192cea87f6488f200cf78909d4b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.458s (0.217s)	Loss 0.0186 (0.0210)	Prec@1  99.2 ( 99.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8dfa45c850d5c3550f4e0c71677cd2ae2dc210d6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 83s	Avg loss 0.0210	Avg Prec@1 99.65 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 0.536s (0.536s)	Loss 0.9745 (0.9745)	Prec@1  82.0 ( 82.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 16s	Avg loss 1.2388	Avg Prec@1 77.41 %	Avg Prec@5 98.45 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2b01983ec5d798cfd2acb18d98c92059a5ed432a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.629s (0.629s)	Loss 0.0155 (0.0155)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2a0df579f93bb999ac6c2c606187b0ad5d15ee0a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.114s (0.213s)	Loss 0.0045 (0.0076)	Prec@1 100.0 (100.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1e7f5557b15ac541a2206b4e791d4cbbce199bea.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 81s	Avg loss 0.0077	Avg Prec@1 99.97 %	Avg Prec@5 100.00 %

[EVAL Batch 000/079]	Time 1.007s (1.007s)	Loss 0.9792 (0.9792)	Prec@1  82.8 ( 82.8)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 16s	Avg loss 1.3190	Avg Prec@1 78.36 %	Avg Prec@5 98.45 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e42ea1df58e73c45e42edf29cb43b68c9209c6dd.png" /></p>
</div>
</div>
<div class="cell markdown" id="w5uqFK2VqNzJ">
<p>with standardization :</p>
<p>TRAIN ===============&gt; Total time 14s Avg loss 0.8629 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 69.82 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.78 %</p>
<p>EVAL ===============&gt; Total time 2s Avg loss 0.9919 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 65.65 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.12 %</p>
<p>with ZCA :</p>
<p>===============&gt; Total time 88s Avg loss 0.8018 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 72.11 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 98.04 %</p>
<p>===============&gt; Total time 17s Avg loss 0.8037 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 73.14 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.86 %</p>
<p>Loss decreased, precision at 1 increased, precision at 5 also. Train,
take more time because of the fitting. the difference in not a lot.</p>
</div>
<section
id="32-increase-in-the-number-of-training-examples-by-data-increase"
class="cell markdown" id="lmilm3OUwm3V">
<h2>3.2 Increase in the number of training examples by data
increase</h2>
<p>A question of proportionality between the number of images and the
number of parameters.</p>
<p>Solution to counter unbalanced ratio --&gt; Data augmentation</p>
<p>Principle : Artificially incrase the number of available examples</p>
<p>Method : generating at each epoch a "variant" of each image, by
applying random transformations to it.</p>
<p>Transformations:</p>
<ul>
<li>random crop (recadrage aléatoire)</li>
<li>random horizontal symmetry (symétrie horizontal aléatoire)</li>
</ul>
<p>Exemple : (100 x 200) --&gt; (symétrie horizontale) --&gt; (200 x
200)</p>
<p><strong>These transformations are applied to the train
images</strong></p>
<p>Pytorch's torchvision.transforms package includes a RandomCrop
function for random cropping: <a
href="https://pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop"
class="uri">https://pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop</a></p>
<p>a CenterCrop function for center cropping : <a
href="https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html"
class="uri">https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html</a></p>
<p>and a RandomHorizontalFlip function for horizontal flip : <a
href="https://pytorch.org/vision/0.15/generated/torchvision.transforms.RandomHorizontalFlip.html"
class="uri">https://pytorch.org/vision/0.15/generated/torchvision.transforms.RandomHorizontalFlip.html</a></p>
</section>
<div class="cell code" data-execution_count="20" id="5IAv8mkXG0a1">
<div class="sourceCode" id="cb380"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb380-1"><a href="#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb380-2"><a href="#cb380-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb380-3"><a href="#cb380-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class defines the structure of the neural network</span></span>
<span id="cb380-4"><a href="#cb380-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb380-5"><a href="#cb380-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb380-6"><a href="#cb380-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb380-7"><a href="#cb380-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb380-8"><a href="#cb380-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We first define the convolution and pooling layers as a features extractor</span></span>
<span id="cb380-9"><a href="#cb380-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb380-10"><a href="#cb380-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb380-11"><a href="#cb380-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb380-12"><a href="#cb380-12" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb380-13"><a href="#cb380-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb380-14"><a href="#cb380-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb380-15"><a href="#cb380-15" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb380-16"><a href="#cb380-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>,<span class="dv">64</span>, (<span class="dv">5</span>,<span class="dv">5</span>), stride <span class="op">=</span> <span class="dv">1</span>, padding <span class="op">=</span> <span class="dv">2</span>),</span>
<span id="cb380-17"><a href="#cb380-17" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb380-18"><a href="#cb380-18" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>,<span class="dv">2</span>), stride <span class="op">=</span> <span class="dv">2</span>, padding <span class="op">=</span> <span class="dv">0</span>, ceil_mode <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb380-19"><a href="#cb380-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb380-20"><a href="#cb380-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We then define fully connected layers as a classifier</span></span>
<span id="cb380-21"><a href="#cb380-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb380-22"><a href="#cb380-22" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">1000</span>), <span class="co">#1024 = 4*4*64</span></span>
<span id="cb380-23"><a href="#cb380-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb380-24"><a href="#cb380-24" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1000</span>, <span class="dv">10</span>),</span>
<span id="cb380-25"><a href="#cb380-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reminder: The softmax is included in the loss, do not put it here</span></span>
<span id="cb380-26"><a href="#cb380-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb380-27"><a href="#cb380-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb380-28"><a href="#cb380-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method called when we apply the network to an input batch</span></span>
<span id="cb380-29"><a href="#cb380-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb380-30"><a href="#cb380-30" aria-hidden="true" tabindex="-1"></a>        bsize <span class="op">=</span> <span class="bu">input</span>.size(<span class="dv">0</span>) <span class="co"># batch size</span></span>
<span id="cb380-31"><a href="#cb380-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.features(<span class="bu">input</span>) <span class="co"># output of the conv layers</span></span>
<span id="cb380-32"><a href="#cb380-32" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.view(bsize, <span class="op">-</span><span class="dv">1</span>) <span class="co"># we flatten the 2D feature maps into one 1D vector for each input</span></span>
<span id="cb380-33"><a href="#cb380-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.classifier(output) <span class="co"># we compute the output of the fc layers</span></span>
<span id="cb380-34"><a href="#cb380-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="21" id="J7neSHprbl0-">
<div class="sourceCode" id="cb381"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb381-1"><a href="#cb381-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb381-2"><a href="#cb381-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb381-3"><a href="#cb381-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb381-4"><a href="#cb381-4" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb381-5"><a href="#cb381-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb381-6"><a href="#cb381-6" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb381-7"><a href="#cb381-7" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb381-8"><a href="#cb381-8" aria-hidden="true" tabindex="-1"></a>            transforms.RandomCrop(size<span class="op">=</span><span class="dv">28</span>),</span>
<span id="cb381-9"><a href="#cb381-9" aria-hidden="true" tabindex="-1"></a>            transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb381-10"><a href="#cb381-10" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb381-11"><a href="#cb381-11" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>] ,</span>
<span id="cb381-12"><a href="#cb381-12" aria-hidden="true" tabindex="-1"></a>                      std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])]))</span>
<span id="cb381-13"><a href="#cb381-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-14"><a href="#cb381-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-15"><a href="#cb381-15" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb381-16"><a href="#cb381-16" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb381-17"><a href="#cb381-17" aria-hidden="true" tabindex="-1"></a>            transforms.CenterCrop(size<span class="op">=</span><span class="dv">28</span>),</span>
<span id="cb381-18"><a href="#cb381-18" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb381-19"><a href="#cb381-19" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>] ,</span>
<span id="cb381-20"><a href="#cb381-20" aria-hidden="true" tabindex="-1"></a>                      std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])]))</span>
<span id="cb381-21"><a href="#cb381-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-22"><a href="#cb381-22" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb381-23"><a href="#cb381-23" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb381-24"><a href="#cb381-24" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb381-25"><a href="#cb381-25" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb381-26"><a href="#cb381-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-27"><a href="#cb381-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="22"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="HxiM8LERILLP" data-outputId="83dd981a-4bd5-40b0-a85d-ae80a765d6c8">
<div class="sourceCode" id="cb382"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb382-1"><a href="#cb382-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.05</span>, epochs <span class="op">=</span> <span class="dv">40</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.522s (0.522s)	Loss 2.3007 (2.3007)	Prec@1   5.5 (  5.5)	Prec@5  55.5 ( 55.5)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/42651624f78cec99ef1d8790f24b3cbc07275903.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.073s (0.052s)	Loss 1.6974 (2.0480)	Prec@1  33.6 ( 24.7)	Prec@5  89.1 ( 75.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4b722abfe8d932d4f236617d877df12117891414.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 1.8985	Avg Prec@1 30.52 %	Avg Prec@5 81.08 %

[EVAL Batch 000/079]	Time 0.322s (0.322s)	Loss 1.6007 (1.6007)	Prec@1  47.7 ( 47.7)	Prec@5  87.5 ( 87.5)

===============&gt; Total time 3s	Avg loss 1.6689	Avg Prec@1 40.33 %	Avg Prec@5 87.60 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7f1b5dd3e591ffe96b6784ae907ae21b09f1fb8c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.212s (0.212s)	Loss 1.5354 (1.5354)	Prec@1  51.6 ( 51.6)	Prec@5  88.3 ( 88.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/af2aafb8b405f40672365717f9a21124476e8557.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.048s)	Loss 1.6201 (1.5563)	Prec@1  32.8 ( 43.4)	Prec@5  91.4 ( 90.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b32bca5c81d6c719c1e2e78122c91bff1a392628.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 1.5040	Avg Prec@1 45.41 %	Avg Prec@5 91.62 %

[EVAL Batch 000/079]	Time 0.161s (0.161s)	Loss 1.3423 (1.3423)	Prec@1  55.5 ( 55.5)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 3s	Avg loss 1.3315	Avg Prec@1 52.46 %	Avg Prec@5 93.90 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6c7007308e250d7f9a902ed2722b2b7e69d324f7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.337s (0.337s)	Loss 1.2789 (1.2789)	Prec@1  54.7 ( 54.7)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ac0773ebb58131b90336c52df8cce7dcd79020c4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.136s (0.051s)	Loss 1.2441 (1.3553)	Prec@1  55.5 ( 51.4)	Prec@5  95.3 ( 93.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d293cd4b6dd0a164f713020f980c36cfb1f2d7a5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 1.3218	Avg Prec@1 52.77 %	Avg Prec@5 94.06 %

[EVAL Batch 000/079]	Time 0.150s (0.150s)	Loss 1.0987 (1.0987)	Prec@1  62.5 ( 62.5)	Prec@5  94.5 ( 94.5)

===============&gt; Total time 2s	Avg loss 1.1987	Avg Prec@1 57.08 %	Avg Prec@5 95.47 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/39d21d3d218723f4de6bd18a5b7977a7501ce0e6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.208s (0.208s)	Loss 1.1566 (1.1566)	Prec@1  54.7 ( 54.7)	Prec@5  95.3 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6a7c3841dba93307b0da013d50625e9e3f44fc84.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.089s (0.050s)	Loss 1.1992 (1.2137)	Prec@1  54.7 ( 57.0)	Prec@5  97.7 ( 94.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/85753b8e05b58b91b41a180e1858d46deabb2742.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 1.1876	Avg Prec@1 58.03 %	Avg Prec@5 95.12 %

[EVAL Batch 000/079]	Time 0.167s (0.167s)	Loss 1.0365 (1.0365)	Prec@1  67.2 ( 67.2)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.1055	Avg Prec@1 60.37 %	Avg Prec@5 96.19 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f578175062098227b89b77806ab904ea523ebb10.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.219s (0.219s)	Loss 1.0999 (1.0999)	Prec@1  59.4 ( 59.4)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/eacca0b8bcbdbecacefcb4e14f73cc45e93506f1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.050s)	Loss 0.8686 (1.0937)	Prec@1  70.3 ( 61.3)	Prec@5  97.7 ( 95.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a8e2a4226cf36b7c61e5609eeec2790277e0927b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 1.0737	Avg Prec@1 62.18 %	Avg Prec@5 96.06 %

[EVAL Batch 000/079]	Time 0.166s (0.166s)	Loss 0.9816 (0.9816)	Prec@1  62.5 ( 62.5)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 1.0242	Avg Prec@1 64.22 %	Avg Prec@5 96.80 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/80d05243f159e2a9d505baeb37ed5ead28558c25.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.196s (0.196s)	Loss 1.2070 (1.2070)	Prec@1  57.8 ( 57.8)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/35acdc9fb364eee6ac49755671050d3b7f7ac244.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.077s (0.050s)	Loss 1.1733 (0.9994)	Prec@1  59.4 ( 65.4)	Prec@5  94.5 ( 96.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c8fab1ddfe5fe64c269a83571492b9d43d377340.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 0.9831	Avg Prec@1 65.76 %	Avg Prec@5 96.70 %

[EVAL Batch 000/079]	Time 0.163s (0.163s)	Loss 0.8915 (0.8915)	Prec@1  70.3 ( 70.3)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.9029	Avg Prec@1 68.43 %	Avg Prec@5 97.56 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/37d760b2e519416e6053ff8f4a7011d978917f07.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.210s (0.210s)	Loss 0.8667 (0.8667)	Prec@1  68.0 ( 68.0)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/062bdae2636365985bfc082432fbf6e63822dde2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.105s (0.046s)	Loss 0.9981 (0.9204)	Prec@1  64.1 ( 68.2)	Prec@5  96.1 ( 97.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/626387f4205826ab34a01ee418d2071c042efd0c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 0.9015	Avg Prec@1 68.63 %	Avg Prec@5 97.41 %

[EVAL Batch 000/079]	Time 0.180s (0.180s)	Loss 0.8705 (0.8705)	Prec@1  71.1 ( 71.1)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.9174	Avg Prec@1 67.60 %	Avg Prec@5 97.44 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7ee8e0aec108f19158cd03d1ea2d21b41ed4b5bc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.276s (0.276s)	Loss 0.9347 (0.9347)	Prec@1  61.7 ( 61.7)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/920bd2beec38a2212c0a91a268f97ea2e7c5eeb4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.063s (0.051s)	Loss 0.8390 (0.8548)	Prec@1  74.2 ( 70.4)	Prec@5  96.1 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d18c5385c4bf6c1901d302ee4348b741c5fdecaf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.8431	Avg Prec@1 70.58 %	Avg Prec@5 97.76 %

[EVAL Batch 000/079]	Time 0.167s (0.167s)	Loss 0.9036 (0.9036)	Prec@1  68.8 ( 68.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 0.8979	Avg Prec@1 68.97 %	Avg Prec@5 97.72 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/47fc9fd5a46a900d83a095a7f25ec0600a9003f8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.206s (0.206s)	Loss 0.9673 (0.9673)	Prec@1  67.2 ( 67.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/410de96996472986677dca9d8a85118b8c350fdc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.050s)	Loss 0.8012 (0.7913)	Prec@1  75.8 ( 72.4)	Prec@5  98.4 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b93778fa2c90302ae4a93371fb34f0224b694b95.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.7828	Avg Prec@1 72.71 %	Avg Prec@5 97.98 %

[EVAL Batch 000/079]	Time 0.163s (0.163s)	Loss 0.7125 (0.7125)	Prec@1  79.7 ( 79.7)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.7729	Avg Prec@1 73.60 %	Avg Prec@5 98.20 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e23cc495abb081d28ba57751b4759e4329212b2a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.217s (0.217s)	Loss 0.9345 (0.9345)	Prec@1  70.3 ( 70.3)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f1d41b5d6569d8540383d940d33307b2b65b5e17.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.051s)	Loss 0.7911 (0.7476)	Prec@1  71.1 ( 73.9)	Prec@5  99.2 ( 98.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46d34f5e82b314567e9b11f51b52552fd5d96250.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.7407	Avg Prec@1 74.04 %	Avg Prec@5 98.24 %

[EVAL Batch 000/079]	Time 0.301s (0.301s)	Loss 0.7099 (0.7099)	Prec@1  79.7 ( 79.7)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.7598	Avg Prec@1 73.50 %	Avg Prec@5 98.44 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6aa8fd3754c04df1b7d8e877841abc103d8311f9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.235s (0.235s)	Loss 0.6085 (0.6085)	Prec@1  78.9 ( 78.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/143e8a6b449a659b85d8f5b2cc6602828a8bc3ef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.085s (0.051s)	Loss 0.8514 (0.6985)	Prec@1  70.3 ( 75.5)	Prec@5  96.1 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b8837698e974dce70435ba43163dc41401b573c2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 0.6992	Avg Prec@1 75.58 %	Avg Prec@5 98.43 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 0.7337 (0.7337)	Prec@1  70.3 ( 70.3)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.7776	Avg Prec@1 73.14 %	Avg Prec@5 98.08 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/41124251238faad04f7ab2f8a46f725bab23b5ed.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.222s (0.222s)	Loss 0.7915 (0.7915)	Prec@1  72.7 ( 72.7)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2bd0ad2fb5f61b3ad1d1c5550f3fae60a39cc001.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.043s (0.046s)	Loss 0.6015 (0.6629)	Prec@1  78.9 ( 77.0)	Prec@5  98.4 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ad3dd0149f37bd898aedfb9572cefa05b615d88e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 0.6645	Avg Prec@1 76.95 %	Avg Prec@5 98.65 %

[EVAL Batch 000/079]	Time 0.169s (0.169s)	Loss 0.6915 (0.6915)	Prec@1  74.2 ( 74.2)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.7074	Avg Prec@1 75.54 %	Avg Prec@5 98.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ba0fb8f2ac346d50425c06be4ce1e6c1fbd6699c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.230s (0.230s)	Loss 0.6485 (0.6485)	Prec@1  75.0 ( 75.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c7b9f2e9f6e15d776e78944ed11ae3b12270d048.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.023s (0.050s)	Loss 0.6720 (0.6456)	Prec@1  77.3 ( 77.5)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/76eb7ff4d36a425f66b709e9dce73c9527825487.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.6376	Avg Prec@1 77.76 %	Avg Prec@5 98.77 %

[EVAL Batch 000/079]	Time 0.179s (0.179s)	Loss 0.5938 (0.5938)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6605	Avg Prec@1 77.39 %	Avg Prec@5 98.69 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b3490b8dd6e0974d4795a4f95bb3ae9dba2b375d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.217s (0.217s)	Loss 0.4988 (0.4988)	Prec@1  82.0 ( 82.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9cd79ad806ad78a7874bf6927bd9ced84d51c39e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.021s (0.050s)	Loss 0.5762 (0.5994)	Prec@1  81.2 ( 79.1)	Prec@5  99.2 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7d7c14598af3c94f344f493c9882dbcfc4a51c4d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.6025	Avg Prec@1 78.95 %	Avg Prec@5 98.87 %

[EVAL Batch 000/079]	Time 0.188s (0.188s)	Loss 0.6269 (0.6269)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6697	Avg Prec@1 76.89 %	Avg Prec@5 98.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/815b2430878ced9aee5c95f0cac1ec022de7459c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.218s (0.218s)	Loss 0.5176 (0.5176)	Prec@1  82.0 ( 82.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1446abc26964deb7dbd9985f352df36d43225ce6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.078s (0.050s)	Loss 0.5169 (0.5763)	Prec@1  85.2 ( 80.0)	Prec@5  98.4 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/166612b094be54037c8860832fbce21b6f0c847d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 0.5750	Avg Prec@1 80.03 %	Avg Prec@5 99.03 %

[EVAL Batch 000/079]	Time 0.315s (0.315s)	Loss 0.6137 (0.6137)	Prec@1  78.9 ( 78.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6773	Avg Prec@1 77.05 %	Avg Prec@5 98.68 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5d830183c4e2f8507e8aebfa65c0196a5a41081f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.218s (0.218s)	Loss 0.5310 (0.5310)	Prec@1  78.9 ( 78.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dbda303b50de5b3c6f16f4aab43dbc6a7f87f936.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.051s (0.050s)	Loss 0.5634 (0.5498)	Prec@1  78.9 ( 80.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7d0c29a2cd32bd490febe57d4d3917351cc49a9c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 18s	Avg loss 0.5540	Avg Prec@1 80.68 %	Avg Prec@5 99.10 %

[EVAL Batch 000/079]	Time 0.190s (0.190s)	Loss 0.6054 (0.6054)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6814	Avg Prec@1 76.60 %	Avg Prec@5 98.53 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2951ee37e5c6d51318b015496aa121e754b0d5ac.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.325s (0.325s)	Loss 0.5997 (0.5997)	Prec@1  75.0 ( 75.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9aa6a7be69f4728f920d53614fe8977fb3f58424.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.024s (0.048s)	Loss 0.4474 (0.5245)	Prec@1  88.3 ( 82.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6bcd84380781a51981db97a249aa8538f96f3f00.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.5288	Avg Prec@1 81.85 %	Avg Prec@5 99.14 %

[EVAL Batch 000/079]	Time 0.185s (0.185s)	Loss 0.5531 (0.5531)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7167	Avg Prec@1 76.06 %	Avg Prec@5 98.57 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0983c3b720d5bddefad0d31fd2770c7a2844a8e7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.239s (0.239s)	Loss 0.5088 (0.5088)	Prec@1  85.2 ( 85.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/da53e7d778307f15fb82655ec19de4537ec7c4c2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.051s)	Loss 0.5606 (0.5079)	Prec@1  82.0 ( 82.2)	Prec@5  99.2 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/714b020384037790607c0b1469da676286b081df.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.5091	Avg Prec@1 82.19 %	Avg Prec@5 99.25 %

[EVAL Batch 000/079]	Time 0.188s (0.188s)	Loss 0.5478 (0.5478)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6064	Avg Prec@1 79.51 %	Avg Prec@5 98.82 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c6785fe0ca9756ef1d7f80f19ef7d8719ab400da.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.227s (0.227s)	Loss 0.3782 (0.3782)	Prec@1  86.7 ( 86.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3539f152e7a609dd4c5dd633efff6f68b6c039ce.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.081s (0.052s)	Loss 0.5197 (0.4884)	Prec@1  81.2 ( 83.0)	Prec@5  99.2 ( 99.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c948ec3f921cec643aae23d91665224ce9aff40f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.4886	Avg Prec@1 82.90 %	Avg Prec@5 99.36 %

[EVAL Batch 000/079]	Time 0.178s (0.178s)	Loss 0.6180 (0.6180)	Prec@1  78.9 ( 78.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6479	Avg Prec@1 78.30 %	Avg Prec@5 98.63 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6ed6109b58d5c48865535374556463764c9d7378.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.236s (0.236s)	Loss 0.4985 (0.4985)	Prec@1  82.0 ( 82.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/63aa9c1de6ee4988410076d1c49a844a9ab57599.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.056s (0.052s)	Loss 0.4694 (0.4634)	Prec@1  80.5 ( 83.5)	Prec@5  99.2 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6db6ffb2144c25ca5dc455bef442b5ec6f5a4323.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.4651	Avg Prec@1 83.54 %	Avg Prec@5 99.46 %

[EVAL Batch 000/079]	Time 0.303s (0.303s)	Loss 0.6317 (0.6317)	Prec@1  79.7 ( 79.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6328	Avg Prec@1 78.50 %	Avg Prec@5 98.81 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5ff30d362df17b76a32957a8d0dd7da3ef915b00.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.236s (0.236s)	Loss 0.5739 (0.5739)	Prec@1  80.5 ( 80.5)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2c5c49fd232e2c48914c488073419f80a44811f5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.053s)	Loss 0.8093 (0.4486)	Prec@1  71.1 ( 84.2)	Prec@5 100.0 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e1f202f0500a970e9a422ecf226a56b3994f820e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.4499	Avg Prec@1 84.25 %	Avg Prec@5 99.47 %

[EVAL Batch 000/079]	Time 0.178s (0.178s)	Loss 0.5456 (0.5456)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 4s	Avg loss 0.6583	Avg Prec@1 78.39 %	Avg Prec@5 98.70 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/619544df1088d9000d9bcd1b4ac8ad94a6f51691.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.249s (0.249s)	Loss 0.3393 (0.3393)	Prec@1  85.9 ( 85.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e4ee36f4a179dea57d40c9787b2add88ce1c8748.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.048s)	Loss 0.4793 (0.4295)	Prec@1  82.0 ( 85.0)	Prec@5 100.0 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/39694823dbf9f1c3d79160c55629565623a0f14e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.4343	Avg Prec@1 84.91 %	Avg Prec@5 99.49 %

[EVAL Batch 000/079]	Time 0.211s (0.211s)	Loss 0.5570 (0.5570)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6125	Avg Prec@1 79.91 %	Avg Prec@5 98.85 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b5a72bad46d717ed29b72b4f4842d000f88712ff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.256s (0.256s)	Loss 0.4336 (0.4336)	Prec@1  85.9 ( 85.9)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/755bb97ed4adc279ab4bdaa8290ccce439e84f63.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.082s (0.058s)	Loss 0.2874 (0.4095)	Prec@1  91.4 ( 85.8)	Prec@5 100.0 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0d3db788b9b74dfebf14fc5c6f5b107b6b6c5346.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 21s	Avg loss 0.4151	Avg Prec@1 85.59 %	Avg Prec@5 99.49 %

[EVAL Batch 000/079]	Time 0.201s (0.201s)	Loss 0.6041 (0.6041)	Prec@1  80.5 ( 80.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6481	Avg Prec@1 78.64 %	Avg Prec@5 98.74 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cb8b39554ffb1912c79de57acdb1c0009e451f83.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.233s (0.233s)	Loss 0.5438 (0.5438)	Prec@1  82.8 ( 82.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0127221656cf8d18495e5c1710ba49ea277d77b7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.082s (0.053s)	Loss 0.4907 (0.3961)	Prec@1  83.6 ( 86.3)	Prec@5  98.4 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/573f460011a82d9de79a12946f2611a9dd952548.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.4025	Avg Prec@1 86.06 %	Avg Prec@5 99.53 %

[EVAL Batch 000/079]	Time 0.184s (0.184s)	Loss 0.6231 (0.6231)	Prec@1  80.5 ( 80.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6343	Avg Prec@1 79.28 %	Avg Prec@5 98.66 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6c30187580afb621e872e8802081e0af213884f5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.261s (0.261s)	Loss 0.2967 (0.2967)	Prec@1  89.1 ( 89.1)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c042e3c518cbc65f8fe60e63eeb86d9b44455603.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.053s)	Loss 0.3869 (0.3875)	Prec@1  84.4 ( 86.2)	Prec@5  99.2 ( 99.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/897276b8162bb21f417b35147421cda7a94097ae.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.3887	Avg Prec@1 86.27 %	Avg Prec@5 99.58 %

[EVAL Batch 000/079]	Time 0.199s (0.199s)	Loss 0.5913 (0.5913)	Prec@1  82.8 ( 82.8)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6375	Avg Prec@1 79.59 %	Avg Prec@5 98.82 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/961f17b9e7d625235dd1707636b380d0d88cb504.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.292s (0.292s)	Loss 0.5148 (0.5148)	Prec@1  85.2 ( 85.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/56b56aa8ff9ce3d45f086f19bcff4d69eee262e6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.063s (0.053s)	Loss 0.3223 (0.3635)	Prec@1  89.1 ( 87.4)	Prec@5 100.0 ( 99.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/73b08d8fe93671c96838f8ec5a2c21d1ee74b1b0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.3684	Avg Prec@1 87.30 %	Avg Prec@5 99.64 %

[EVAL Batch 000/079]	Time 0.199s (0.199s)	Loss 0.6287 (0.6287)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.5937	Avg Prec@1 80.07 %	Avg Prec@5 98.83 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d4b9b3f621c64aa5490bf05f668f484aaeee41b0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.289s (0.289s)	Loss 0.4282 (0.4282)	Prec@1  85.9 ( 85.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/325b3f34895b0d40a44512515b379a21d9255b3c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.052s)	Loss 0.4328 (0.3503)	Prec@1  85.9 ( 87.6)	Prec@5 100.0 ( 99.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e12e12ac8e7879a0dbb47477e131bc59471802de.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.3581	Avg Prec@1 87.36 %	Avg Prec@5 99.68 %

[EVAL Batch 000/079]	Time 0.301s (0.301s)	Loss 0.5750 (0.5750)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6277	Avg Prec@1 80.07 %	Avg Prec@5 98.80 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/076e0a0572051852fe0dbda21e61743ea9e03f4e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.278s (0.278s)	Loss 0.3251 (0.3251)	Prec@1  89.1 ( 89.1)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d1733b3a16bb9ef34896944d106e526451f5145c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.094s (0.052s)	Loss 0.2966 (0.3316)	Prec@1  91.4 ( 88.4)	Prec@5 100.0 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b6307b4d9687cb6a90fccbfab7653dd7b5014cbd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.3411	Avg Prec@1 88.09 %	Avg Prec@5 99.74 %

[EVAL Batch 000/079]	Time 0.189s (0.189s)	Loss 0.6774 (0.6774)	Prec@1  75.8 ( 75.8)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6029	Avg Prec@1 80.01 %	Avg Prec@5 98.69 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/834c0e2cca8da24c7f047f5bf722e1ca4e458c0d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.285s (0.285s)	Loss 0.4138 (0.4138)	Prec@1  85.2 ( 85.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9bc8e7ca84a052ceefaf76f18d1199cb7f256ec5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.079s (0.048s)	Loss 0.2300 (0.3166)	Prec@1  93.0 ( 89.0)	Prec@5 100.0 ( 99.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d3935cee2a73048a18f2414d658ce0129b31b912.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.3232	Avg Prec@1 88.73 %	Avg Prec@5 99.74 %

[EVAL Batch 000/079]	Time 0.210s (0.210s)	Loss 0.6021 (0.6021)	Prec@1  82.0 ( 82.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.6454	Avg Prec@1 79.76 %	Avg Prec@5 98.56 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/541e08640eee34deffa51d48d9110e9bb2ee6ea9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.368s (0.368s)	Loss 0.2961 (0.2961)	Prec@1  90.6 ( 90.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/56087917162b81da0418a6718e1742c76685749f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.083s (0.051s)	Loss 0.2655 (0.3090)	Prec@1  90.6 ( 89.1)	Prec@5 100.0 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e951b74c2ee54a6024c726582fdf00304f2364b4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.3142	Avg Prec@1 88.98 %	Avg Prec@5 99.75 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 0.6366 (0.6366)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6834	Avg Prec@1 79.27 %	Avg Prec@5 98.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ce220839909a7791bd8954805de7565cecfe71d2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 31 =====
=================

[TRAIN Batch 000/391]	Time 0.279s (0.279s)	Loss 0.3601 (0.3601)	Prec@1  84.4 ( 84.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e925dc5e8e79d49480ee88d03980b9e16d2d94b2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.027s (0.054s)	Loss 0.4802 (0.2969)	Prec@1  82.8 ( 89.4)	Prec@5  99.2 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1f5c8eb7756206d99a3aa3d5f7a8da5e76a06d3e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 21s	Avg loss 0.3040	Avg Prec@1 89.28 %	Avg Prec@5 99.77 %

[EVAL Batch 000/079]	Time 0.212s (0.212s)	Loss 0.5616 (0.5616)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6252	Avg Prec@1 80.35 %	Avg Prec@5 98.69 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5b1fdd42a37d627c5d11a4ee4651968cb02f13b6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 32 =====
=================

[TRAIN Batch 000/391]	Time 0.251s (0.251s)	Loss 0.3094 (0.3094)	Prec@1  87.5 ( 87.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/76424ae28dd781d1321f54249c82f1323f445dad.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.054s)	Loss 0.3051 (0.2851)	Prec@1  90.6 ( 89.9)	Prec@5  99.2 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9ee1110291061c2740cdb938b06e3201839579a1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.2945	Avg Prec@1 89.66 %	Avg Prec@5 99.79 %

[EVAL Batch 000/079]	Time 0.225s (0.225s)	Loss 0.6684 (0.6684)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6507	Avg Prec@1 80.35 %	Avg Prec@5 98.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ba8602fdf5a00c8d1ff04e843a852a762eb50f47.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 33 =====
=================

[TRAIN Batch 000/391]	Time 0.280s (0.280s)	Loss 0.3240 (0.3240)	Prec@1  85.2 ( 85.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ab9133f708d7ed8e9f6b788ecd21920728d7160a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.080s (0.053s)	Loss 0.3120 (0.2757)	Prec@1  88.3 ( 90.2)	Prec@5  99.2 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/20c95bd35aee1942034bb6d980b644018fbf6703.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.2829	Avg Prec@1 90.02 %	Avg Prec@5 99.83 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.6105 (0.6105)	Prec@1  82.8 ( 82.8)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6326	Avg Prec@1 81.30 %	Avg Prec@5 98.85 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d05eebf7a056b6f26c9d1d73952250575a41a4e8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 34 =====
=================

[TRAIN Batch 000/391]	Time 0.286s (0.286s)	Loss 0.2040 (0.2040)	Prec@1  93.8 ( 93.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5eafa6a4682e511d2f21e3b5f2555bdfeb762675.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.008s (0.053s)	Loss 0.3329 (0.2605)	Prec@1  89.1 ( 90.9)	Prec@5  99.2 ( 99.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/27545a51644eba54378060e0381680c20f5c3709.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.2691	Avg Prec@1 90.50 %	Avg Prec@5 99.81 %

[EVAL Batch 000/079]	Time 0.210s (0.210s)	Loss 0.5666 (0.5666)	Prec@1  85.2 ( 85.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6630	Avg Prec@1 80.82 %	Avg Prec@5 98.68 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/49fc5b97cbcbd190135d044f181c6946b67cfcd2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 35 =====
=================

[TRAIN Batch 000/391]	Time 0.259s (0.259s)	Loss 0.2619 (0.2619)	Prec@1  91.4 ( 91.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e11276bfb703d060870b01ae6ccfa233c0585365.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.051s)	Loss 0.2625 (0.2567)	Prec@1  91.4 ( 91.2)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0b96f327dee304c4f368ed1d4552d00e68aa406b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.2585	Avg Prec@1 91.01 %	Avg Prec@5 99.87 %

[EVAL Batch 000/079]	Time 0.261s (0.261s)	Loss 0.6660 (0.6660)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.7400	Avg Prec@1 79.79 %	Avg Prec@5 98.48 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b11c46573bb5e4b9e76c4ff4ebbbb90d3491361c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 36 =====
=================

[TRAIN Batch 000/391]	Time 0.257s (0.257s)	Loss 0.2180 (0.2180)	Prec@1  92.2 ( 92.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1945692400045c570d43023c71b2c195d6c298d1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.051s)	Loss 0.3148 (0.2428)	Prec@1  87.5 ( 91.5)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0589deb4d4c09ad7c16f191721cea085d78e23fb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.2523	Avg Prec@1 91.13 %	Avg Prec@5 99.86 %

[EVAL Batch 000/079]	Time 0.196s (0.196s)	Loss 0.5777 (0.5777)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.7023	Avg Prec@1 79.81 %	Avg Prec@5 98.68 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/42d5fd892f0ae120259a11ac6ad27ebeb134183a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 37 =====
=================

[TRAIN Batch 000/391]	Time 0.316s (0.316s)	Loss 0.2736 (0.2736)	Prec@1  91.4 ( 91.4)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5eee2aa5331a58f1bec734b4d7d83197ccdd7be8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.066s (0.049s)	Loss 0.3013 (0.2391)	Prec@1  92.2 ( 91.9)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0059cda56fcd43d4a1a75428326f5a1ef9ae48b9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 19s	Avg loss 0.2410	Avg Prec@1 91.60 %	Avg Prec@5 99.88 %

[EVAL Batch 000/079]	Time 0.188s (0.188s)	Loss 0.6505 (0.6505)	Prec@1  81.2 ( 81.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6605	Avg Prec@1 80.51 %	Avg Prec@5 98.86 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5ee9d1098e663b29c4e5b67c6ca52430dcb0e9e0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 38 =====
=================

[TRAIN Batch 000/391]	Time 0.272s (0.272s)	Loss 0.2682 (0.2682)	Prec@1  93.0 ( 93.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f331bf6569be61f45e6ae8ad1b1a95be8244f221.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.068s (0.052s)	Loss 0.2135 (0.2316)	Prec@1  91.4 ( 92.0)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/df567a5f93363e3b9a96aef628c2f927d89484b9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.2376	Avg Prec@1 91.76 %	Avg Prec@5 99.89 %

[EVAL Batch 000/079]	Time 0.212s (0.212s)	Loss 0.7322 (0.7322)	Prec@1  80.5 ( 80.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7507	Avg Prec@1 78.82 %	Avg Prec@5 98.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d37fea0eaa9bfb5153a05f11493d170cb5d9f12f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 39 =====
=================

[TRAIN Batch 000/391]	Time 0.265s (0.265s)	Loss 0.2072 (0.2072)	Prec@1  91.4 ( 91.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/43761b77e8b78254bcab63170126fb86612bf53c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.051s)	Loss 0.2906 (0.2133)	Prec@1  92.2 ( 92.4)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/045a01eb8bdabd02a69bcd98121e049318c073d1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.2229	Avg Prec@1 92.12 %	Avg Prec@5 99.92 %

[EVAL Batch 000/079]	Time 0.180s (0.180s)	Loss 0.5216 (0.5216)	Prec@1  84.4 ( 84.4)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6705	Avg Prec@1 80.41 %	Avg Prec@5 98.74 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bfb24515481c3ad06518f97811352b1cece6fee8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 40 =====
=================

[TRAIN Batch 000/391]	Time 0.306s (0.306s)	Loss 0.1709 (0.1709)	Prec@1  93.0 ( 93.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6e127e2c8fefd0038eec59aa288e1d305c115386.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.076s (0.052s)	Loss 0.2080 (0.2114)	Prec@1  93.8 ( 92.5)	Prec@5 100.0 ( 99.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1b99901b5903e4df0918897bfc83287fbce0d1d2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 20s	Avg loss 0.2176	Avg Prec@1 92.28 %	Avg Prec@5 99.89 %

[EVAL Batch 000/079]	Time 0.258s (0.258s)	Loss 0.6269 (0.6269)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6993	Avg Prec@1 80.17 %	Avg Prec@5 98.85 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/07ac079be6b1b64d49d137296a7b676798ed7af0.png" /></p>
</div>
</div>
<div class="cell markdown" id="aNSHshx-KLo2">
<p>Results without transforms.Normalize :</p>
<p>===============&gt; Total time 14s Avg loss 1.3972 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 50.11 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 92.97 %</p>
<p>===============&gt; Total time 3s Avg loss 1.3459 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 52.21 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 93.95 %</p>
<p>It seems like transforms.Normalize has a hugh effect on the
results</p>
<p>let's see that.</p>
<p>Results with transforms.Normalize:</p>
<p>===============&gt; Total time 20s Avg loss 1.0375 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 63.83 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 96.34 %</p>
<p>===============&gt; Total time 2s Avg loss 0.9621 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 66.87 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 96.92 %</p>
<p>Results of the previous experimentation (standardization) :</p>
<p>TRAIN ===============&gt; Total time 14s Avg loss 0.8629 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 69.82 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.78 %</p>
<p>EVAL ===============&gt; Total time 2s Avg loss 0.9919 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 65.65 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.12 %</p>
<p>The previous results was greater without data augmentation. The
stability of the learning is not good also, principaly on test,
(surement du au fait que les dataset de test n'est adapté (le center
crop ne capture pas les mêmes chose que le randomcrop))</p>
</div>
<section id="25-bonus--other-data-augmentation-methods"
class="cell markdown" id="RpGlFhlM17Ea">
<h3>25. Bonus : Other data augmentation methods</h3>
</section>
<div class="cell code" data-execution_count="23" id="AhCroPYb16EG">
<div class="sourceCode" id="cb504"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb504-1"><a href="#cb504-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb504-2"><a href="#cb504-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb504-3"><a href="#cb504-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb504-4"><a href="#cb504-4" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb504-5"><a href="#cb504-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb504-6"><a href="#cb504-6" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb504-7"><a href="#cb504-7" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb504-8"><a href="#cb504-8" aria-hidden="true" tabindex="-1"></a>            transforms.RandomCrop(<span class="dv">32</span>, padding<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb504-9"><a href="#cb504-9" aria-hidden="true" tabindex="-1"></a>            transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb504-10"><a href="#cb504-10" aria-hidden="true" tabindex="-1"></a>            transforms.RandomRotation(<span class="dv">15</span>),</span>
<span id="cb504-11"><a href="#cb504-11" aria-hidden="true" tabindex="-1"></a>            transforms.ColorJitter(brightness<span class="op">=</span><span class="fl">0.2</span>, contrast<span class="op">=</span><span class="fl">0.2</span>, saturation<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb504-12"><a href="#cb504-12" aria-hidden="true" tabindex="-1"></a>            transforms.RandomAffine(degrees<span class="op">=</span><span class="dv">0</span>, translate<span class="op">=</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>), shear<span class="op">=</span><span class="fl">0.2</span>, scale<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">1.1</span>)),</span>
<span id="cb504-13"><a href="#cb504-13" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb504-14"><a href="#cb504-14" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>] ,</span>
<span id="cb504-15"><a href="#cb504-15" aria-hidden="true" tabindex="-1"></a>                      std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])]))</span>
<span id="cb504-16"><a href="#cb504-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb504-17"><a href="#cb504-17" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb504-18"><a href="#cb504-18" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb504-19"><a href="#cb504-19" aria-hidden="true" tabindex="-1"></a>            transforms.CenterCrop(size<span class="op">=</span><span class="dv">28</span>),</span>
<span id="cb504-20"><a href="#cb504-20" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb504-21"><a href="#cb504-21" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>],</span>
<span id="cb504-22"><a href="#cb504-22" aria-hidden="true" tabindex="-1"></a>                      std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])]))</span>
<span id="cb504-23"><a href="#cb504-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb504-24"><a href="#cb504-24" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb504-25"><a href="#cb504-25" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb504-26"><a href="#cb504-26" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb504-27"><a href="#cb504-27" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb504-28"><a href="#cb504-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb504-29"><a href="#cb504-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="24"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="z-5v8BKYYcIe" data-outputId="3445db7b-8b73-4fb2-e78d-71422000dc8b">
<div class="sourceCode" id="cb505"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb505-1"><a href="#cb505-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, epochs <span class="op">=</span> <span class="dv">40</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.302s (0.302s)	Loss 2.2989 (2.2989)	Prec@1  12.5 ( 12.5)	Prec@5  52.3 ( 52.3)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/09b303c39a88be783191f551552d9aacac4ee613.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.020s (0.117s)	Loss 2.0582 (2.0694)	Prec@1  25.8 ( 23.2)	Prec@5  76.6 ( 74.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cdac017ca155b9a43931249303352c5767b88d4a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 1.9474	Avg Prec@1 28.36 %	Avg Prec@5 79.83 %

[EVAL Batch 000/079]	Time 0.344s (0.344s)	Loss 1.7456 (1.7456)	Prec@1  35.9 ( 35.9)	Prec@5  89.8 ( 89.8)

===============&gt; Total time 4s	Avg loss 1.6920	Avg Prec@1 37.88 %	Avg Prec@5 88.61 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f0c415f1685c3addc3e2f5200bc97aff33dc2d0f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.342s (0.342s)	Loss 1.7396 (1.7396)	Prec@1  39.1 ( 39.1)	Prec@5  88.3 ( 88.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/675c62b0c82778cd4fa56c3be6d37d1141a103a6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.089s (0.121s)	Loss 1.5371 (1.7035)	Prec@1  41.4 ( 38.2)	Prec@5  94.5 ( 87.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d3befe048137ae89de922e61bfde0e9a87504dfa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 46s	Avg loss 1.6438	Avg Prec@1 40.33 %	Avg Prec@5 89.09 %

[EVAL Batch 000/079]	Time 0.345s (0.345s)	Loss 1.5288 (1.5288)	Prec@1  44.5 ( 44.5)	Prec@5  90.6 ( 90.6)

===============&gt; Total time 3s	Avg loss 1.5496	Avg Prec@1 44.46 %	Avg Prec@5 89.54 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/131ece0f8716697ef4b4e788e8901d2856ca9528.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.329s (0.329s)	Loss 1.5799 (1.5799)	Prec@1  42.2 ( 42.2)	Prec@5  91.4 ( 91.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f2290223333cd000950ccf1ae69cd4bd2c566608.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.192s (0.119s)	Loss 1.4391 (1.5219)	Prec@1  50.8 ( 45.1)	Prec@5  94.5 ( 91.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/10a8d88879b19840a0e69de85869cacadc9945e7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 1.4803	Avg Prec@1 46.66 %	Avg Prec@5 91.87 %

[EVAL Batch 000/079]	Time 0.193s (0.193s)	Loss 1.3165 (1.3165)	Prec@1  59.4 ( 59.4)	Prec@5  93.0 ( 93.0)

===============&gt; Total time 4s	Avg loss 1.3825	Avg Prec@1 50.59 %	Avg Prec@5 93.10 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c0617ae3503105f885a0d91ef9467ea075347ec7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.346s (0.346s)	Loss 1.5179 (1.5179)	Prec@1  43.0 ( 43.0)	Prec@5  90.6 ( 90.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ff5cdf3127fd0f30a9daf2c25cc76d1ae1e1f118.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.020s (0.116s)	Loss 1.3978 (1.3760)	Prec@1  50.8 ( 50.9)	Prec@5  92.2 ( 93.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6d3e7f394ee4a18b8151a0f64a9753ac6396a703.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 1.3555	Avg Prec@1 51.56 %	Avg Prec@5 93.30 %

[EVAL Batch 000/079]	Time 0.168s (0.168s)	Loss 1.2377 (1.2377)	Prec@1  57.0 ( 57.0)	Prec@5  92.2 ( 92.2)

===============&gt; Total time 3s	Avg loss 1.2867	Avg Prec@1 54.91 %	Avg Prec@5 93.63 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/88864ad622c131f1cc2ba8ec1daa88d7b3203aae.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.575s (0.575s)	Loss 1.4659 (1.4659)	Prec@1  43.0 ( 43.0)	Prec@5  92.2 ( 92.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d92e4bdd1284ec8357f50cb8fd54d2ed069da5e5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.115s)	Loss 1.4495 (1.2861)	Prec@1  45.3 ( 54.1)	Prec@5  93.8 ( 94.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cc76375cd88330fd98bdcd7f78e0e05337159c9e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 1.2672	Avg Prec@1 54.76 %	Avg Prec@5 94.43 %

[EVAL Batch 000/079]	Time 0.167s (0.167s)	Loss 1.1460 (1.1460)	Prec@1  60.2 ( 60.2)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.1562	Avg Prec@1 58.32 %	Avg Prec@5 95.86 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f1655e229d8fa646f549a30b78f94b008e91042d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.553s (0.553s)	Loss 1.2704 (1.2704)	Prec@1  53.9 ( 53.9)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b361266efed43d1dc8b020d5cfbeb79cb0e2f528.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.189s (0.117s)	Loss 1.0707 (1.2082)	Prec@1  59.4 ( 57.1)	Prec@5  97.7 ( 94.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/87e245b23fc6f10fe2158d13f963207753c5ea7f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 1.1892	Avg Prec@1 57.79 %	Avg Prec@5 95.13 %

[EVAL Batch 000/079]	Time 0.171s (0.171s)	Loss 1.2400 (1.2400)	Prec@1  58.6 ( 58.6)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.2705	Avg Prec@1 55.38 %	Avg Prec@5 95.12 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/26b219ac1136949b02ed6bace417c5a54e4ef449.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.305s (0.305s)	Loss 1.1675 (1.1675)	Prec@1  62.5 ( 62.5)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2c52f08a264d9f79df4c28a61feb55bdd906fd92.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.114s)	Loss 1.1842 (1.1382)	Prec@1  61.7 ( 59.7)	Prec@5  92.2 ( 95.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9f8fbf327ce8196b95c2d3bfdc3adbd30cb4d7ca.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 1.1228	Avg Prec@1 60.10 %	Avg Prec@5 95.73 %

[EVAL Batch 000/079]	Time 0.174s (0.174s)	Loss 1.1516 (1.1516)	Prec@1  63.3 ( 63.3)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.1358	Avg Prec@1 60.83 %	Avg Prec@5 95.59 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e356a6d09d30a54ba8493a0e1fdb88f3707bb6d6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.332s (0.332s)	Loss 1.4153 (1.4153)	Prec@1  57.8 ( 57.8)	Prec@5  91.4 ( 91.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/009e22e1d4f942c1ece612a852923037f1276191.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.209s (0.114s)	Loss 1.2314 (1.1010)	Prec@1  57.8 ( 61.2)	Prec@5  96.9 ( 95.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9930b861e01173aadb708d6cd240e7c99800cc76.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 1.0791	Avg Prec@1 61.74 %	Avg Prec@5 95.95 %

[EVAL Batch 000/079]	Time 0.273s (0.273s)	Loss 1.0884 (1.0884)	Prec@1  58.6 ( 58.6)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 1.0167	Avg Prec@1 65.33 %	Avg Prec@5 96.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a44cf1864d6829bcb655d77b809b0db643b90a9e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.334s (0.334s)	Loss 1.0694 (1.0694)	Prec@1  64.8 ( 64.8)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e9b2b6ed79463d6ece3488200a2050875b20c7dd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.203s (0.115s)	Loss 1.0399 (1.0433)	Prec@1  64.1 ( 63.3)	Prec@5  95.3 ( 96.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aa8206f1912f65a10b3863b23af6fb6d88ecb253.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 1.0327	Avg Prec@1 63.88 %	Avg Prec@5 96.34 %

[EVAL Batch 000/079]	Time 0.169s (0.169s)	Loss 1.1838 (1.1838)	Prec@1  54.7 ( 54.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 1.0927	Avg Prec@1 61.88 %	Avg Prec@5 95.98 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e548eacffcebeafed237440fd6b450b92a6b4de2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.618s (0.618s)	Loss 1.0322 (1.0322)	Prec@1  71.1 ( 71.1)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1ece6a4a1ba1e6ee1c1f8dab782096aaf97a5bba.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.111s)	Loss 0.8335 (0.9924)	Prec@1  72.7 ( 65.1)	Prec@5  99.2 ( 96.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/71c1296539e92511763ecba11483f3a36b51f1fd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.9914	Avg Prec@1 65.21 %	Avg Prec@5 96.65 %

[EVAL Batch 000/079]	Time 0.179s (0.179s)	Loss 0.9489 (0.9489)	Prec@1  61.7 ( 61.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.9990	Avg Prec@1 64.02 %	Avg Prec@5 97.05 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2849727c91bf11452cd11545b0126237983b584c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.356s (0.356s)	Loss 1.1254 (1.1254)	Prec@1  60.9 ( 60.9)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/53ceb010b4e596818220336e726b00bd2beeb55e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.182s (0.116s)	Loss 0.7180 (0.9573)	Prec@1  78.9 ( 66.3)	Prec@5  96.1 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3ad90f19711632c6832141f2806cb90e479e269b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.9521	Avg Prec@1 66.72 %	Avg Prec@5 96.87 %

[EVAL Batch 000/079]	Time 0.173s (0.173s)	Loss 0.8493 (0.8493)	Prec@1  71.9 ( 71.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.8584	Avg Prec@1 71.19 %	Avg Prec@5 97.60 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e9c05ff5b7465266247cef6641aad38f5f59efc6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.326s (0.326s)	Loss 0.7998 (0.7998)	Prec@1  71.1 ( 71.1)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e4122cf688918997c62941123bc8ac28a86d0a6f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.127s (0.124s)	Loss 1.1798 (0.9250)	Prec@1  60.9 ( 67.6)	Prec@5  95.3 ( 97.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9943abaf385a49b02f304327322a02f58bbe56f1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 46s	Avg loss 0.9334	Avg Prec@1 67.52 %	Avg Prec@5 97.12 %

[EVAL Batch 000/079]	Time 0.164s (0.164s)	Loss 0.8843 (0.8843)	Prec@1  69.5 ( 69.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.9206	Avg Prec@1 67.24 %	Avg Prec@5 97.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/08c418bfd8833888ff6c6b4282b9a859e057d0e2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.342s (0.342s)	Loss 0.9492 (0.9492)	Prec@1  65.6 ( 65.6)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8cef3a0b900f3b9483b2b2bac1e34fe1e33ae884.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.011s (0.115s)	Loss 0.7373 (0.9031)	Prec@1  75.8 ( 68.3)	Prec@5  97.7 ( 97.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8e4977dff6d7e95ff87d33247f018e3fc436939d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.8985	Avg Prec@1 68.59 %	Avg Prec@5 97.26 %

[EVAL Batch 000/079]	Time 0.274s (0.274s)	Loss 0.7879 (0.7879)	Prec@1  75.8 ( 75.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.8388	Avg Prec@1 71.91 %	Avg Prec@5 97.78 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1d232a559ad14b22056252b24c561bd4b42995d8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.352s (0.352s)	Loss 0.9827 (0.9827)	Prec@1  70.3 ( 70.3)	Prec@5  93.8 ( 93.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/df14256e109186eeb39d10f5d0b5bdd87bef959f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.109s (0.115s)	Loss 0.8921 (0.8878)	Prec@1  68.8 ( 69.3)	Prec@5  96.9 ( 97.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8c31904d12833e70db062942b50117cc6dd98853.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.8817	Avg Prec@1 69.32 %	Avg Prec@5 97.45 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.9509 (0.9509)	Prec@1  68.0 ( 68.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.8771	Avg Prec@1 70.12 %	Avg Prec@5 97.28 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/146b9c2e81704e777c8f689760010df707e19701.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.475s (0.475s)	Loss 0.9728 (0.9728)	Prec@1  68.0 ( 68.0)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/66b9c1c4e31c2b8d225ec6804fa914d2da5b4699.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.054s (0.109s)	Loss 0.7661 (0.8529)	Prec@1  71.1 ( 69.9)	Prec@5  99.2 ( 97.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/694c374222fc0b832a24c02a7268812bc25f3996.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.8563	Avg Prec@1 70.02 %	Avg Prec@5 97.62 %

[EVAL Batch 000/079]	Time 0.177s (0.177s)	Loss 0.8930 (0.8930)	Prec@1  72.7 ( 72.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.9113	Avg Prec@1 69.36 %	Avg Prec@5 96.84 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8d4cbf938b53f9fb4cc4faec268f158acf3546ef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.346s (0.346s)	Loss 0.9759 (0.9759)	Prec@1  65.6 ( 65.6)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f87b0aeeb94e8f4259f469fab3eac9d0b3e25f8a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.185s (0.114s)	Loss 0.8118 (0.8397)	Prec@1  71.1 ( 70.9)	Prec@5  99.2 ( 97.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3c24c2f96f20324e9bcef0d4f1a6f33f30258e14.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.8343	Avg Prec@1 71.12 %	Avg Prec@5 97.63 %

[EVAL Batch 000/079]	Time 0.193s (0.193s)	Loss 0.7520 (0.7520)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7560	Avg Prec@1 74.66 %	Avg Prec@5 98.04 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/317dd28a4ce68f8eba4d5b47c1cbba2909f78830.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.356s (0.356s)	Loss 0.8175 (0.8175)	Prec@1  67.2 ( 67.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c09c2efb8c09ca4bc34cfcb70b7ec500b1fa2221.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.115s)	Loss 0.8619 (0.8144)	Prec@1  68.8 ( 72.0)	Prec@5  96.1 ( 97.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2f0ba6d0c99d8600d02cdc632391d4e90f55fda1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.8186	Avg Prec@1 71.63 %	Avg Prec@5 97.86 %

[EVAL Batch 000/079]	Time 0.268s (0.268s)	Loss 0.8323 (0.8323)	Prec@1  74.2 ( 74.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.8674	Avg Prec@1 70.66 %	Avg Prec@5 97.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d05b5be2822bb3bd749371dc7e0d2b5ede33c3b8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.359s (0.359s)	Loss 1.0916 (1.0916)	Prec@1  64.8 ( 64.8)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4248b0d987ba6bd5b130e3464d09c43c21070be6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.160s (0.115s)	Loss 0.8003 (0.8145)	Prec@1  73.4 ( 71.7)	Prec@5  99.2 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ddc44e625624dc37f9fc04c7917fd55e1f8a3ccc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.8088	Avg Prec@1 71.88 %	Avg Prec@5 97.93 %

[EVAL Batch 000/079]	Time 0.174s (0.174s)	Loss 0.7748 (0.7748)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.7853	Avg Prec@1 73.06 %	Avg Prec@5 98.05 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b76a670eb3133f29720eca98d68f60d4738d767f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.340s (0.340s)	Loss 0.7435 (0.7435)	Prec@1  75.0 ( 75.0)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/44696fb4f677db97f0e58a61307043b0c0e741e4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.050s (0.110s)	Loss 0.6827 (0.7963)	Prec@1  79.7 ( 72.2)	Prec@5  99.2 ( 98.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1c16bd42795ebb142739b543ca73d10a9c0c4bd5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7925	Avg Prec@1 72.42 %	Avg Prec@5 97.96 %

[EVAL Batch 000/079]	Time 0.192s (0.192s)	Loss 0.7039 (0.7039)	Prec@1  81.2 ( 81.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7180	Avg Prec@1 75.46 %	Avg Prec@5 98.20 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3aef13c0cd3fcff45d4265700fddfe39aab834ef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.389s (0.389s)	Loss 0.7788 (0.7788)	Prec@1  71.1 ( 71.1)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2f0efd6b6464eb5cb0a1f8ad9802990b20d91a3e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.069s (0.116s)	Loss 0.7444 (0.7744)	Prec@1  71.9 ( 72.8)	Prec@5  97.7 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/be072161386283ddbf2bb1c145c55ad498c0c715.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.7764	Avg Prec@1 72.79 %	Avg Prec@5 98.10 %

[EVAL Batch 000/079]	Time 0.186s (0.186s)	Loss 0.7266 (0.7266)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7726	Avg Prec@1 73.95 %	Avg Prec@5 97.59 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/33d1e90e43c97c1969de69c11a28c5886130461b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.355s (0.355s)	Loss 0.9514 (0.9514)	Prec@1  73.4 ( 73.4)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8f12312c061b96c410a686f1b87f012461582cec.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.190s (0.118s)	Loss 1.0688 (0.7763)	Prec@1  65.6 ( 72.7)	Prec@5  95.3 ( 98.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fe64c0aad5f6e22a4f39cc9cf3a03445cd7eaaaa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.7712	Avg Prec@1 72.97 %	Avg Prec@5 98.15 %

[EVAL Batch 000/079]	Time 0.194s (0.194s)	Loss 0.7805 (0.7805)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.8163	Avg Prec@1 72.71 %	Avg Prec@5 97.37 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5cb6a4134fbefd5a9e30b27c5ac17f483452bd49.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.358s (0.358s)	Loss 0.8539 (0.8539)	Prec@1  69.5 ( 69.5)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4dc09ed02fdf27f3ba4d26a5d748b2ea770c8466.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.115s)	Loss 0.7735 (0.7506)	Prec@1  75.0 ( 73.8)	Prec@5  98.4 ( 98.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8d651752c3073b23ab922cb973c64991203d72af.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.7489	Avg Prec@1 73.96 %	Avg Prec@5 98.15 %

[EVAL Batch 000/079]	Time 0.284s (0.284s)	Loss 0.6678 (0.6678)	Prec@1  82.0 ( 82.0)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.7269	Avg Prec@1 75.90 %	Avg Prec@5 98.06 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9db97c1b08c8178f57665b8fe75b12ee3bad40fb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.376s (0.376s)	Loss 0.6896 (0.6896)	Prec@1  78.9 ( 78.9)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/07f4f03af940cc3f29fb747e864b91e7a3135873.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.062s (0.115s)	Loss 0.7356 (0.7431)	Prec@1  74.2 ( 74.3)	Prec@5  98.4 ( 98.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6ea0778796ebb9a33f4709d26fcbd22ccc890495.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7448	Avg Prec@1 74.20 %	Avg Prec@5 98.16 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.7446 (0.7446)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.7588	Avg Prec@1 74.23 %	Avg Prec@5 97.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2f66e244319a54a98f012692d4c81dc3a86200ce.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.419s (0.419s)	Loss 0.7217 (0.7217)	Prec@1  74.2 ( 74.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5250d7c8c11d7c2053d61f785fd51707236d8294.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.110s (0.111s)	Loss 0.7318 (0.7326)	Prec@1  75.0 ( 74.3)	Prec@5  98.4 ( 98.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/49a5a4c5e5b86cf66057cac28abfab9df709c7a2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.7324	Avg Prec@1 74.43 %	Avg Prec@5 98.17 %

[EVAL Batch 000/079]	Time 0.183s (0.183s)	Loss 0.7318 (0.7318)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7269	Avg Prec@1 75.47 %	Avg Prec@5 98.12 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/20fa100dd1acfb83290aea22091d0355d140ae89.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.355s (0.355s)	Loss 0.8367 (0.8367)	Prec@1  71.9 ( 71.9)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7a5150bc941a926afe85c2c7c9af12e2cf558c0c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.115s)	Loss 0.7540 (0.7242)	Prec@1  74.2 ( 74.8)	Prec@5  97.7 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bcf9b025cc9e1f1577e3c1c770af0aa5e679d5ec.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.7223	Avg Prec@1 74.85 %	Avg Prec@5 98.31 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.6734 (0.6734)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7213	Avg Prec@1 75.29 %	Avg Prec@5 98.34 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1c4a83553b924b45e7dfcff925f659904f1635a0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.382s (0.382s)	Loss 0.8189 (0.8189)	Prec@1  75.8 ( 75.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/65cb1f130c16a4ca3a82315215fdd4a77746122e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.190s (0.117s)	Loss 0.7379 (0.7142)	Prec@1  75.0 ( 75.3)	Prec@5  98.4 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f684ce215af5e0df727d5987be1bce7482baebb3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.7149	Avg Prec@1 75.36 %	Avg Prec@5 98.33 %

[EVAL Batch 000/079]	Time 0.207s (0.207s)	Loss 0.6976 (0.6976)	Prec@1  78.9 ( 78.9)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.7058	Avg Prec@1 75.90 %	Avg Prec@5 98.46 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1bcfbb6b5636a88788c3b3166cc51f07fcca0f59.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.361s (0.361s)	Loss 0.6810 (0.6810)	Prec@1  73.4 ( 73.4)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46192f34d582da7b759b805994a23d2d9e147ce3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.115s)	Loss 0.7086 (0.6932)	Prec@1  76.6 ( 75.8)	Prec@5  99.2 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0ad8dbbadf73c71da3fcba1ab9c415b7e2358d20.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7062	Avg Prec@1 75.45 %	Avg Prec@5 98.33 %

[EVAL Batch 000/079]	Time 0.194s (0.194s)	Loss 0.7063 (0.7063)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.7114	Avg Prec@1 75.57 %	Avg Prec@5 98.12 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a337622e439e413663f800df2ba37ac1cd80abff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.806s (0.806s)	Loss 0.6338 (0.6338)	Prec@1  77.3 ( 77.3)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e036c344308fe83ea389378649904a191e9bfec9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.117s)	Loss 0.5737 (0.6870)	Prec@1  79.7 ( 76.1)	Prec@5  98.4 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5bea633d5a8ed4f1ff14067e6dc98b29e75923a0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6942	Avg Prec@1 75.98 %	Avg Prec@5 98.46 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.6686 (0.6686)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6814	Avg Prec@1 76.57 %	Avg Prec@5 98.48 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a7f7d25b66320d08838d8cfe943080272c72f017.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.579s (0.579s)	Loss 0.6200 (0.6200)	Prec@1  75.8 ( 75.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cea364bad6cdabf8ff79e3a9f1c313f814ba66cb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.113s)	Loss 0.5786 (0.6800)	Prec@1  77.3 ( 76.4)	Prec@5 100.0 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ebe3ce865102daa12676c349a71993218ed76553.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6893	Avg Prec@1 76.15 %	Avg Prec@5 98.45 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 0.7732 (0.7732)	Prec@1  77.3 ( 77.3)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.7276	Avg Prec@1 75.44 %	Avg Prec@5 98.18 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/64b36d03d762728945710ebf35567ee9df3c8464.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.346s (0.346s)	Loss 0.8783 (0.8783)	Prec@1  71.1 ( 71.1)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cd425c3cab279ec5f49bd9a936994cd2b7626faa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.199s (0.117s)	Loss 0.6808 (0.6759)	Prec@1  75.0 ( 76.6)	Prec@5  96.9 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/94a53174e968c199fd16bf4af96c2ab9f9e8b410.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.6873	Avg Prec@1 76.11 %	Avg Prec@5 98.45 %

[EVAL Batch 000/079]	Time 0.207s (0.207s)	Loss 0.6616 (0.6616)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6684	Avg Prec@1 77.43 %	Avg Prec@5 98.44 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7302886d5fb67a2fd4b86dc296dbe1da5f9c7226.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 31 =====
=================

[TRAIN Batch 000/391]	Time 0.386s (0.386s)	Loss 0.7680 (0.7680)	Prec@1  73.4 ( 73.4)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/69fca1c16b575dbaace273971b065eba55db9f01.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.206s (0.116s)	Loss 0.7520 (0.6683)	Prec@1  73.4 ( 77.1)	Prec@5  97.7 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ee3fc0876e79f954340306905fb714903af481e2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.6735	Avg Prec@1 76.76 %	Avg Prec@5 98.49 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.6418 (0.6418)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6624	Avg Prec@1 77.00 %	Avg Prec@5 98.32 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c2b6d7e06d554c4f825d2a31ef42c77b52586e71.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 32 =====
=================

[TRAIN Batch 000/391]	Time 0.345s (0.345s)	Loss 0.5816 (0.5816)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5c67874d44749efe52e75624707eda1d9f82173c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.186s (0.116s)	Loss 0.8742 (0.6662)	Prec@1  72.7 ( 76.9)	Prec@5  98.4 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b327d6bf8ecfd41c050a67cb2ac65d4c7eff3930.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6678	Avg Prec@1 76.76 %	Avg Prec@5 98.52 %

[EVAL Batch 000/079]	Time 0.189s (0.189s)	Loss 0.7190 (0.7190)	Prec@1  78.1 ( 78.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6816	Avg Prec@1 76.87 %	Avg Prec@5 98.42 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/13992189eba858fcc53012661450d357c217f3f9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 33 =====
=================

[TRAIN Batch 000/391]	Time 0.408s (0.408s)	Loss 0.6831 (0.6831)	Prec@1  74.2 ( 74.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e92cc02023ac1f035bd1d1af44c64b450dfbcf64.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.116s)	Loss 0.6364 (0.6553)	Prec@1  71.9 ( 77.2)	Prec@5  99.2 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ca6bb54ca21b629765b6dc23aa6ff1df6a9d8a4f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.6621	Avg Prec@1 76.92 %	Avg Prec@5 98.59 %

[EVAL Batch 000/079]	Time 0.191s (0.191s)	Loss 0.7124 (0.7124)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6379	Avg Prec@1 78.67 %	Avg Prec@5 98.55 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/840440e1fb60d5576bb1c995144a1ca245aac097.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 34 =====
=================

[TRAIN Batch 000/391]	Time 0.484s (0.484s)	Loss 0.8082 (0.8082)	Prec@1  74.2 ( 74.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0120cf325c3fd4a5e742fc93a38fa4126f26477a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.109s)	Loss 0.6129 (0.6454)	Prec@1  75.0 ( 77.4)	Prec@5 100.0 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b54230d56334241e776125a82a8b8baaeef722ee.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6574	Avg Prec@1 77.13 %	Avg Prec@5 98.60 %

[EVAL Batch 000/079]	Time 0.200s (0.200s)	Loss 0.7347 (0.7347)	Prec@1  73.4 ( 73.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6693	Avg Prec@1 77.02 %	Avg Prec@5 98.55 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3e326a612c3ca443da4f201f81dd506e760b1e42.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 35 =====
=================

[TRAIN Batch 000/391]	Time 0.397s (0.397s)	Loss 0.7853 (0.7853)	Prec@1  72.7 ( 72.7)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4c4d0c499924597a9d1f14a1df2c0fa7b48f4812.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.018s (0.116s)	Loss 0.6306 (0.6524)	Prec@1  75.8 ( 77.3)	Prec@5  99.2 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec51ac8b564a015282d559cb2804338de05e3426.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6580	Avg Prec@1 77.17 %	Avg Prec@5 98.61 %

[EVAL Batch 000/079]	Time 0.195s (0.195s)	Loss 0.7235 (0.7235)	Prec@1  81.2 ( 81.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6490	Avg Prec@1 77.73 %	Avg Prec@5 98.46 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ab342dc42d5bd01f46d807e8d114ed41c8b44e7c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 36 =====
=================

[TRAIN Batch 000/391]	Time 0.386s (0.386s)	Loss 0.5448 (0.5448)	Prec@1  80.5 ( 80.5)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e08af5caa5464cab1c0ca34eede179c9076d1eb5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.115s)	Loss 0.5929 (0.6329)	Prec@1  78.9 ( 78.0)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b6c84f81240d7749b8ce70bada2e9862760a4459.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.6461	Avg Prec@1 77.43 %	Avg Prec@5 98.65 %

[EVAL Batch 000/079]	Time 0.203s (0.203s)	Loss 0.8055 (0.8055)	Prec@1  75.8 ( 75.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.7024	Avg Prec@1 76.44 %	Avg Prec@5 98.05 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3d967ce50f5abb1790036af3ef9aa9478bda66d1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 37 =====
=================

[TRAIN Batch 000/391]	Time 0.465s (0.465s)	Loss 0.7120 (0.7120)	Prec@1  76.6 ( 76.6)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fc2a6cc52c3bd725e1e6dc0a5836a909767de252.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.186s (0.118s)	Loss 0.6147 (0.6430)	Prec@1  77.3 ( 77.3)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/09d768929d179de53c27e9b0df2a562da93371aa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 0.6426	Avg Prec@1 77.50 %	Avg Prec@5 98.66 %

[EVAL Batch 000/079]	Time 0.205s (0.205s)	Loss 0.7126 (0.7126)	Prec@1  76.6 ( 76.6)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6921	Avg Prec@1 76.89 %	Avg Prec@5 98.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/05bfb9db0ddaef9fd97695b00d633ddc3dc5e155.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 38 =====
=================

[TRAIN Batch 000/391]	Time 0.349s (0.349s)	Loss 0.7936 (0.7936)	Prec@1  74.2 ( 74.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/03f9f894b8f3c234bbc96414731aa8d08c72f331.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.055s (0.117s)	Loss 0.6162 (0.6274)	Prec@1  77.3 ( 78.1)	Prec@5  99.2 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c3b01c569039717b5b33b3ce12907c7748120828.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6272	Avg Prec@1 78.29 %	Avg Prec@5 98.75 %

[EVAL Batch 000/079]	Time 0.226s (0.226s)	Loss 0.6918 (0.6918)	Prec@1  83.6 ( 83.6)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6075	Avg Prec@1 79.30 %	Avg Prec@5 98.67 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/198bf934df1ec365b0681d70c9bf398e9f867511.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 39 =====
=================

[TRAIN Batch 000/391]	Time 0.428s (0.428s)	Loss 0.4402 (0.4402)	Prec@1  83.6 ( 83.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/049bfb813742893d09db25701ddf86d4a7e3269d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.066s (0.115s)	Loss 0.7499 (0.6197)	Prec@1  71.1 ( 78.7)	Prec@5 100.0 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ef4908eced5920b8a5ab51ff2d3b7ad4abac3aaa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6257	Avg Prec@1 78.41 %	Avg Prec@5 98.76 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.6594 (0.6594)	Prec@1  79.7 ( 79.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6107	Avg Prec@1 79.50 %	Avg Prec@5 98.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e86291f86af7da554bbce82f286949f3576d25c8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 40 =====
=================

[TRAIN Batch 000/391]	Time 0.560s (0.560s)	Loss 0.7852 (0.7852)	Prec@1  72.7 ( 72.7)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4e9dcac04bf4bc7cb41e70c3dc649e78a054342e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.114s)	Loss 0.6819 (0.6158)	Prec@1  74.2 ( 78.7)	Prec@5  97.7 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e57db1dae50d438b6da4a6e4a9334cd818610dcc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.6246	Avg Prec@1 78.36 %	Avg Prec@5 98.73 %

[EVAL Batch 000/079]	Time 0.195s (0.195s)	Loss 0.6872 (0.6872)	Prec@1  80.5 ( 80.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6974	Avg Prec@1 77.15 %	Avg Prec@5 98.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/45c831f1ab7ba6c69d760e1dfe036b44454343b1.png" /></p>
</div>
</div>
<div class="cell markdown" id="U0HYMYIGdoPC">
<p>===============&gt; Total time 80s Avg loss 1.3609 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 51.12 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 93.36 %</p>
<p>===============&gt; Total time 2s Avg loss 1.2629 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 54.14 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 94.74 %</p>
<p>Les résultats sont moins bien que la data augmentation précédente. On
en déduit qu'ajouter plus de transformations ne garantit pas de
meilleurs résultats. La data augmentation semble être un art autant
qu'une science, et ce qui fonctionne bien pour un modèle ou une
configuration peut ne pas fonctionner pour une autre.</p>
<p>Essayons d'ajouter des transformations de manière incrémentale.</p>
</div>
<div class="cell code" data-execution_count="25" id="Xlw1wqhGYfxl">
<div class="sourceCode" id="cb627"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb627-1"><a href="#cb627-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(batch_size, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb627-2"><a href="#cb627-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb627-3"><a href="#cb627-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This function loads the dataset and performs transformations on each</span></span>
<span id="cb627-4"><a href="#cb627-4" aria-hidden="true" tabindex="-1"></a><span class="co">    image (listed in `transform = ...`).</span></span>
<span id="cb627-5"><a href="#cb627-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb627-6"><a href="#cb627-6" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb627-7"><a href="#cb627-7" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb627-8"><a href="#cb627-8" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb627-9"><a href="#cb627-9" aria-hidden="true" tabindex="-1"></a>            transforms.RandomCrop(<span class="dv">32</span>, padding<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb627-10"><a href="#cb627-10" aria-hidden="true" tabindex="-1"></a>            transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb627-11"><a href="#cb627-11" aria-hidden="true" tabindex="-1"></a>            transforms.RandomAffine(degrees<span class="op">=</span><span class="dv">0</span>, translate<span class="op">=</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>), shear<span class="op">=</span><span class="fl">0.2</span>, scale<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">1.1</span>)),</span>
<span id="cb627-12"><a href="#cb627-12" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>] ,</span>
<span id="cb627-13"><a href="#cb627-13" aria-hidden="true" tabindex="-1"></a>                      std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])]))</span>
<span id="cb627-14"><a href="#cb627-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-15"><a href="#cb627-15" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> datasets.CIFAR10(PATH, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb627-16"><a href="#cb627-16" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb627-17"><a href="#cb627-17" aria-hidden="true" tabindex="-1"></a>            transforms.CenterCrop(size<span class="op">=</span><span class="dv">28</span>),</span>
<span id="cb627-18"><a href="#cb627-18" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb627-19"><a href="#cb627-19" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean <span class="op">=</span> [<span class="fl">0.491</span>, <span class="fl">0.482</span>, <span class="fl">0.447</span>],</span>
<span id="cb627-20"><a href="#cb627-20" aria-hidden="true" tabindex="-1"></a>                      std <span class="op">=</span> [<span class="fl">0.202</span>, <span class="fl">0.199</span>, <span class="fl">0.201</span>])]))</span>
<span id="cb627-21"><a href="#cb627-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-22"><a href="#cb627-22" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset,</span>
<span id="cb627-23"><a href="#cb627-23" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb627-24"><a href="#cb627-24" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_dataset,</span>
<span id="cb627-25"><a href="#cb627-25" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, pin_memory<span class="op">=</span>cuda, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb627-26"><a href="#cb627-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-27"><a href="#cb627-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loader, val_loader</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="26"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="qqHP3SzzgO59" data-outputId="fb0df18a-daa7-4cc5-a0fe-3986f8f205f5">
<div class="sourceCode" id="cb628"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb628-1"><a href="#cb628-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, epochs <span class="op">=</span> <span class="dv">40</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.589s (0.589s)	Loss 2.3055 (2.3055)	Prec@1   9.4 (  9.4)	Prec@5  49.2 ( 49.2)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/eaa9a29d5b343099318bbc348b886cc35d1f3187.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.180s (0.110s)	Loss 1.7365 (2.0495)	Prec@1  34.4 ( 24.3)	Prec@5  86.7 ( 75.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a51512dceac786344f84274cbc775213cbb48f15.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 1.9175	Avg Prec@1 29.33 %	Avg Prec@5 80.77 %

[EVAL Batch 000/079]	Time 0.168s (0.168s)	Loss 1.8616 (1.8616)	Prec@1  28.1 ( 28.1)	Prec@5  80.5 ( 80.5)

===============&gt; Total time 2s	Avg loss 1.7958	Avg Prec@1 34.70 %	Avg Prec@5 83.81 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/525addcac9e9de89e6c06af606e1dbd8f0d2dbb9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.349s (0.349s)	Loss 1.7126 (1.7126)	Prec@1  32.8 ( 32.8)	Prec@5  82.0 ( 82.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a0f681d2942223814523b4d2e0e94cac14bad854.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.159s (0.111s)	Loss 1.5346 (1.6211)	Prec@1  48.4 ( 40.7)	Prec@5  97.7 ( 89.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d92522e82f2ab649af3b2b136401f298d19a48bf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 1.5730	Avg Prec@1 42.69 %	Avg Prec@5 90.43 %

[EVAL Batch 000/079]	Time 0.252s (0.252s)	Loss 1.5080 (1.5080)	Prec@1  52.3 ( 52.3)	Prec@5  89.8 ( 89.8)

===============&gt; Total time 3s	Avg loss 1.5294	Avg Prec@1 47.07 %	Avg Prec@5 91.39 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3ab50afa0df92147f42b5380ce0f296414b3836c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.316s (0.316s)	Loss 1.5048 (1.5048)	Prec@1  43.8 ( 43.8)	Prec@5  86.7 ( 86.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6453d26fdf89e3384f95d4d487fd5c6d9e1b3990.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.157s (0.103s)	Loss 1.4660 (1.4232)	Prec@1  47.7 ( 48.9)	Prec@5  89.8 ( 92.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ee06c2a99d3d42618fc10718dd63159e64f521a0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 1.3835	Avg Prec@1 50.40 %	Avg Prec@5 92.99 %

[EVAL Batch 000/079]	Time 0.170s (0.170s)	Loss 1.2861 (1.2861)	Prec@1  55.5 ( 55.5)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.2766	Avg Prec@1 54.49 %	Avg Prec@5 95.11 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9b107e60167914049f1e60cf45dfa1980c66e071.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.305s (0.305s)	Loss 1.1652 (1.1652)	Prec@1  61.7 ( 61.7)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4de5476146bc1658541d7a69649203874d3cb9e7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.109s)	Loss 1.2053 (1.2601)	Prec@1  60.2 ( 54.6)	Prec@5  93.8 ( 94.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7d4ef669b0b32822786551d0a8c75a4be4a2cde8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.2269	Avg Prec@1 55.92 %	Avg Prec@5 94.80 %

[EVAL Batch 000/079]	Time 0.168s (0.168s)	Loss 1.1723 (1.1723)	Prec@1  58.6 ( 58.6)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.1454	Avg Prec@1 59.41 %	Avg Prec@5 95.73 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/60be7a44f04d6308518a69f2e508739bcfdb0bde.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.310s (0.310s)	Loss 1.0864 (1.0864)	Prec@1  57.8 ( 57.8)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c0162bfb872481a2c227b43a5c321fd4d9ebe1ad.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.111s)	Loss 1.0546 (1.1515)	Prec@1  58.6 ( 59.1)	Prec@5  96.9 ( 95.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/882b97ac9350a6991cc21a50709d48f6c8716561.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.1251	Avg Prec@1 60.11 %	Avg Prec@5 95.82 %

[EVAL Batch 000/079]	Time 0.183s (0.183s)	Loss 1.1272 (1.1272)	Prec@1  63.3 ( 63.3)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 3s	Avg loss 1.2789	Avg Prec@1 53.83 %	Avg Prec@5 95.00 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/58b5c24853a4ac34063d389d1f8da63b0dda1c98.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.371s (0.371s)	Loss 1.3005 (1.3005)	Prec@1  53.1 ( 53.1)	Prec@5  93.0 ( 93.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/06e079515fcecab049da3e3d2baa23e487fe7770.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.194s (0.106s)	Loss 0.9662 (1.0501)	Prec@1  68.8 ( 63.2)	Prec@5  98.4 ( 96.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7d396d34c7527accd1a9e9fb636f725856c4ca17.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.0319	Avg Prec@1 63.84 %	Avg Prec@5 96.56 %

[EVAL Batch 000/079]	Time 0.184s (0.184s)	Loss 1.1245 (1.1245)	Prec@1  56.2 ( 56.2)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 1.0431	Avg Prec@1 63.49 %	Avg Prec@5 96.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3897a05414b072c1a6a2ec4e440f02f893e19330.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.321s (0.321s)	Loss 1.0289 (1.0289)	Prec@1  63.3 ( 63.3)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ce9eeb27ae45cbc9962ff528926fc3655d8fa0ec.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.036s (0.111s)	Loss 0.8701 (0.9760)	Prec@1  68.0 ( 65.6)	Prec@5  96.9 ( 96.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/669104ce7a238837496025ed41894f1227afbfac.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.9617	Avg Prec@1 65.97 %	Avg Prec@5 96.91 %

[EVAL Batch 000/079]	Time 0.179s (0.179s)	Loss 1.1382 (1.1382)	Prec@1  58.6 ( 58.6)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.1126	Avg Prec@1 62.35 %	Avg Prec@5 97.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/447201ebf5cd5930af65935c1286b95f6664ee72.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.341s (0.341s)	Loss 1.1315 (1.1315)	Prec@1  60.9 ( 60.9)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c42c77319dd39a126d79a1efcfd35a797f0f4328.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.067s (0.110s)	Loss 0.9351 (0.9175)	Prec@1  64.1 ( 67.8)	Prec@5  96.9 ( 97.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c9d0729e29b8daf3388f09a30cf76a72f10d6862.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.9079	Avg Prec@1 68.27 %	Avg Prec@5 97.26 %

[EVAL Batch 000/079]	Time 0.174s (0.174s)	Loss 0.8312 (0.8312)	Prec@1  71.9 ( 71.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.8851	Avg Prec@1 69.41 %	Avg Prec@5 97.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/23d70bcad15bec243d10aff0644c42327350bfaf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.539s (0.539s)	Loss 0.6298 (0.6298)	Prec@1  74.2 ( 74.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fbafdd96de2b584a351bc03c3896ef8845b8b4e9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.192s (0.108s)	Loss 0.7366 (0.8539)	Prec@1  71.1 ( 70.2)	Prec@5  98.4 ( 97.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cc4cdaddd299754473ff1ba35ef3c3af6cd0dd2e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.9360	Avg Prec@1 68.19 %	Avg Prec@5 96.82 %

[EVAL Batch 000/079]	Time 0.179s (0.179s)	Loss 0.8854 (0.8854)	Prec@1  73.4 ( 73.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.8997	Avg Prec@1 69.24 %	Avg Prec@5 97.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bb82c19391d6a37773a476b70868d719036d3b97.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.314s (0.314s)	Loss 0.8881 (0.8881)	Prec@1  68.8 ( 68.8)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2310e281c4856de234de7cc97346f5a7a3898f3a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.200s (0.111s)	Loss 0.7305 (0.8600)	Prec@1  75.0 ( 70.1)	Prec@5 100.0 ( 97.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b0895d67a77602b609baf28cd3d5bab554c1d0aa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.8484	Avg Prec@1 70.32 %	Avg Prec@5 97.66 %

[EVAL Batch 000/079]	Time 0.356s (0.356s)	Loss 0.9763 (0.9763)	Prec@1  64.8 ( 64.8)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.9473	Avg Prec@1 67.92 %	Avg Prec@5 96.67 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/93cada0ef97e5985420ce83a392a2601c005d24e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.337s (0.337s)	Loss 1.0357 (1.0357)	Prec@1  64.8 ( 64.8)	Prec@5  95.3 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/68de3b30dd2478e8a63cc4e53a4640aa932cbeb2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.134s (0.104s)	Loss 0.6377 (0.8030)	Prec@1  76.6 ( 71.8)	Prec@5  96.1 ( 97.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3e11770b4597549406e3f8c06129a563ef899b53.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7953	Avg Prec@1 72.24 %	Avg Prec@5 97.88 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.8946 (0.8946)	Prec@1  71.9 ( 71.9)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 0.8958	Avg Prec@1 69.60 %	Avg Prec@5 97.41 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5c4a3303337b56d76e73937f02fc6e92fe570d43.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.338s (0.338s)	Loss 0.6827 (0.6827)	Prec@1  76.6 ( 76.6)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3e8a045bd6b3b06819aa6afa3fddc10850318819.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.110s (0.112s)	Loss 1.0361 (0.7710)	Prec@1  68.0 ( 73.0)	Prec@5  94.5 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bac4459587726f34aacdb69b4f040389eafa080c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7670	Avg Prec@1 73.08 %	Avg Prec@5 98.09 %

[EVAL Batch 000/079]	Time 0.181s (0.181s)	Loss 0.7953 (0.7953)	Prec@1  71.1 ( 71.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7957	Avg Prec@1 72.60 %	Avg Prec@5 97.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4c22de8c28a7c9ed27602109895c2896360ca137.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.346s (0.346s)	Loss 0.6000 (0.6000)	Prec@1  75.8 ( 75.8)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fa20c5b9c644740afb84fc3eeb9e8fad92e9c099.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.110s)	Loss 0.7145 (0.7367)	Prec@1  74.2 ( 74.4)	Prec@5  96.1 ( 98.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4724810bd89a8a7050a5e4d210dd57cd44d7ace7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7372	Avg Prec@1 74.28 %	Avg Prec@5 98.20 %

[EVAL Batch 000/079]	Time 0.175s (0.175s)	Loss 0.8063 (0.8063)	Prec@1  75.0 ( 75.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.7690	Avg Prec@1 73.65 %	Avg Prec@5 97.99 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f13663978580e1e74efd50dc337de64375955759.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.437s (0.437s)	Loss 0.5710 (0.5710)	Prec@1  82.8 ( 82.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/83a28d6076e81df4806842562ebc1fe92980bf7d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.032s (0.105s)	Loss 0.8614 (0.7116)	Prec@1  70.3 ( 75.1)	Prec@5  96.1 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dd6fdd2b6d01ccfef93d42659a57a7831c8be29c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7067	Avg Prec@1 75.47 %	Avg Prec@5 98.33 %

[EVAL Batch 000/079]	Time 0.201s (0.201s)	Loss 0.7167 (0.7167)	Prec@1  75.0 ( 75.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7639	Avg Prec@1 73.99 %	Avg Prec@5 98.27 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d00b009a58d0e7c5ba6df7d5e2475e782b058084.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.334s (0.334s)	Loss 0.7090 (0.7090)	Prec@1  76.6 ( 76.6)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d0dbaafdf3e53c2a56c74ee049442f5f3748168d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.056s (0.111s)	Loss 0.7600 (0.6889)	Prec@1  74.2 ( 75.8)	Prec@5  98.4 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/846c4153cbec07060f785b8b23e214d6d812015c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6886	Avg Prec@1 75.99 %	Avg Prec@5 98.49 %

[EVAL Batch 000/079]	Time 0.376s (0.376s)	Loss 0.6914 (0.6914)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.7229	Avg Prec@1 75.21 %	Avg Prec@5 98.29 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/42722c4ab421c18855841f50cadf359f6c5d36b1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.321s (0.321s)	Loss 0.6805 (0.6805)	Prec@1  77.3 ( 77.3)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d07e7bf455ca93235681d4aabc39acc9ff13f409.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.112s)	Loss 0.6267 (0.6661)	Prec@1  78.1 ( 76.5)	Prec@5  98.4 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec2ced8bb91418f70654c7388986017831624c06.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6663	Avg Prec@1 76.70 %	Avg Prec@5 98.50 %

[EVAL Batch 000/079]	Time 0.185s (0.185s)	Loss 0.5883 (0.5883)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6809	Avg Prec@1 76.34 %	Avg Prec@5 98.42 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b0f3ccbfdc2b4914a8bd7ac28036b9fb06744da4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.550s (0.550s)	Loss 0.6843 (0.6843)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ae83d4d6127664bf701735b45e033e7746922364.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.145s (0.113s)	Loss 0.6057 (0.6603)	Prec@1  76.6 ( 77.3)	Prec@5  96.9 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1ce7f7ff445f4aa9e9e15b3c7a501a876cb192b1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.6580	Avg Prec@1 77.37 %	Avg Prec@5 98.62 %

[EVAL Batch 000/079]	Time 0.186s (0.186s)	Loss 0.6566 (0.6566)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6799	Avg Prec@1 76.83 %	Avg Prec@5 98.56 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bbb4d980de78c89998d757994cb2cec985ef00c5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.361s (0.361s)	Loss 0.5305 (0.5305)	Prec@1  80.5 ( 80.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d6a751000738cf99ad392ec7988a4271dbd6bb59.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.111s)	Loss 0.5477 (0.6327)	Prec@1  77.3 ( 77.8)	Prec@5 100.0 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5f30e35bb14e8602fd2e8785ab9288eb7a7e49b5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.6347	Avg Prec@1 77.89 %	Avg Prec@5 98.65 %

[EVAL Batch 000/079]	Time 0.191s (0.191s)	Loss 0.8028 (0.8028)	Prec@1  75.0 ( 75.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7207	Avg Prec@1 75.45 %	Avg Prec@5 98.16 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec082a4a09bfce264ce9cb24d5a372de3a75b1e4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.343s (0.343s)	Loss 0.6294 (0.6294)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2a3a8360526425584a2025a7c72052560204caa9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.114s (0.110s)	Loss 0.5563 (0.6271)	Prec@1  78.9 ( 78.4)	Prec@5  98.4 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b7343397bd4cdbbf1be3bb6cdaff5096e0b55e72.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6258	Avg Prec@1 78.33 %	Avg Prec@5 98.74 %

[EVAL Batch 000/079]	Time 0.183s (0.183s)	Loss 0.6911 (0.6911)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7264	Avg Prec@1 75.04 %	Avg Prec@5 98.21 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2107cc391206957194d21e3f5cf33977b901fa6b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.589s (0.589s)	Loss 0.5940 (0.5940)	Prec@1  82.8 ( 82.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b91b79d614713b299b212bdb425e3f6c4c95bf52.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.107s)	Loss 0.6058 (0.6016)	Prec@1  85.2 ( 79.2)	Prec@5  97.7 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/35dae1f954b92c16070523e72d608706df440e73.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6107	Avg Prec@1 78.80 %	Avg Prec@5 98.72 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 0.7013 (0.7013)	Prec@1  75.0 ( 75.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7074	Avg Prec@1 76.42 %	Avg Prec@5 98.17 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9815b2d2270522cc913be0ee7d31fb9e27c2471e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.351s (0.351s)	Loss 0.5773 (0.5773)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1a741d2bbf28157140e28424aef2051810463109.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.109s)	Loss 0.4609 (0.5892)	Prec@1  83.6 ( 79.4)	Prec@5  98.4 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/656c437c370caba3fe12147d78d0027fb7352bf2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5924	Avg Prec@1 79.24 %	Avg Prec@5 98.81 %

[EVAL Batch 000/079]	Time 0.196s (0.196s)	Loss 0.6485 (0.6485)	Prec@1  83.6 ( 83.6)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.6484	Avg Prec@1 78.21 %	Avg Prec@5 98.45 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bb85bd8d8561506c037f655cd357b8405df58756.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.342s (0.342s)	Loss 0.6000 (0.6000)	Prec@1  79.7 ( 79.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e1bde76b622bd7545e0f97db500a470cc0738f75.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.103s)	Loss 0.5446 (0.5754)	Prec@1  81.2 ( 79.9)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/119271a412b8f5d6b81ad1049bc07ff5842f4de5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5763	Avg Prec@1 79.87 %	Avg Prec@5 98.98 %

[EVAL Batch 000/079]	Time 0.181s (0.181s)	Loss 0.6586 (0.6586)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6689	Avg Prec@1 77.32 %	Avg Prec@5 98.56 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c21c1f15ab563ddf0adbcaf4456855b0d5c44bf2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.371s (0.371s)	Loss 0.5082 (0.5082)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e83f5639db5eab9755ecbfee3e83c3edbfba407d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.110s)	Loss 0.6641 (0.5713)	Prec@1  76.6 ( 80.3)	Prec@5 100.0 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/23f67359bd0f2d71e1e53ea501c9d9d45e615866.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5746	Avg Prec@1 80.09 %	Avg Prec@5 98.94 %

[EVAL Batch 000/079]	Time 0.207s (0.207s)	Loss 0.7400 (0.7400)	Prec@1  78.9 ( 78.9)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.6879	Avg Prec@1 76.50 %	Avg Prec@5 98.51 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b5f29dd4e1e046d1a76251966b8f944cf4cee0e5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.357s (0.357s)	Loss 0.5799 (0.5799)	Prec@1  80.5 ( 80.5)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/77a73fd6d0ee040df24a385348b8146d074bba32.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.111s)	Loss 0.4901 (0.5602)	Prec@1  82.0 ( 80.4)	Prec@5 100.0 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fb578de589a15970052d6461cafdd787f61b59a2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5606	Avg Prec@1 80.48 %	Avg Prec@5 99.02 %

[EVAL Batch 000/079]	Time 0.212s (0.212s)	Loss 0.6797 (0.6797)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6724	Avg Prec@1 77.62 %	Avg Prec@5 98.66 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/54b598f23a6b992df52c31d51b50d888bf3c84fb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.374s (0.374s)	Loss 0.6531 (0.6531)	Prec@1  82.8 ( 82.8)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/443cbef82bde819dff1043b8d16490ec131b5541.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.019s (0.106s)	Loss 0.5770 (0.5505)	Prec@1  79.7 ( 81.0)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/70c46bb12a39f429629b50b9ad5707b6579c58b9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5539	Avg Prec@1 80.90 %	Avg Prec@5 98.99 %

[EVAL Batch 000/079]	Time 0.228s (0.228s)	Loss 0.7280 (0.7280)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7299	Avg Prec@1 75.92 %	Avg Prec@5 98.19 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c5a5828ac15c8a7d8afcf0f0459da7df3c9f36f0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.364s (0.364s)	Loss 0.5173 (0.5173)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2301437d629b2d6ead4c5cdea2354b6ea9064d12.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.070s (0.111s)	Loss 0.6677 (0.5413)	Prec@1  78.1 ( 81.2)	Prec@5  99.2 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1e7b3d4eeced91dcfeacf4c4ce0ca3072ed53bb7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5438	Avg Prec@1 81.14 %	Avg Prec@5 99.12 %

[EVAL Batch 000/079]	Time 0.189s (0.189s)	Loss 0.6369 (0.6369)	Prec@1  82.0 ( 82.0)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6148	Avg Prec@1 79.23 %	Avg Prec@5 98.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/45da53dbcfbaa8242d1089cc88ce3db9ee07b2c0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.404s (0.404s)	Loss 0.5231 (0.5231)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e4904c43265c509312d90d159aec814d10fdb8a6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.110s)	Loss 0.4457 (0.5286)	Prec@1  85.9 ( 81.5)	Prec@5 100.0 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/64e18633910ab9ee515dc2f2b19da9a7b29ff67c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5320	Avg Prec@1 81.38 %	Avg Prec@5 99.09 %

[EVAL Batch 000/079]	Time 0.192s (0.192s)	Loss 0.6156 (0.6156)	Prec@1  81.2 ( 81.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6100	Avg Prec@1 79.40 %	Avg Prec@5 98.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6b2fda2251aefbffc002b60c2caa79f09d835b91.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.347s (0.347s)	Loss 0.4736 (0.4736)	Prec@1  85.2 ( 85.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ad7ad546f1ed8bf55eef42b30a44fe27c56730bf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.105s)	Loss 0.6250 (0.5215)	Prec@1  77.3 ( 82.1)	Prec@5  98.4 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fcf2153a45203285b9f6d616c83c06413df33f89.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5267	Avg Prec@1 81.92 %	Avg Prec@5 99.10 %

[EVAL Batch 000/079]	Time 0.194s (0.194s)	Loss 0.6484 (0.6484)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6751	Avg Prec@1 77.45 %	Avg Prec@5 98.36 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/36f469023b7186b311a8ac10a6a766319082a197.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.370s (0.370s)	Loss 0.8259 (0.8259)	Prec@1  68.0 ( 68.0)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a9542a0209ce12c57a32a9d8e92aa9d5516b4fca.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.019s (0.110s)	Loss 0.5198 (0.5203)	Prec@1  79.7 ( 82.0)	Prec@5  99.2 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/744fe5e3ee74f8b21d7a09d621eb025bfe228c97.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5196	Avg Prec@1 81.96 %	Avg Prec@5 99.17 %

[EVAL Batch 000/079]	Time 0.204s (0.204s)	Loss 0.5730 (0.5730)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6156	Avg Prec@1 79.39 %	Avg Prec@5 98.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/169f98cda30ef350089a0d628ed5937fe4a04023.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.350s (0.350s)	Loss 0.4580 (0.4580)	Prec@1  88.3 ( 88.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3c84575c932c0a7803d30070089ef08674f9fa0c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.165s (0.111s)	Loss 0.3544 (0.5030)	Prec@1  87.5 ( 82.6)	Prec@5 100.0 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ae6b9ffba0ad90a7a73ac35eff000588597402f4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5065	Avg Prec@1 82.50 %	Avg Prec@5 99.17 %

[EVAL Batch 000/079]	Time 0.193s (0.193s)	Loss 0.6726 (0.6726)	Prec@1  82.0 ( 82.0)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 3s	Avg loss 0.6249	Avg Prec@1 79.17 %	Avg Prec@5 98.57 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/442c01bfb1ac55d65513030196b3715a17f783ff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 31 =====
=================

[TRAIN Batch 000/391]	Time 0.678s (0.678s)	Loss 0.4549 (0.4549)	Prec@1  78.9 ( 78.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8795bdfa04e5851851ffa29ca8cd3b94fd29ea0a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.107s)	Loss 0.5405 (0.4909)	Prec@1  81.2 ( 83.0)	Prec@5  97.7 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/de876243c0f46b0c550be5da57d4250c670e822c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.4995	Avg Prec@1 82.57 %	Avg Prec@5 99.16 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.9036 (0.9036)	Prec@1  75.0 ( 75.0)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.8660	Avg Prec@1 73.02 %	Avg Prec@5 97.61 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dc43716ecc6bf20fd53554c9b642304ae5b0a407.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 32 =====
=================

[TRAIN Batch 000/391]	Time 0.348s (0.348s)	Loss 0.7732 (0.7732)	Prec@1  79.7 ( 79.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9d80b9764eca9223f1492246409d3a1804f8179d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.079s (0.110s)	Loss 0.4687 (0.4905)	Prec@1  84.4 ( 83.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dabf72320983701c3c3c6797364a43029be10b02.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.4942	Avg Prec@1 82.82 %	Avg Prec@5 99.20 %

[EVAL Batch 000/079]	Time 0.359s (0.359s)	Loss 0.7147 (0.7147)	Prec@1  78.9 ( 78.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6460	Avg Prec@1 78.54 %	Avg Prec@5 98.51 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46feb52e85e6e47fc948910f7e722798fc08b631.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 33 =====
=================

[TRAIN Batch 000/391]	Time 0.403s (0.403s)	Loss 0.6112 (0.6112)	Prec@1  76.6 ( 76.6)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/350caccc8f754264b12b2c7b30be6aca69b82ee3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.350s (0.113s)	Loss 0.5248 (0.4847)	Prec@1  82.0 ( 83.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/56a30e22b86e36b3795a65f178357d824df3b575.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.4878	Avg Prec@1 82.99 %	Avg Prec@5 99.24 %

[EVAL Batch 000/079]	Time 0.196s (0.196s)	Loss 0.6525 (0.6525)	Prec@1  83.6 ( 83.6)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.6691	Avg Prec@1 78.44 %	Avg Prec@5 98.56 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/72a7d5b12b44326669c66ca85817fbd25af54a6e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 34 =====
=================

[TRAIN Batch 000/391]	Time 0.446s (0.446s)	Loss 0.4408 (0.4408)	Prec@1  85.9 ( 85.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/42a74521cb5820736941213bce5e9d5e69e19600.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.111s)	Loss 0.4812 (0.4731)	Prec@1  83.6 ( 83.5)	Prec@5 100.0 ( 99.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/94aa2e8e67c63b0d93aeb58d4d38fe2196c98fdf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.4755	Avg Prec@1 83.41 %	Avg Prec@5 99.33 %

[EVAL Batch 000/079]	Time 0.196s (0.196s)	Loss 0.5878 (0.5878)	Prec@1  83.6 ( 83.6)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 4s	Avg loss 0.6567	Avg Prec@1 79.06 %	Avg Prec@5 98.51 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2d8dde3ed33cc5997dc4b963f475a72aa851aeb8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 35 =====
=================

[TRAIN Batch 000/391]	Time 0.424s (0.424s)	Loss 0.3442 (0.3442)	Prec@1  85.9 ( 85.9)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2439fbe284f536d694f40127a11dc9e02a31b24a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.090s (0.114s)	Loss 0.3828 (0.4739)	Prec@1  85.2 ( 83.6)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/087c87243c6741023d51cc173b8ddce3b9ed667b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 0.4761	Avg Prec@1 83.51 %	Avg Prec@5 99.29 %

[EVAL Batch 000/079]	Time 0.210s (0.210s)	Loss 0.5369 (0.5369)	Prec@1  89.8 ( 89.8)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6100	Avg Prec@1 79.79 %	Avg Prec@5 98.68 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9c903146eaa6f42e228ab24662f3e01c5010017a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 36 =====
=================

[TRAIN Batch 000/391]	Time 0.431s (0.431s)	Loss 0.4582 (0.4582)	Prec@1  82.0 ( 82.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/947a854b51cfb309e819e79b9f566f4c9c35c4c4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.180s (0.114s)	Loss 0.4719 (0.4626)	Prec@1  85.2 ( 84.1)	Prec@5  99.2 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cdaa9c28aa394a9c598abfd72cfd08e3acaa1196.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 48s	Avg loss 0.4728	Avg Prec@1 83.67 %	Avg Prec@5 99.32 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.7416 (0.7416)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6289	Avg Prec@1 79.49 %	Avg Prec@5 98.78 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b0f2bc4fbb7e77a9f15a5bac0318cf29f954ef2a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 37 =====
=================

[TRAIN Batch 000/391]	Time 0.405s (0.405s)	Loss 0.4723 (0.4723)	Prec@1  84.4 ( 84.4)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7e249fdc896032b9726ac8377e008fdbc0e2eb5b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.106s (0.110s)	Loss 0.4036 (0.4523)	Prec@1  85.2 ( 84.2)	Prec@5 100.0 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/85f5c943015a160cdc535b725a973a1fbe0459af.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4597	Avg Prec@1 83.98 %	Avg Prec@5 99.38 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.6322 (0.6322)	Prec@1  81.2 ( 81.2)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.6243	Avg Prec@1 79.33 %	Avg Prec@5 98.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5ead5d4c4793a4756c28716cc40011025de5aa94.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 38 =====
=================

[TRAIN Batch 000/391]	Time 0.344s (0.344s)	Loss 0.4817 (0.4817)	Prec@1  88.3 ( 88.3)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5985dea1129c826efed65ea22a9c0b9fe6361f40.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.176s (0.106s)	Loss 0.4116 (0.4453)	Prec@1  88.3 ( 84.6)	Prec@5  99.2 ( 99.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/612dd12eeb309d6816f91b4611a6f61203862721.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.4541	Avg Prec@1 84.31 %	Avg Prec@5 99.38 %

[EVAL Batch 000/079]	Time 0.201s (0.201s)	Loss 0.7392 (0.7392)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6236	Avg Prec@1 80.03 %	Avg Prec@5 98.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/323e430329b73764b07c5a92344920b9cd6056f9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 39 =====
=================

[TRAIN Batch 000/391]	Time 0.410s (0.410s)	Loss 0.5210 (0.5210)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f4cca93643dc56e74c1232d6b13c7e2ccf326d93.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.105s (0.111s)	Loss 0.6209 (0.4512)	Prec@1  78.1 ( 84.3)	Prec@5  97.7 ( 99.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f115d5094e2e5a4be93e9632994ba5b5f2ececd8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.4545	Avg Prec@1 84.24 %	Avg Prec@5 99.40 %

[EVAL Batch 000/079]	Time 0.218s (0.218s)	Loss 0.6377 (0.6377)	Prec@1  83.6 ( 83.6)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.6506	Avg Prec@1 79.22 %	Avg Prec@5 98.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f0932e07adc0bf039cda828caf6c22389ed248a2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 40 =====
=================

[TRAIN Batch 000/391]	Time 0.423s (0.423s)	Loss 0.5421 (0.5421)	Prec@1  77.3 ( 77.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e61ce2bccdb80688ccb691640e2448fec4961306.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.025s (0.109s)	Loss 0.5518 (0.4502)	Prec@1  82.8 ( 84.4)	Prec@5  99.2 ( 99.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/abfcb6d3c3eae564ee46330c77c944e3b02b4a2f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4490	Avg Prec@1 84.36 %	Avg Prec@5 99.38 %

[EVAL Batch 000/079]	Time 0.203s (0.203s)	Loss 0.6177 (0.6177)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6370	Avg Prec@1 79.66 %	Avg Prec@5 98.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bb4cb3262c890d5b3140111c6202ce4ea3714243.png" /></p>
</div>
</div>
<div class="cell markdown" id="3Q8CbKO1hvpo">
<p>Il semblerait que certaines techniques de data augmentation sont
adaptés a certains type de dataset, ici, on se rend compte que la
rotation n'est pas approprié au dataset CIFAR-10</p>
</div>
<section id="33-variants-on-the-optimization-algorithm"
class="cell markdown" id="Gfo8m_p4nkZb">
<h2>3.3 Variants on the optimization algorithm</h2>
</section>
<div class="cell code" data-execution_count="27" id="Gt90n-ptgkOu">
<div class="sourceCode" id="cb750"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb750-1"><a href="#cb750-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the package</span></span>
<span id="cb750-2"><a href="#cb750-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim.lr_scheduler</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="28" id="aGw5w3fbo_ws">
<div class="sourceCode" id="cb751"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb751-1"><a href="#cb751-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(batch_size<span class="op">=</span><span class="dv">128</span>, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>, cuda<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb751-2"><a href="#cb751-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb751-3"><a href="#cb751-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ex :</span></span>
<span id="cb751-4"><a href="#cb751-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   {&quot;batch_size&quot;: 128, &quot;epochs&quot;: 5, &quot;lr&quot;: 0.1}</span></span>
<span id="cb751-5"><a href="#cb751-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb751-6"><a href="#cb751-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define model, loss, optim</span></span>
<span id="cb751-7"><a href="#cb751-7" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNet()</span>
<span id="cb751-8"><a href="#cb751-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the scheduler</span></span>
<span id="cb751-9"><a href="#cb751-9" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb751-10"><a href="#cb751-10" aria-hidden="true" tabindex="-1"></a>    lr_sched <span class="op">=</span> torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb751-11"><a href="#cb751-11" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb751-12"><a href="#cb751-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb751-13"><a href="#cb751-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cuda: <span class="co"># only with GPU, and not with CPU</span></span>
<span id="cb751-14"><a href="#cb751-14" aria-hidden="true" tabindex="-1"></a>        cudnn.benchmark <span class="op">=</span> <span class="va">True</span></span>
<span id="cb751-15"><a href="#cb751-15" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb751-16"><a href="#cb751-16" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> criterion.cuda()</span>
<span id="cb751-17"><a href="#cb751-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb751-18"><a href="#cb751-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the data</span></span>
<span id="cb751-19"><a href="#cb751-19" aria-hidden="true" tabindex="-1"></a>    train, test <span class="op">=</span> get_dataset(batch_size, cuda)</span>
<span id="cb751-20"><a href="#cb751-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb751-21"><a href="#cb751-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init plots</span></span>
<span id="cb751-22"><a href="#cb751-22" aria-hidden="true" tabindex="-1"></a>    plot <span class="op">=</span> AccLossPlot()</span>
<span id="cb751-23"><a href="#cb751-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> loss_plot</span>
<span id="cb751-24"><a href="#cb751-24" aria-hidden="true" tabindex="-1"></a>    loss_plot <span class="op">=</span> TrainLossPlot()</span>
<span id="cb751-25"><a href="#cb751-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb751-26"><a href="#cb751-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We iterate on the epochs</span></span>
<span id="cb751-27"><a href="#cb751-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb751-28"><a href="#cb751-28" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">&quot;=================</span><span class="ch">\n</span><span class="st">=== EPOCH &quot;</span><span class="op">+</span><span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>)<span class="op">+</span><span class="st">&quot; =====</span><span class="ch">\n</span><span class="st">=================</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb751-29"><a href="#cb751-29" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Train phase</span></span>
<span id="cb751-30"><a href="#cb751-30" aria-hidden="true" tabindex="-1"></a>      top1_acc, avg_top5_acc, loss <span class="op">=</span> epoch(train, model, criterion, optimizer, cuda)</span>
<span id="cb751-31"><a href="#cb751-31" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Test phase</span></span>
<span id="cb751-32"><a href="#cb751-32" aria-hidden="true" tabindex="-1"></a>      top1_acc_test, top5_acc_test, loss_test <span class="op">=</span> epoch(test, model, criterion, cuda<span class="op">=</span>cuda)</span>
<span id="cb751-33"><a href="#cb751-33" aria-hidden="true" tabindex="-1"></a>      lr_sched.step()</span>
<span id="cb751-34"><a href="#cb751-34" aria-hidden="true" tabindex="-1"></a>      <span class="co"># plot</span></span>
<span id="cb751-35"><a href="#cb751-35" aria-hidden="true" tabindex="-1"></a>      plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="29"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="3JWg0WWBrJg4" data-outputId="21e1a19e-2df5-4488-dc03-7c52b0368302">
<div class="sourceCode" id="cb752"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb752-1"><a href="#cb752-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, epochs <span class="op">=</span> <span class="dv">30</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.300s (0.300s)	Loss 2.3059 (2.3059)	Prec@1   7.8 (  7.8)	Prec@5  45.3 ( 45.3)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cc9f406f608b045e0255ef5843fb1e108228af33.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.025s (0.108s)	Loss 1.7795 (2.0248)	Prec@1  31.2 ( 24.6)	Prec@5  88.3 ( 76.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/62be57114cb3fbb226d25e34739b1de1988746cf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 1.9089	Avg Prec@1 29.53 %	Avg Prec@5 81.28 %

[EVAL Batch 000/079]	Time 0.184s (0.184s)	Loss 1.7066 (1.7066)	Prec@1  29.7 ( 29.7)	Prec@5  86.7 ( 86.7)

===============&gt; Total time 2s	Avg loss 1.6777	Avg Prec@1 38.81 %	Avg Prec@5 87.23 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0fdbacb92e8e7474d371944e1d6da355e21c6435.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.317s (0.317s)	Loss 1.6095 (1.6095)	Prec@1  40.6 ( 40.6)	Prec@5  86.7 ( 86.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f07aef476c0358b461459267923b987fed674c45.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.111s)	Loss 1.5714 (1.6303)	Prec@1  36.7 ( 40.1)	Prec@5  89.1 ( 89.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/81d3dcba3a65fe8b2e10fe1ebeb7caeb86110fc3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 1.5715	Avg Prec@1 42.73 %	Avg Prec@5 90.59 %

[EVAL Batch 000/079]	Time 0.189s (0.189s)	Loss 1.5059 (1.5059)	Prec@1  43.8 ( 43.8)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 2s	Avg loss 1.4969	Avg Prec@1 44.37 %	Avg Prec@5 91.79 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6a846947a2e3755b57dca17c4589663e0694065c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.313s (0.313s)	Loss 1.1897 (1.1897)	Prec@1  60.9 ( 60.9)	Prec@5  94.5 ( 94.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/99a5c5def1067378494468a9f2ba36f2de29de3b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.167s (0.112s)	Loss 1.3736 (1.4212)	Prec@1  47.7 ( 48.4)	Prec@5  91.4 ( 92.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/952aa1bbd7103d349a83c91fb9f9e1526b683c43.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.3800	Avg Prec@1 49.94 %	Avg Prec@5 93.33 %

[EVAL Batch 000/079]	Time 0.210s (0.210s)	Loss 1.2972 (1.2972)	Prec@1  58.6 ( 58.6)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 3s	Avg loss 1.3163	Avg Prec@1 54.17 %	Avg Prec@5 94.32 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3cc731d4626ac518e3f898f7ff2662f9a39a290a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.316s (0.316s)	Loss 1.3637 (1.3637)	Prec@1  50.8 ( 50.8)	Prec@5  94.5 ( 94.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d81e4fe54b33efbee67ae7cc6ed60ac5cda25103.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.111s)	Loss 1.2319 (1.2604)	Prec@1  54.7 ( 55.3)	Prec@5  94.5 ( 94.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d32ae3908237d4e875b2eb6d430d1f234a2892d9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.2343	Avg Prec@1 55.93 %	Avg Prec@5 94.80 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 1.1940 (1.1940)	Prec@1  58.6 ( 58.6)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.2195	Avg Prec@1 55.94 %	Avg Prec@5 95.27 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/40e33496973906631f2132488094fda6c775fb72.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.602s (0.602s)	Loss 1.2315 (1.2315)	Prec@1  54.7 ( 54.7)	Prec@5  95.3 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ecf9bbe1a9f95f17d5135473cb139b84c111a788.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.035s (0.109s)	Loss 1.0806 (1.1435)	Prec@1  65.6 ( 59.4)	Prec@5  95.3 ( 95.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6e070b2a27afe5d42dc3d35206e8e474e9ea844d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.1245	Avg Prec@1 60.05 %	Avg Prec@5 95.78 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 1.1912 (1.1912)	Prec@1  57.8 ( 57.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 1.1901	Avg Prec@1 59.01 %	Avg Prec@5 95.13 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/339dcf678e8e7218cc8b9615872496abcdda4d3b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.320s (0.320s)	Loss 1.0277 (1.0277)	Prec@1  64.1 ( 64.1)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/faec5253182e8d625b1fdf4e4431706999010d95.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.110s)	Loss 1.0848 (1.0430)	Prec@1  60.9 ( 62.8)	Prec@5  95.3 ( 96.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bb33a9b996db4fad5a10cf33399e2a54d2201734.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 45s	Avg loss 1.0363	Avg Prec@1 63.16 %	Avg Prec@5 96.45 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 1.0811 (1.0811)	Prec@1  65.6 ( 65.6)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 1.0675	Avg Prec@1 62.92 %	Avg Prec@5 96.04 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1536ecd959d1e08295d31eda29dba2b44efe59a1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.330s (0.330s)	Loss 1.0610 (1.0610)	Prec@1  57.0 ( 57.0)	Prec@5  95.3 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/031089ba94d2badddc76a9ad146bac14ec4c7fe5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.197s (0.112s)	Loss 1.0946 (0.9772)	Prec@1  60.2 ( 65.3)	Prec@5  96.9 ( 97.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/237e518625d5cd3bef1a099035a4b00527273d92.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.9657	Avg Prec@1 65.80 %	Avg Prec@5 97.00 %

[EVAL Batch 000/079]	Time 0.176s (0.176s)	Loss 1.0273 (1.0273)	Prec@1  65.6 ( 65.6)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 1.0136	Avg Prec@1 64.99 %	Avg Prec@5 96.72 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/376e9d9496855954e67ff6b911c9f5cf95f14efd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.467s (0.467s)	Loss 1.0119 (1.0119)	Prec@1  68.0 ( 68.0)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6d756d52abc22abac4b91da5f641e16bafa1924e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.107s)	Loss 1.0188 (0.9144)	Prec@1  62.5 ( 67.7)	Prec@5  96.1 ( 97.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5a3f01a4110a36865723c542f078b48af4f862e8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.9065	Avg Prec@1 67.94 %	Avg Prec@5 97.31 %

[EVAL Batch 000/079]	Time 0.180s (0.180s)	Loss 0.9046 (0.9046)	Prec@1  69.5 ( 69.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.9765	Avg Prec@1 66.77 %	Avg Prec@5 96.88 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8f8e2aff13f8b28ea23f5aa7ea03ac1250592112.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.325s (0.325s)	Loss 1.0670 (1.0670)	Prec@1  65.6 ( 65.6)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8742354fffa8b506019d959739ad86b4684fba3e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.157s (0.111s)	Loss 0.9312 (0.8536)	Prec@1  70.3 ( 69.9)	Prec@5  94.5 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/278a6bed076c76f64d492ee741623e21295122cd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.8546	Avg Prec@1 69.97 %	Avg Prec@5 97.69 %

[EVAL Batch 000/079]	Time 0.241s (0.241s)	Loss 0.8917 (0.8917)	Prec@1  64.1 ( 64.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.9184	Avg Prec@1 68.85 %	Avg Prec@5 97.10 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8e11fc5f58b2d3a3fe5f1358c19191b1146242d5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.340s (0.340s)	Loss 0.7278 (0.7278)	Prec@1  72.7 ( 72.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1694040acb18349df37454c8eeb1ff2ca3e71757.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.013s (0.111s)	Loss 0.7935 (0.8136)	Prec@1  70.3 ( 71.3)	Prec@5  96.9 ( 97.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aa35c2afd14f017d1eb77d3c9567023b4c67b0bd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.8097	Avg Prec@1 71.64 %	Avg Prec@5 97.93 %

[EVAL Batch 000/079]	Time 0.183s (0.183s)	Loss 0.8133 (0.8133)	Prec@1  69.5 ( 69.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.8270	Avg Prec@1 71.21 %	Avg Prec@5 97.93 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fd3d55c81cdfa26b8250f48156351503cd476c89.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.372s (0.372s)	Loss 0.8203 (0.8203)	Prec@1  71.1 ( 71.1)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/71275bebab1b7ea78ee9814d44c15172578aafca.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.189s (0.110s)	Loss 0.7906 (0.7835)	Prec@1  74.2 ( 72.6)	Prec@5  98.4 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0f7dd6997b8dfa4fffe4ef6f81a3d4ec8037552f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.7782	Avg Prec@1 72.83 %	Avg Prec@5 98.13 %

[EVAL Batch 000/079]	Time 0.184s (0.184s)	Loss 0.8046 (0.8046)	Prec@1  71.9 ( 71.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.8068	Avg Prec@1 72.38 %	Avg Prec@5 97.91 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3bc06a4cf265020d20c27a279d64d8bbeb783ae4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.351s (0.351s)	Loss 0.7059 (0.7059)	Prec@1  75.0 ( 75.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/505e0af497acc4c932ac49e8921459121c49cdee.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.113s)	Loss 0.7907 (0.7463)	Prec@1  73.4 ( 73.8)	Prec@5  96.9 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0e167ba40c78681ab943bcd1dc6d0274d5dd03c4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7465	Avg Prec@1 73.87 %	Avg Prec@5 98.31 %

[EVAL Batch 000/079]	Time 0.227s (0.227s)	Loss 0.7937 (0.7937)	Prec@1  74.2 ( 74.2)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 3s	Avg loss 0.8172	Avg Prec@1 71.36 %	Avg Prec@5 98.02 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1cb6cdb4e3727215683fcf1ee68066f5e11dd5e2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.339s (0.339s)	Loss 0.7820 (0.7820)	Prec@1  75.8 ( 75.8)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2b021bf4285268a68047f253fca00fc978116ac7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.324s (0.108s)	Loss 0.7963 (0.7235)	Prec@1  74.2 ( 74.7)	Prec@5  97.7 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e3a66a87f4e68f1016d3447d2348f61bd783a764.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7174	Avg Prec@1 74.89 %	Avg Prec@5 98.40 %

[EVAL Batch 000/079]	Time 0.177s (0.177s)	Loss 0.8084 (0.8084)	Prec@1  70.3 ( 70.3)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.8065	Avg Prec@1 72.05 %	Avg Prec@5 98.15 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/609b1ce87b7eade2c76b117bb6ea68dd720ddea1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.325s (0.325s)	Loss 0.4963 (0.4963)	Prec@1  83.6 ( 83.6)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7f04337d7f08ad54a025eb46232bfe5cde54a5da.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.111s)	Loss 0.7278 (0.6962)	Prec@1  73.4 ( 75.2)	Prec@5  98.4 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d6d59e169f36d9341589403eb09e28e679b9e435.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.6990	Avg Prec@1 75.42 %	Avg Prec@5 98.53 %

[EVAL Batch 000/079]	Time 0.192s (0.192s)	Loss 0.7483 (0.7483)	Prec@1  75.8 ( 75.8)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7477	Avg Prec@1 74.40 %	Avg Prec@5 98.39 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/923f60ab5b0fe944e775acee003fc7224fe45295.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.344s (0.344s)	Loss 0.8548 (0.8548)	Prec@1  65.6 ( 65.6)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d04b97395e6e6b3c2f1f5bb72bf8fa3a704a8f5f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.175s (0.109s)	Loss 0.5599 (0.6730)	Prec@1  82.0 ( 76.3)	Prec@5  98.4 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/577731a2eacc4de668bce7cf77405877161f8294.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6730	Avg Prec@1 76.35 %	Avg Prec@5 98.58 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 0.7886 (0.7886)	Prec@1  71.9 ( 71.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7558	Avg Prec@1 73.79 %	Avg Prec@5 98.39 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/76250e40360b20fee13f0e66098ae82819a0fbc7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.355s (0.355s)	Loss 0.7093 (0.7093)	Prec@1  75.0 ( 75.0)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/06eeaa9b149d968be1b39094e2961b22d6eef94c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.012s (0.105s)	Loss 0.5546 (0.6556)	Prec@1  79.7 ( 76.9)	Prec@5  97.7 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1087ffafa46d96723fd5ab3920be3049133031c7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6541	Avg Prec@1 77.03 %	Avg Prec@5 98.65 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.7068 (0.7068)	Prec@1  76.6 ( 76.6)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7188	Avg Prec@1 75.46 %	Avg Prec@5 98.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6020d974c2910934fecbb87dc202b81b145b19e8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.351s (0.351s)	Loss 0.6148 (0.6148)	Prec@1  76.6 ( 76.6)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/442aadd541b4f48b129ebc7d117d892baa5ea26b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.109s)	Loss 0.4854 (0.6384)	Prec@1  83.6 ( 77.5)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d683546ef961f8948ff03eefd976e046a082efe6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6376	Avg Prec@1 77.55 %	Avg Prec@5 98.70 %

[EVAL Batch 000/079]	Time 0.191s (0.191s)	Loss 0.6337 (0.6337)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6863	Avg Prec@1 76.78 %	Avg Prec@5 98.42 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3de47fb1aebe93443e2148a32f6924a0491339d0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.349s (0.349s)	Loss 0.6011 (0.6011)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/026e308542fcb1e4a7a1416f40569aab08f7e50f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.183s (0.110s)	Loss 0.5150 (0.6135)	Prec@1  85.2 ( 78.6)	Prec@5  99.2 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/72fadeb20012a8af713b6cb06a858d9c25845c92.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6156	Avg Prec@1 78.35 %	Avg Prec@5 98.80 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 0.6522 (0.6522)	Prec@1  78.1 ( 78.1)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7122	Avg Prec@1 75.42 %	Avg Prec@5 98.44 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cb9f0866c9d3b09152ba008400a0c6b3de746a68.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.341s (0.341s)	Loss 0.5891 (0.5891)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/27f119e2f058befaa535e3eb7b8390cf48688d94.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.029s (0.104s)	Loss 0.5644 (0.5976)	Prec@1  81.2 ( 79.3)	Prec@5  98.4 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a55ec0cd687b4b8b65102e788334bda2f1d0ebf0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5997	Avg Prec@1 79.12 %	Avg Prec@5 98.90 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.7320 (0.7320)	Prec@1  73.4 ( 73.4)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.7048	Avg Prec@1 75.32 %	Avg Prec@5 98.62 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dba7ee1ba7b3d12b31836aa1d8cb5d51be9eae68.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.330s (0.330s)	Loss 0.4218 (0.4218)	Prec@1  88.3 ( 88.3)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4e6c3f5c5d6d08480c6e710f5888964730c3ce6c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.107s (0.112s)	Loss 0.4998 (0.5734)	Prec@1  82.0 ( 79.8)	Prec@5 100.0 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5448886c07fd2dc53e2b0e4d02b192585298dcd2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5817	Avg Prec@1 79.61 %	Avg Prec@5 99.00 %

[EVAL Batch 000/079]	Time 0.190s (0.190s)	Loss 0.7709 (0.7709)	Prec@1  73.4 ( 73.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.7182	Avg Prec@1 75.27 %	Avg Prec@5 98.52 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ede013e6cf3c39242874383bcaaf24c727e4c510.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.373s (0.373s)	Loss 0.7137 (0.7137)	Prec@1  72.7 ( 72.7)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3434b7fba35569ab00b5037cd9056a27f46421d4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.180s (0.111s)	Loss 0.4725 (0.5663)	Prec@1  82.8 ( 80.3)	Prec@5  99.2 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f57aae0c29ab1ab26f57f33430d27f5497d769ba.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5705	Avg Prec@1 80.13 %	Avg Prec@5 98.94 %

[EVAL Batch 000/079]	Time 0.185s (0.185s)	Loss 0.5954 (0.5954)	Prec@1  75.8 ( 75.8)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6562	Avg Prec@1 77.27 %	Avg Prec@5 98.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f7034f9b6aca5d34ecdee07344071555f2b3af67.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.517s (0.517s)	Loss 0.5917 (0.5917)	Prec@1  77.3 ( 77.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ac66b66f0883138f2f35a24b202114dbac7505de.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.040s (0.105s)	Loss 0.4816 (0.5584)	Prec@1  78.9 ( 80.4)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0fb06fcad85e6a02c896c9c8eff8dbe904b79411.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5618	Avg Prec@1 80.14 %	Avg Prec@5 99.05 %

[EVAL Batch 000/079]	Time 0.172s (0.172s)	Loss 0.6415 (0.6415)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6360	Avg Prec@1 78.10 %	Avg Prec@5 98.65 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d016ac1c2940479bf98f852f21f9df6a7c2fa968.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.336s (0.336s)	Loss 0.4823 (0.4823)	Prec@1  83.6 ( 83.6)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7ee14c9261eb7d511950239627795fd93c31c811.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.148s (0.109s)	Loss 0.5038 (0.5452)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/306949c27f9faa4e0ea032c4b4f6708e3f9c2527.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5490	Avg Prec@1 80.87 %	Avg Prec@5 99.11 %

[EVAL Batch 000/079]	Time 0.289s (0.289s)	Loss 0.6619 (0.6619)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6506	Avg Prec@1 77.96 %	Avg Prec@5 98.70 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e032e883de7a08dbc9c5e47f2e5d621df06b34c3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.364s (0.364s)	Loss 0.6186 (0.6186)	Prec@1  85.2 ( 85.2)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9331c5006d9e5b0d1916e9c2fc5ec4aace3e61e3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.241s (0.107s)	Loss 0.6380 (0.5283)	Prec@1  82.0 ( 81.8)	Prec@5  96.9 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8af179bc1afdd17c194dc6d61835cccd86a84e81.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5325	Avg Prec@1 81.49 %	Avg Prec@5 99.13 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.6627 (0.6627)	Prec@1  77.3 ( 77.3)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.6397	Avg Prec@1 78.16 %	Avg Prec@5 98.75 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d6d59cd184cbf8fa6bcb17a3be3c988fe04f551e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.354s (0.354s)	Loss 0.4735 (0.4735)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e3115fbfccd7ca2ceddbd322a064978b50b6dc83.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.170s (0.110s)	Loss 0.5277 (0.5230)	Prec@1  82.0 ( 81.6)	Prec@5  98.4 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c8966b39d2f2d0db8d94e8e7dd435319e2e8ae46.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5221	Avg Prec@1 81.68 %	Avg Prec@5 99.17 %

[EVAL Batch 000/079]	Time 0.203s (0.203s)	Loss 0.6020 (0.6020)	Prec@1  82.0 ( 82.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6235	Avg Prec@1 79.15 %	Avg Prec@5 98.78 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3f723371181e4775146f56aa60371678ebaf40be.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.354s (0.354s)	Loss 0.6097 (0.6097)	Prec@1  76.6 ( 76.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bfabf7484b6d911fb68ce5ccf20098db58393c42.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.110s)	Loss 0.4050 (0.5118)	Prec@1  85.9 ( 81.9)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/99c6cc6b55979f2368dd7972cdcd1ba09c6e5ae6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5150	Avg Prec@1 81.91 %	Avg Prec@5 99.20 %

[EVAL Batch 000/079]	Time 0.192s (0.192s)	Loss 0.5911 (0.5911)	Prec@1  82.0 ( 82.0)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6003	Avg Prec@1 79.74 %	Avg Prec@5 98.83 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/af57fad4b73e26d7f45164760064f8540bcc3e24.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.372s (0.372s)	Loss 0.3567 (0.3567)	Prec@1  87.5 ( 87.5)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/42b74051683a6073dbdce06ceed0a4bbf2877b63.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.106s)	Loss 0.5157 (0.4988)	Prec@1  82.0 ( 82.5)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5f091bfb036b89359ef2ff364d8e7b0079208473.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5029	Avg Prec@1 82.36 %	Avg Prec@5 99.25 %

[EVAL Batch 000/079]	Time 0.193s (0.193s)	Loss 0.6321 (0.6321)	Prec@1  78.1 ( 78.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6227	Avg Prec@1 78.55 %	Avg Prec@5 98.89 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6f7871b06a7ae87ae472680b2c0462e682098b11.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.366s (0.366s)	Loss 0.5132 (0.5132)	Prec@1  82.8 ( 82.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2fe01a0881a06e1db32c1406fef9c13d762409ee.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.191s (0.110s)	Loss 0.6648 (0.4925)	Prec@1  76.6 ( 82.6)	Prec@5  98.4 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8fdc1ba5708c05bb05d4515d104802c74e504d5a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.4975	Avg Prec@1 82.55 %	Avg Prec@5 99.28 %

[EVAL Batch 000/079]	Time 0.186s (0.186s)	Loss 0.6296 (0.6296)	Prec@1  79.7 ( 79.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6241	Avg Prec@1 78.60 %	Avg Prec@5 98.76 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9d06d9a1a849fa0846ae758ae1626566e153f0d1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.400s (0.400s)	Loss 0.4374 (0.4374)	Prec@1  84.4 ( 84.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/10cda66ef489ec9bc5cfdfbc0d9a3e65717d5230.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.111s)	Loss 0.5386 (0.4869)	Prec@1  78.9 ( 82.7)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f64425d9fd552ccf0663798cb67bcba36beda57f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.4878	Avg Prec@1 82.78 %	Avg Prec@5 99.30 %

[EVAL Batch 000/079]	Time 0.189s (0.189s)	Loss 0.6296 (0.6296)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.5991	Avg Prec@1 79.42 %	Avg Prec@5 98.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7a915d3a439ce2c6f85efab4c337716ed96755b7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.385s (0.385s)	Loss 0.3828 (0.3828)	Prec@1  89.1 ( 89.1)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7ce8be873c8c43c1b414e16a4ca7ca9d1d341003.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.167s (0.110s)	Loss 0.4421 (0.4657)	Prec@1  87.5 ( 83.6)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9665bb9e81d012046e90f66d6e49b1b93a385d9f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.4716	Avg Prec@1 83.53 %	Avg Prec@5 99.32 %

[EVAL Batch 000/079]	Time 0.201s (0.201s)	Loss 0.6533 (0.6533)	Prec@1  78.1 ( 78.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.5924	Avg Prec@1 79.86 %	Avg Prec@5 98.84 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ba420db5a5eb44c695d97dea5d006095ac56f085.png" /></p>
</div>
</div>
<div class="cell markdown" id="UaU3rQzIx1R9">
<p>avec normalization :</p>
<p>TRAIN ===============&gt; Total time 14s Avg loss 0.8629 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 69.82 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.78 %</p>
<p>EVAL ===============&gt; Total time 2s Avg loss 0.9919 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 65.65 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.12 %</p>
<p>avec lr_exp :</p>
<p>===============&gt; Total time 16s Avg loss 0.8619 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 69.93 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.73 %</p>
<p>===============&gt; Total time 2s Avg loss 0.9632 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 66.79 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.01 %</p>
</div>
<section id="34-regularization-of-the-network-by-dropout"
class="cell markdown" id="rFvzeNe3vB6z">
<h2>3.4 Regularization of the network by dropout</h2>
<p>Link to Droupout layer : <a
href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"
class="uri">https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html</a></p>
<p>Dropout(p=0.5, inplace=False)</p>
</section>
<div class="cell code" data-execution_count="30" id="q-rmDBu7rWx9">
<div class="sourceCode" id="cb844"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb844-1"><a href="#cb844-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb844-2"><a href="#cb844-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb844-3"><a href="#cb844-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class defines the structure of the neural network</span></span>
<span id="cb844-4"><a href="#cb844-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb844-5"><a href="#cb844-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb844-6"><a href="#cb844-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb844-7"><a href="#cb844-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb844-8"><a href="#cb844-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We first define the convolution and pooling layers as a features extractor</span></span>
<span id="cb844-9"><a href="#cb844-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb844-10"><a href="#cb844-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb844-11"><a href="#cb844-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb844-12"><a href="#cb844-12" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb844-13"><a href="#cb844-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb844-14"><a href="#cb844-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb844-15"><a href="#cb844-15" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb844-16"><a href="#cb844-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>,<span class="dv">64</span>, (<span class="dv">5</span>,<span class="dv">5</span>), stride <span class="op">=</span> <span class="dv">1</span>, padding <span class="op">=</span> <span class="dv">2</span>),</span>
<span id="cb844-17"><a href="#cb844-17" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb844-18"><a href="#cb844-18" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>,<span class="dv">2</span>), stride <span class="op">=</span> <span class="dv">2</span>, padding <span class="op">=</span> <span class="dv">0</span>, ceil_mode <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb844-19"><a href="#cb844-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb844-20"><a href="#cb844-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We then define fully connected layers as a classifier</span></span>
<span id="cb844-21"><a href="#cb844-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb844-22"><a href="#cb844-22" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">1000</span>), <span class="co">#1024 = 4*4*64</span></span>
<span id="cb844-23"><a href="#cb844-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb844-24"><a href="#cb844-24" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb844-25"><a href="#cb844-25" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1000</span>, <span class="dv">10</span>),</span>
<span id="cb844-26"><a href="#cb844-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reminder: The softmax is included in the loss, do not put it here</span></span>
<span id="cb844-27"><a href="#cb844-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb844-28"><a href="#cb844-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb844-29"><a href="#cb844-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method called when we apply the network to an input batch</span></span>
<span id="cb844-30"><a href="#cb844-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb844-31"><a href="#cb844-31" aria-hidden="true" tabindex="-1"></a>        bsize <span class="op">=</span> <span class="bu">input</span>.size(<span class="dv">0</span>) <span class="co"># batch size</span></span>
<span id="cb844-32"><a href="#cb844-32" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.features(<span class="bu">input</span>) <span class="co"># output of the conv layers</span></span>
<span id="cb844-33"><a href="#cb844-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.view(bsize, <span class="op">-</span><span class="dv">1</span>) <span class="co"># we flatten the 2D feature maps into one 1D vector for each input</span></span>
<span id="cb844-34"><a href="#cb844-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.classifier(output) <span class="co"># we compute the output of the fc layers</span></span>
<span id="cb844-35"><a href="#cb844-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="31"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="TghQsaW7xbmm" data-outputId="5944c747-6d8b-4908-d31b-1548ae77aafe">
<div class="sourceCode" id="cb845"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb845-1"><a href="#cb845-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">40</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.664s (0.664s)	Loss 2.3058 (2.3058)	Prec@1  11.7 ( 11.7)	Prec@5  44.5 ( 44.5)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1ccf0ab4beb5304ce10e01d1735a8216892270fb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.111s)	Loss 1.8035 (2.0390)	Prec@1  33.6 ( 23.8)	Prec@5  79.7 ( 76.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a1693c4792b6a65155f3c62e4cf58450105ddcdb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 44s	Avg loss 1.9234	Avg Prec@1 28.72 %	Avg Prec@5 80.77 %

[EVAL Batch 000/079]	Time 0.324s (0.324s)	Loss 1.6748 (1.6748)	Prec@1  45.3 ( 45.3)	Prec@5  87.5 ( 87.5)

===============&gt; Total time 3s	Avg loss 1.6638	Avg Prec@1 40.77 %	Avg Prec@5 90.32 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e850586216b6800ce8204930a070bc609304a513.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.318s (0.318s)	Loss 1.6452 (1.6452)	Prec@1  38.3 ( 38.3)	Prec@5  90.6 ( 90.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/95d589ea4e86a9816d43c97e532d908c7326ac5d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.111s)	Loss 1.5002 (1.6422)	Prec@1  46.1 ( 39.6)	Prec@5  93.0 ( 89.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5b42abdfcf6b91c1d800ccd0f77d48db5f959e11.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 1.5913	Avg Prec@1 41.77 %	Avg Prec@5 90.01 %

[EVAL Batch 000/079]	Time 0.176s (0.176s)	Loss 1.4232 (1.4232)	Prec@1  53.9 ( 53.9)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 2s	Avg loss 1.4379	Avg Prec@1 47.35 %	Avg Prec@5 92.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/182446bb61d4732b30ebf39b8b8f9a46ca94780d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.352s (0.352s)	Loss 1.4968 (1.4968)	Prec@1  43.0 ( 43.0)	Prec@5  93.0 ( 93.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/75a37201583ee4b00680ec7eff9acb38ba301bd6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.015s (0.114s)	Loss 1.3259 (1.4463)	Prec@1  52.3 ( 47.4)	Prec@5  97.7 ( 92.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/56c867ee153c1c443051aba05884a2da74b3f161.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.4069	Avg Prec@1 48.89 %	Avg Prec@5 92.80 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 1.4793 (1.4793)	Prec@1  46.1 ( 46.1)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 3s	Avg loss 1.4525	Avg Prec@1 49.46 %	Avg Prec@5 92.19 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6b29615c47a006f726593d81af0a06a469be97a5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.318s (0.318s)	Loss 1.3470 (1.3470)	Prec@1  53.1 ( 53.1)	Prec@5  93.0 ( 93.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c3b89e61c782a2bcf77bd39c06c9e16a91f58a36.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.075s (0.104s)	Loss 1.1704 (1.2869)	Prec@1  55.5 ( 53.9)	Prec@5  97.7 ( 94.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/39950fb98eba4f6f67a6fc0448637d135c59fef1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 1.2599	Avg Prec@1 54.95 %	Avg Prec@5 94.60 %

[EVAL Batch 000/079]	Time 0.181s (0.181s)	Loss 1.2563 (1.2563)	Prec@1  57.0 ( 57.0)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.2317	Avg Prec@1 57.41 %	Avg Prec@5 94.73 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/50deee72c9f13a1474cbaf7c1a6bae1be40b6bd5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.320s (0.320s)	Loss 1.3852 (1.3852)	Prec@1  46.1 ( 46.1)	Prec@5  95.3 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/22c96e72f59d5e643aacfc1c1daf35f7d7a844f9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.051s (0.110s)	Loss 1.0822 (1.1750)	Prec@1  59.4 ( 58.4)	Prec@5  96.1 ( 95.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/67a880bd38330eca4e3a0c4abd65fdd946209138.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 1.1571	Avg Prec@1 58.97 %	Avg Prec@5 95.53 %

[EVAL Batch 000/079]	Time 0.176s (0.176s)	Loss 1.0215 (1.0215)	Prec@1  68.0 ( 68.0)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 1.0462	Avg Prec@1 63.89 %	Avg Prec@5 96.41 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3fcb998ba378ab2a6e6d863560c1b6b7524a8ec7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.330s (0.330s)	Loss 1.0291 (1.0291)	Prec@1  63.3 ( 63.3)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/373c2c1a28bef036708bbe9166f1242d0b8426f7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.146s (0.109s)	Loss 0.9032 (1.1002)	Prec@1  63.3 ( 61.2)	Prec@5  96.1 ( 95.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e1403d405e3690f5529185c57b4b16bc197dfb0b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.0750	Avg Prec@1 61.95 %	Avg Prec@5 96.02 %

[EVAL Batch 000/079]	Time 0.193s (0.193s)	Loss 1.0999 (1.0999)	Prec@1  63.3 ( 63.3)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 3s	Avg loss 1.0617	Avg Prec@1 61.86 %	Avg Prec@5 95.73 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a858281488b86d83dc1da6679b20803faa13bc6a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.349s (0.349s)	Loss 1.0210 (1.0210)	Prec@1  61.7 ( 61.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/241e31a47a9cd359d468dcb9387d92ddfbb8b8b1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.186s (0.109s)	Loss 0.9211 (1.0155)	Prec@1  66.4 ( 63.8)	Prec@5  98.4 ( 96.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/96f182bb6a4a25ed8aee7717eb5e08afc6e821f0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.0039	Avg Prec@1 64.26 %	Avg Prec@5 96.71 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 0.9478 (0.9478)	Prec@1  66.4 ( 66.4)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.9478	Avg Prec@1 66.97 %	Avg Prec@5 97.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2aa5acc0d098844d99ea3968d064d7671d3d214d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.330s (0.330s)	Loss 0.8797 (0.8797)	Prec@1  69.5 ( 69.5)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/969addb663dd2fc93871bb61cea28a9a961c3d4c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.205s (0.111s)	Loss 0.8615 (0.9527)	Prec@1  68.8 ( 66.6)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/de8a92e86a8e819355ad1af17ab04a10de5badcc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.9472	Avg Prec@1 66.74 %	Avg Prec@5 97.01 %

[EVAL Batch 000/079]	Time 0.185s (0.185s)	Loss 0.8393 (0.8393)	Prec@1  72.7 ( 72.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.9135	Avg Prec@1 68.97 %	Avg Prec@5 97.08 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/38a96ee00369fe09ba7a59f319a306e1eca6fdfe.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.328s (0.328s)	Loss 1.0200 (1.0200)	Prec@1  66.4 ( 66.4)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/768b0b222ea489bb5a1a6c9820cb0afb898c04ba.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.072s (0.112s)	Loss 1.0741 (0.9034)	Prec@1  60.9 ( 68.4)	Prec@5  97.7 ( 97.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0c4b57af525283f0bf2be0cc1b8fab9119e41ea9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.8960	Avg Prec@1 68.59 %	Avg Prec@5 97.30 %

[EVAL Batch 000/079]	Time 0.182s (0.182s)	Loss 0.8209 (0.8209)	Prec@1  70.3 ( 70.3)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.8843	Avg Prec@1 69.12 %	Avg Prec@5 97.52 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/66d05eb0664763333c83895eb15a224d64858493.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.328s (0.328s)	Loss 0.8460 (0.8460)	Prec@1  68.8 ( 68.8)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3dd50335bd99fbe147ed74250f2873ebe7dda319.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.025s (0.104s)	Loss 0.8282 (0.8661)	Prec@1  69.5 ( 69.3)	Prec@5  96.9 ( 97.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/598e4508c41fa2a99170471fc7192d8f5cc34c08.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.8596	Avg Prec@1 69.79 %	Avg Prec@5 97.57 %

[EVAL Batch 000/079]	Time 0.178s (0.178s)	Loss 0.8131 (0.8131)	Prec@1  71.9 ( 71.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.8648	Avg Prec@1 70.28 %	Avg Prec@5 97.65 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3ef9184c85e7df9d9e81675d3b4b78c792bf7647.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.316s (0.316s)	Loss 0.7907 (0.7907)	Prec@1  69.5 ( 69.5)	Prec@5  96.1 ( 96.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1b6885892b1deb8f8bd4a9d6dc9e654b7c66bb3d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.109s)	Loss 0.9491 (0.8185)	Prec@1  66.4 ( 71.5)	Prec@5  96.1 ( 97.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/26090375878ad4ba6b1018678710ba5c27880cd8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.8210	Avg Prec@1 71.32 %	Avg Prec@5 97.75 %

[EVAL Batch 000/079]	Time 0.179s (0.179s)	Loss 0.8043 (0.8043)	Prec@1  69.5 ( 69.5)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.8796	Avg Prec@1 69.81 %	Avg Prec@5 97.23 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4daac9f05ce95afdaa05bbe72961d4e576302715.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.345s (0.345s)	Loss 0.8052 (0.8052)	Prec@1  67.2 ( 67.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4e6d56fb3b0eea373af0f38e6588ea8ddc10629b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.179s (0.110s)	Loss 0.8075 (0.8035)	Prec@1  68.8 ( 71.9)	Prec@5  98.4 ( 97.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/39a81ceaa9b83f4cc26bb79469dcce9cf607e9c5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7995	Avg Prec@1 72.08 %	Avg Prec@5 97.83 %

[EVAL Batch 000/079]	Time 0.179s (0.179s)	Loss 0.7290 (0.7290)	Prec@1  75.0 ( 75.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7942	Avg Prec@1 72.43 %	Avg Prec@5 97.84 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/87ea2be812d01d45b28d9eb61515b6f2a53cf1c8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.557s (0.557s)	Loss 0.7563 (0.7563)	Prec@1  69.5 ( 69.5)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f977f8b720168227f8ed64aadd490c27a8d6c9a4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.179s (0.107s)	Loss 0.8347 (0.7782)	Prec@1  72.7 ( 72.7)	Prec@5  97.7 ( 98.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7076f894bef87835316061a3840214897c04f65e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7696	Avg Prec@1 73.19 %	Avg Prec@5 98.03 %

[EVAL Batch 000/079]	Time 0.199s (0.199s)	Loss 0.7562 (0.7562)	Prec@1  73.4 ( 73.4)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.7698	Avg Prec@1 73.24 %	Avg Prec@5 98.15 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0cb8fd527a2a9c02bd72e5be4f18ae3b13b61f7f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.355s (0.355s)	Loss 0.7781 (0.7781)	Prec@1  73.4 ( 73.4)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5d90e3f6e2ba8002523604778ed3a992cfcec018.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.148s (0.110s)	Loss 0.6681 (0.7419)	Prec@1  75.0 ( 74.2)	Prec@5  99.2 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a049bb8ebf42465dc017b061e70015d94e5d3dff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7492	Avg Prec@1 73.95 %	Avg Prec@5 98.19 %

[EVAL Batch 000/079]	Time 0.356s (0.356s)	Loss 0.7182 (0.7182)	Prec@1  74.2 ( 74.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.7310	Avg Prec@1 75.01 %	Avg Prec@5 98.45 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46735b49d743c3e79c77c8be92c1c15ae0f5eed2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.345s (0.345s)	Loss 0.5756 (0.5756)	Prec@1  75.8 ( 75.8)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5b21136d9f8385b3b5be64d6b3277c3b938d5bf0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.176s (0.117s)	Loss 0.6782 (0.7256)	Prec@1  74.2 ( 74.6)	Prec@5  98.4 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d377a99c6b46b8062f17d88e5f0d67afb257915d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7268	Avg Prec@1 74.57 %	Avg Prec@5 98.26 %

[EVAL Batch 000/079]	Time 0.188s (0.188s)	Loss 0.6701 (0.6701)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7158	Avg Prec@1 75.16 %	Avg Prec@5 98.36 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/581edbb2144a973f9ddfc78ff8b15954a3b06a9a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.612s (0.612s)	Loss 0.7105 (0.7105)	Prec@1  74.2 ( 74.2)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1d32f5830b7d8dbdfc2200222f66cd39adae5063.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.114s (0.109s)	Loss 0.6516 (0.7087)	Prec@1  74.2 ( 75.2)	Prec@5  99.2 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7e083ef2e588663ecc00e7829892a449a98ea48a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.7104	Avg Prec@1 75.10 %	Avg Prec@5 98.35 %

[EVAL Batch 000/079]	Time 0.193s (0.193s)	Loss 0.7130 (0.7130)	Prec@1  72.7 ( 72.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7787	Avg Prec@1 72.39 %	Avg Prec@5 98.16 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/68b6b68be67eb8d213caa4723aba726a72e18475.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.373s (0.373s)	Loss 0.6464 (0.6464)	Prec@1  77.3 ( 77.3)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d149089aec12c7b3607cc3cf2a485569a11f9a6b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.092s (0.113s)	Loss 0.7443 (0.6923)	Prec@1  74.2 ( 76.1)	Prec@5  99.2 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5d72d1e26190d5e600cec5b89038f307178b38e7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.6913	Avg Prec@1 75.90 %	Avg Prec@5 98.45 %

[EVAL Batch 000/079]	Time 0.201s (0.201s)	Loss 0.6240 (0.6240)	Prec@1  79.7 ( 79.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6907	Avg Prec@1 76.15 %	Avg Prec@5 98.32 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6401459f50dde914f339bef5eac68e9a61c18564.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.353s (0.353s)	Loss 0.7500 (0.7500)	Prec@1  71.9 ( 71.9)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dcd4377aba6151b94410d4771a969e19dd721160.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.060s (0.111s)	Loss 0.6991 (0.6637)	Prec@1  73.4 ( 76.8)	Prec@5  97.7 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/50576e5c44cd529aad00b031166cb2ac88d12cc0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6699	Avg Prec@1 76.61 %	Avg Prec@5 98.56 %

[EVAL Batch 000/079]	Time 0.210s (0.210s)	Loss 0.6813 (0.6813)	Prec@1  77.3 ( 77.3)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7591	Avg Prec@1 74.28 %	Avg Prec@5 97.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8c1bbf0b43ea95e80e380194836746829e4eeb26.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.381s (0.381s)	Loss 0.7951 (0.7951)	Prec@1  69.5 ( 69.5)	Prec@5  95.3 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b90af81987365d172db7a0cc5951983a97976c0d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.009s (0.105s)	Loss 0.6850 (0.6591)	Prec@1  79.7 ( 77.2)	Prec@5  98.4 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/14547158c22d6cb7995a0816ad3ecb4b6e48b1d2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6596	Avg Prec@1 76.99 %	Avg Prec@5 98.53 %

[EVAL Batch 000/079]	Time 0.188s (0.188s)	Loss 0.5517 (0.5517)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6807	Avg Prec@1 76.81 %	Avg Prec@5 98.35 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/48ef75f04e9141d7f2d359d1dd24dc21c0048276.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.361s (0.361s)	Loss 0.5943 (0.5943)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/36bbfdcf057ab890be2c4db4de271f88c3d8ae15.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.189s (0.110s)	Loss 0.5700 (0.6401)	Prec@1  79.7 ( 77.6)	Prec@5  99.2 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9ba52353a4a89da9bacc4765fcb723b23e3765e4.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6434	Avg Prec@1 77.57 %	Avg Prec@5 98.65 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.5864 (0.5864)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6679	Avg Prec@1 77.23 %	Avg Prec@5 98.52 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/382c83dc242bdee6e3c469dc81b2244f34dea404.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.359s (0.359s)	Loss 0.7247 (0.7247)	Prec@1  75.0 ( 75.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ed282e034470dc8e0a813123d09485978e48dd4f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.137s (0.115s)	Loss 0.6333 (0.6400)	Prec@1  82.0 ( 78.0)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bc5822f9b2df0e53614a970e5f346f66bb22f330.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6369	Avg Prec@1 77.95 %	Avg Prec@5 98.66 %

[EVAL Batch 000/079]	Time 0.195s (0.195s)	Loss 0.5557 (0.5557)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6337	Avg Prec@1 78.30 %	Avg Prec@5 98.71 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e189e0a044acbb12f4e1984b0486e178a5a0a591.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.360s (0.360s)	Loss 0.4893 (0.4893)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c649c91f983a4e98b441d05fc16572f60aabe2d3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.081s (0.104s)	Loss 0.5426 (0.6102)	Prec@1  78.1 ( 78.5)	Prec@5 100.0 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d652dc8b0ec6737ad25d4ab0cc4bd32813f526ab.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6201	Avg Prec@1 78.25 %	Avg Prec@5 98.69 %

[EVAL Batch 000/079]	Time 0.190s (0.190s)	Loss 0.5636 (0.5636)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6819	Avg Prec@1 76.12 %	Avg Prec@5 98.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f487fff60d77cab67ff75fde3822935ce41ed86b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.347s (0.347s)	Loss 0.5361 (0.5361)	Prec@1  81.2 ( 81.2)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5b5a765a74457e9f85df058b2189bdc54edb6cc8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.138s (0.109s)	Loss 0.6609 (0.6130)	Prec@1  77.3 ( 78.6)	Prec@5  99.2 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e15083c2a5f76d008694d1b8d397cfb4fcc073a2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6156	Avg Prec@1 78.65 %	Avg Prec@5 98.80 %

[EVAL Batch 000/079]	Time 0.195s (0.195s)	Loss 0.5431 (0.5431)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6157	Avg Prec@1 78.75 %	Avg Prec@5 98.81 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d8deec91d72ec6f260ad324449d88c5506620041.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.354s (0.354s)	Loss 0.5914 (0.5914)	Prec@1  82.0 ( 82.0)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8a969efc953744f4737565324cfc3f034299706c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.166s (0.109s)	Loss 0.6356 (0.6034)	Prec@1  78.1 ( 79.1)	Prec@5  99.2 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f339c7d7d9dd1ae40ac68a46770f2554bcfde928.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6012	Avg Prec@1 78.95 %	Avg Prec@5 98.78 %

[EVAL Batch 000/079]	Time 0.199s (0.199s)	Loss 0.5422 (0.5422)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6251	Avg Prec@1 78.28 %	Avg Prec@5 98.71 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/949cb79233d7b71ea1f4522dd12046f16f4e13f3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.526s (0.526s)	Loss 0.7315 (0.7315)	Prec@1  78.9 ( 78.9)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0840c6284e8bcc649bb5941e1939ea5019785a73.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.198s (0.106s)	Loss 0.5796 (0.5969)	Prec@1  78.9 ( 79.3)	Prec@5  99.2 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a4799030de264f3d1236a6149b6b48dae79313cb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5960	Avg Prec@1 79.33 %	Avg Prec@5 98.81 %

[EVAL Batch 000/079]	Time 0.214s (0.214s)	Loss 0.5304 (0.5304)	Prec@1  82.8 ( 82.8)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6337	Avg Prec@1 78.01 %	Avg Prec@5 98.73 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/46a4ea93d9a59f21440b8d77f279af7b9487931e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.377s (0.377s)	Loss 0.5303 (0.5303)	Prec@1  79.7 ( 79.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/22534d7fb999abf33f86180ed6409988ab79ec98.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.040s (0.111s)	Loss 0.6421 (0.5849)	Prec@1  76.6 ( 79.6)	Prec@5  96.9 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b96aa59c84cffb5090fd006b6e75c8db0156390f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5843	Avg Prec@1 79.63 %	Avg Prec@5 98.92 %

[EVAL Batch 000/079]	Time 0.329s (0.329s)	Loss 0.5257 (0.5257)	Prec@1  85.2 ( 85.2)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6122	Avg Prec@1 78.88 %	Avg Prec@5 98.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8ec75428917fe073c163b65d8bdc4b4f176b7847.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.362s (0.362s)	Loss 0.5650 (0.5650)	Prec@1  82.0 ( 82.0)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/77498839b17110a1323ba88f064fe9662fda7d67.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.177s (0.115s)	Loss 0.5533 (0.5833)	Prec@1  77.3 ( 79.7)	Prec@5  99.2 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c3dfc2408d966f0d8f5ca85e5fc65f60e51198d5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5787	Avg Prec@1 79.84 %	Avg Prec@5 98.84 %

[EVAL Batch 000/079]	Time 0.200s (0.200s)	Loss 0.5206 (0.5206)	Prec@1  82.0 ( 82.0)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6231	Avg Prec@1 78.12 %	Avg Prec@5 98.80 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/64be3d7424947ce79d89a8b3ad4e04f517e6f6f6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.338s (0.338s)	Loss 0.6279 (0.6279)	Prec@1  77.3 ( 77.3)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/088a89b1c7a958bee3f38a5534b35c3d2ee533b9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.047s (0.109s)	Loss 0.5851 (0.5632)	Prec@1  80.5 ( 80.5)	Prec@5  98.4 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8f5657bc1038f0c43dbe211ba9205635d90dbbb2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5699	Avg Prec@1 80.16 %	Avg Prec@5 98.91 %

[EVAL Batch 000/079]	Time 0.191s (0.191s)	Loss 0.4778 (0.4778)	Prec@1  86.7 ( 86.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6251	Avg Prec@1 78.25 %	Avg Prec@5 98.71 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8959ea39cc9d4023a4f9f5d6a612e6c985c06ffa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.365s (0.365s)	Loss 0.6210 (0.6210)	Prec@1  77.3 ( 77.3)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c133d4547927e4b94951e0b66e694ca2cc3eacbb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.007s (0.110s)	Loss 0.4566 (0.5571)	Prec@1  82.0 ( 80.4)	Prec@5  99.2 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/34f990311e5a02825b4ab11482f14a15bdb199d9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5611	Avg Prec@1 80.32 %	Avg Prec@5 98.94 %

[EVAL Batch 000/079]	Time 0.214s (0.214s)	Loss 0.5121 (0.5121)	Prec@1  84.4 ( 84.4)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.6017	Avg Prec@1 79.12 %	Avg Prec@5 98.86 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/62ebb6a30bedde1331f2c47c6271534bfbb84cff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.368s (0.368s)	Loss 0.4514 (0.4514)	Prec@1  87.5 ( 87.5)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0ab561e05818055a1bdba45fa9a4aaf81372e8c3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.181s (0.106s)	Loss 0.4567 (0.5461)	Prec@1  85.9 ( 80.7)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bf4783fc7c7e7b3eb1b19ba0ab43dca366ef7c76.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5567	Avg Prec@1 80.52 %	Avg Prec@5 98.94 %

[EVAL Batch 000/079]	Time 0.195s (0.195s)	Loss 0.4921 (0.4921)	Prec@1  84.4 ( 84.4)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.5915	Avg Prec@1 79.54 %	Avg Prec@5 98.80 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/563a09bb16f04685597dd59107e5858e06bfda6c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 31 =====
=================

[TRAIN Batch 000/391]	Time 0.396s (0.396s)	Loss 0.5492 (0.5492)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/096b2b7460fda5cf3b366b8e25f1b2cf7b6140c3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.155s (0.110s)	Loss 0.5720 (0.5477)	Prec@1  82.0 ( 80.8)	Prec@5 100.0 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ba799dbd71e75badbbb536106dc38216ba24990f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5450	Avg Prec@1 81.08 %	Avg Prec@5 98.95 %

[EVAL Batch 000/079]	Time 0.199s (0.199s)	Loss 0.4688 (0.4688)	Prec@1  85.2 ( 85.2)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6084	Avg Prec@1 78.70 %	Avg Prec@5 98.71 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2ca5fa806a61c82f71b1c4d4cbfcde6012ae8921.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 32 =====
=================

[TRAIN Batch 000/391]	Time 0.401s (0.401s)	Loss 0.6143 (0.6143)	Prec@1  79.7 ( 79.7)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c2cf5b21b713171e43708076da7dc765ef04d124.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.083s (0.112s)	Loss 0.7555 (0.5485)	Prec@1  75.0 ( 80.8)	Prec@5  96.9 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e3e8de92f9d88545229dcdcf05440d1f3edc12ff.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5370	Avg Prec@1 81.16 %	Avg Prec@5 99.07 %

[EVAL Batch 000/079]	Time 0.204s (0.204s)	Loss 0.4655 (0.4655)	Prec@1  85.9 ( 85.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6132	Avg Prec@1 78.78 %	Avg Prec@5 98.79 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ab1e4ebb89c9cbc5623551f34453dd3239308c35.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 33 =====
=================

[TRAIN Batch 000/391]	Time 0.605s (0.605s)	Loss 0.6114 (0.6114)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a3ba19066fb3238e2c018ce43926a5d5bb0b9e9e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.177s (0.106s)	Loss 0.5167 (0.5326)	Prec@1  85.2 ( 81.5)	Prec@5  99.2 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6e0f74fbc6cc7189797b46a3b8db78b55ecab986.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5352	Avg Prec@1 81.33 %	Avg Prec@5 99.08 %

[EVAL Batch 000/079]	Time 0.194s (0.194s)	Loss 0.4718 (0.4718)	Prec@1  84.4 ( 84.4)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.5789	Avg Prec@1 80.20 %	Avg Prec@5 99.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2a28f8a179ee1286214c0c4208872a7c84dc4dd9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 34 =====
=================

[TRAIN Batch 000/391]	Time 0.371s (0.371s)	Loss 0.5557 (0.5557)	Prec@1  77.3 ( 77.3)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d5fa88c7875512d9a4a41e82b93acf2711d510f8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.109s)	Loss 0.5789 (0.5381)	Prec@1  77.3 ( 81.2)	Prec@5  98.4 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3815af6eb0c1ad6130e944ba8c8d38847fc1fd11.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5313	Avg Prec@1 81.57 %	Avg Prec@5 99.05 %

[EVAL Batch 000/079]	Time 0.323s (0.323s)	Loss 0.4525 (0.4525)	Prec@1  85.9 ( 85.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5751	Avg Prec@1 79.90 %	Avg Prec@5 99.00 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bf4339699d2c45f00dc61f9875d592ea91f1fac9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 35 =====
=================

[TRAIN Batch 000/391]	Time 0.350s (0.350s)	Loss 0.3993 (0.3993)	Prec@1  85.9 ( 85.9)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c135ef1fcb22cbe5b97306e08e099e8e737dcc66.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.011s (0.110s)	Loss 0.5163 (0.5281)	Prec@1  79.7 ( 81.6)	Prec@5  99.2 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6887d890b691466dc3fd8ae710ee2df39594834d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5254	Avg Prec@1 81.59 %	Avg Prec@5 99.09 %

[EVAL Batch 000/079]	Time 0.200s (0.200s)	Loss 0.4334 (0.4334)	Prec@1  85.9 ( 85.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5810	Avg Prec@1 79.93 %	Avg Prec@5 98.84 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/492f7c9377685a2b86bc017ccee0a5c928ebf289.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 36 =====
=================

[TRAIN Batch 000/391]	Time 0.412s (0.412s)	Loss 0.5341 (0.5341)	Prec@1  77.3 ( 77.3)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/320a47212f8c3b3f6216fc68a4a070959d8db338.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.189s (0.112s)	Loss 0.5375 (0.5263)	Prec@1  82.0 ( 81.7)	Prec@5  98.4 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/98d01bf3b9b0fe72cd9db918c6f4ce1483a3c5d2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5226	Avg Prec@1 81.78 %	Avg Prec@5 99.07 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.4503 (0.4503)	Prec@1  86.7 ( 86.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.5750	Avg Prec@1 80.16 %	Avg Prec@5 99.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec2c51c6c5a37377a9954daa6607f67ff8d7f10e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 37 =====
=================

[TRAIN Batch 000/391]	Time 0.388s (0.388s)	Loss 0.4497 (0.4497)	Prec@1  85.2 ( 85.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/43d4f9826c6048b640de5df55f0d213ec81eab89.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.065s (0.114s)	Loss 0.4519 (0.5060)	Prec@1  85.9 ( 82.3)	Prec@5  98.4 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3a72009ea15fb5d745b5f3b5fdfc32e5d967e6a6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5109	Avg Prec@1 82.10 %	Avg Prec@5 99.11 %

[EVAL Batch 000/079]	Time 0.338s (0.338s)	Loss 0.4412 (0.4412)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5605	Avg Prec@1 80.59 %	Avg Prec@5 98.83 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0661be40e32a68cc8be02e6aa0fd9e1cdc871d35.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 38 =====
=================

[TRAIN Batch 000/391]	Time 0.335s (0.335s)	Loss 0.6251 (0.6251)	Prec@1  79.7 ( 79.7)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/53676ed2843442fe6bda04b053a0346f31334d71.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.011s (0.108s)	Loss 0.5982 (0.5133)	Prec@1  75.8 ( 81.9)	Prec@5  98.4 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/66d510081c54c73013446f69c1fc5daade310805.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 43s	Avg loss 0.5109	Avg Prec@1 82.09 %	Avg Prec@5 99.18 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.4857 (0.4857)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5715	Avg Prec@1 80.07 %	Avg Prec@5 99.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ba919a65122b768fdc6ec1657ea887084257967c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 39 =====
=================

[TRAIN Batch 000/391]	Time 0.360s (0.360s)	Loss 0.6075 (0.6075)	Prec@1  76.6 ( 76.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/62a8010f2e505a33c1f8195e3818f86e86581bb3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.016s (0.110s)	Loss 0.5253 (0.5019)	Prec@1  80.5 ( 82.5)	Prec@5 100.0 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/40240871e05c059474b83cba90dbe970ca1d894d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5045	Avg Prec@1 82.40 %	Avg Prec@5 99.17 %

[EVAL Batch 000/079]	Time 0.217s (0.217s)	Loss 0.4528 (0.4528)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5703	Avg Prec@1 80.07 %	Avg Prec@5 98.94 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/90606ce362686dc9901a7d7e56dc4ff5067fa892.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 40 =====
=================

[TRAIN Batch 000/391]	Time 0.386s (0.386s)	Loss 0.3970 (0.3970)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/308d29da32b25d0a14c9954dac117c079255a425.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.189s (0.110s)	Loss 0.5461 (0.4994)	Prec@1  78.9 ( 82.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bb70621c05819d4728edfd558a88989dbd4cf7ec.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4992	Avg Prec@1 82.40 %	Avg Prec@5 99.19 %

[EVAL Batch 000/079]	Time 0.215s (0.215s)	Loss 0.4234 (0.4234)	Prec@1  87.5 ( 87.5)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5514	Avg Prec@1 80.78 %	Avg Prec@5 99.03 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/73bc876d11cdf3b8574131f4b3b778914d539b09.png" /></p>
</div>
</div>
<section id="35-use-of-batch-normalization" class="cell markdown"
id="T8VbwYP5yItH">
<h2>3.5 Use of batch normalization</h2>
</section>
<div class="cell code" data-execution_count="32" id="CEyR8J03yJmu">
<div class="sourceCode" id="cb967"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb967-1"><a href="#cb967-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet(nn.Module):</span>
<span id="cb967-2"><a href="#cb967-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb967-3"><a href="#cb967-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This class defines the structure of the neural network</span></span>
<span id="cb967-4"><a href="#cb967-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb967-5"><a href="#cb967-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb967-6"><a href="#cb967-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb967-7"><a href="#cb967-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb967-8"><a href="#cb967-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We first define the convolution and pooling layers as a features extractor</span></span>
<span id="cb967-9"><a href="#cb967-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb967-10"><a href="#cb967-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb967-11"><a href="#cb967-11" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">32</span>),</span>
<span id="cb967-12"><a href="#cb967-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb967-13"><a href="#cb967-13" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb967-14"><a href="#cb967-14" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb967-15"><a href="#cb967-15" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">64</span>),</span>
<span id="cb967-16"><a href="#cb967-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb967-17"><a href="#cb967-17" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb967-18"><a href="#cb967-18" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, (<span class="dv">5</span>, <span class="dv">5</span>), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb967-19"><a href="#cb967-19" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">64</span>),</span>
<span id="cb967-20"><a href="#cb967-20" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb967-21"><a href="#cb967-21" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>, ceil_mode<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb967-22"><a href="#cb967-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb967-23"><a href="#cb967-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We then define fully connected layers as a classifier</span></span>
<span id="cb967-24"><a href="#cb967-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb967-25"><a href="#cb967-25" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">1000</span>),</span>
<span id="cb967-26"><a href="#cb967-26" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb967-27"><a href="#cb967-27" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb967-28"><a href="#cb967-28" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1000</span>, <span class="dv">10</span>),</span>
<span id="cb967-29"><a href="#cb967-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reminder: The softmax is included in the loss, do not put it here</span></span>
<span id="cb967-30"><a href="#cb967-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb967-31"><a href="#cb967-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb967-32"><a href="#cb967-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Method called when we apply the network to an input batch</span></span>
<span id="cb967-33"><a href="#cb967-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb967-34"><a href="#cb967-34" aria-hidden="true" tabindex="-1"></a>        bsize <span class="op">=</span> <span class="bu">input</span>.size(<span class="dv">0</span>) <span class="co"># batch size</span></span>
<span id="cb967-35"><a href="#cb967-35" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.features(<span class="bu">input</span>) <span class="co"># output of the conv layers</span></span>
<span id="cb967-36"><a href="#cb967-36" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.view(bsize, <span class="op">-</span><span class="dv">1</span>) <span class="co"># we flatten the 2D feature maps into one 1D vector for each input</span></span>
<span id="cb967-37"><a href="#cb967-37" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.classifier(output) <span class="co"># we compute the output of the fc layers</span></span>
<span id="cb967-38"><a href="#cb967-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="33"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="y9w7BtuoyOe1" data-outputId="8f5665d7-d5d7-44d4-d0b0-2644ceca7f06">
<div class="sourceCode" id="cb968"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb968-1"><a href="#cb968-1" aria-hidden="true" tabindex="-1"></a>main(<span class="dv">128</span>, <span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">40</span>, cuda<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
=================
=== EPOCH 1 =====
=================

[TRAIN Batch 000/391]	Time 0.434s (0.434s)	Loss 2.3842 (2.3842)	Prec@1   7.8 (  7.8)	Prec@5  47.7 ( 47.7)
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dfbf98651987e1b0199d047729bb82d3239114c6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.022s (0.109s)	Loss 1.7324 (1.8042)	Prec@1  37.5 ( 33.0)	Prec@5  88.3 ( 85.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5620bd163e7920e0f6145f3439b9087a3fd4d789.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.6759	Avg Prec@1 37.91 %	Avg Prec@5 88.03 %

[EVAL Batch 000/079]	Time 0.196s (0.196s)	Loss 1.3057 (1.3057)	Prec@1  52.3 ( 52.3)	Prec@5  89.8 ( 89.8)

===============&gt; Total time 2s	Avg loss 1.3671	Avg Prec@1 48.79 %	Avg Prec@5 93.98 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/74471480f896f38ce6adfdcb528c960aec03d4e7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 2 =====
=================

[TRAIN Batch 000/391]	Time 0.351s (0.351s)	Loss 1.3628 (1.3628)	Prec@1  50.8 ( 50.8)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bf1251e40bd09f01b06dac28a4618c5446fc802f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.017s (0.107s)	Loss 1.5189 (1.3903)	Prec@1  45.3 ( 49.1)	Prec@5  89.1 ( 93.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e32998d728dacc1d39f85e9863cb9af5fe5b8e38.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 1.3391	Avg Prec@1 50.90 %	Avg Prec@5 93.84 %

[EVAL Batch 000/079]	Time 0.174s (0.174s)	Loss 1.4058 (1.4058)	Prec@1  47.7 ( 47.7)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 2s	Avg loss 1.3781	Avg Prec@1 49.60 %	Avg Prec@5 93.71 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6bb4652ad830aa342b8fa1b06d79e86c235049cb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 3 =====
=================

[TRAIN Batch 000/391]	Time 0.315s (0.315s)	Loss 1.2340 (1.2340)	Prec@1  51.6 ( 51.6)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/00cc4c9b2dddf0f0ada265870859f98abca4ca4b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.301s (0.105s)	Loss 1.1631 (1.1989)	Prec@1  63.3 ( 56.7)	Prec@5  96.1 ( 95.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7b24ab075692b84912ebd682e0f78a4515cfa099.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 1.1702	Avg Prec@1 57.85 %	Avg Prec@5 95.63 %

[EVAL Batch 000/079]	Time 0.177s (0.177s)	Loss 1.3645 (1.3645)	Prec@1  53.9 ( 53.9)	Prec@5  93.8 ( 93.8)

===============&gt; Total time 2s	Avg loss 1.3629	Avg Prec@1 53.16 %	Avg Prec@5 94.06 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0e1b8455626eabe92962c82160395ff4dd850302.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 4 =====
=================

[TRAIN Batch 000/391]	Time 0.338s (0.338s)	Loss 1.1151 (1.1151)	Prec@1  53.9 ( 53.9)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec771de939a3fba9ac43a99f46d2903ce69a9313.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.185s (0.108s)	Loss 1.1845 (1.0823)	Prec@1  57.0 ( 61.2)	Prec@5  96.9 ( 96.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fb01dad4391101193c378c7a17fb3e41acd613b2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 1.0624	Avg Prec@1 62.04 %	Avg Prec@5 96.43 %

[EVAL Batch 000/079]	Time 0.185s (0.185s)	Loss 0.9899 (0.9899)	Prec@1  64.8 ( 64.8)	Prec@5  96.1 ( 96.1)

===============&gt; Total time 2s	Avg loss 0.9711	Avg Prec@1 64.85 %	Avg Prec@5 96.80 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6ca711169a86c77f664d980878c31de2d1eb985e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 5 =====
=================

[TRAIN Batch 000/391]	Time 0.317s (0.317s)	Loss 1.0222 (1.0222)	Prec@1  60.9 ( 60.9)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2b397dfe9bae434184b656e9f9e0be34a8db4632.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.173s (0.108s)	Loss 0.8921 (0.9922)	Prec@1  70.3 ( 64.6)	Prec@5  97.7 ( 97.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a50477d9993982bab22e2b87f7e7ed79f1ee3f48.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.9806	Avg Prec@1 64.88 %	Avg Prec@5 97.04 %

[EVAL Batch 000/079]	Time 0.180s (0.180s)	Loss 0.8975 (0.8975)	Prec@1  67.2 ( 67.2)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.9452	Avg Prec@1 65.77 %	Avg Prec@5 97.51 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/86a533b6b06739ae49992a9162d2c828be24e75d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 6 =====
=================

[TRAIN Batch 000/391]	Time 0.370s (0.370s)	Loss 0.8955 (0.8955)	Prec@1  73.4 ( 73.4)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/233eee5f032045cc4b4de9d2e191c9e7bc23829b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.139s (0.103s)	Loss 0.8712 (0.9269)	Prec@1  70.3 ( 67.1)	Prec@5  96.1 ( 97.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a7c3581deaad3f6aa62f4622b463b4f5bb78fcbf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.9202	Avg Prec@1 67.37 %	Avg Prec@5 97.37 %

[EVAL Batch 000/079]	Time 0.195s (0.195s)	Loss 1.4382 (1.4382)	Prec@1  54.7 ( 54.7)	Prec@5  95.3 ( 95.3)

===============&gt; Total time 2s	Avg loss 1.5184	Avg Prec@1 51.98 %	Avg Prec@5 94.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0ccaea6107e550d3bc20cb9182e05ad008a53eef.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 7 =====
=================

[TRAIN Batch 000/391]	Time 0.332s (0.332s)	Loss 0.9008 (0.9008)	Prec@1  67.2 ( 67.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/5ff2523e011368d1f7665b0edbca0a3d867e5ac6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.192s (0.108s)	Loss 1.2083 (0.8903)	Prec@1  61.7 ( 68.7)	Prec@5  95.3 ( 97.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/be37bb1425e207043d113fdbb3e3e090c74a2c1f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.8731	Avg Prec@1 69.29 %	Avg Prec@5 97.62 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 0.9100 (0.9100)	Prec@1  72.7 ( 72.7)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.9159	Avg Prec@1 67.90 %	Avg Prec@5 97.77 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b10c7c46def0015884a2e22872a687f309cce3e6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 8 =====
=================

[TRAIN Batch 000/391]	Time 0.453s (0.453s)	Loss 0.7093 (0.7093)	Prec@1  75.8 ( 75.8)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e14d5eec4f7ae7695bf24095166082e071633c11.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.174s (0.101s)	Loss 0.7856 (0.8370)	Prec@1  68.0 ( 70.8)	Prec@5  98.4 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/741cc42fc1d5b8d0f101b57e56109f99a7ad9988.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.8295	Avg Prec@1 71.01 %	Avg Prec@5 97.71 %

[EVAL Batch 000/079]	Time 0.184s (0.184s)	Loss 0.7973 (0.7973)	Prec@1  77.3 ( 77.3)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.9029	Avg Prec@1 69.04 %	Avg Prec@5 97.58 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a4cae07d2037aeb271067ce1d137ca92c7322046.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 9 =====
=================

[TRAIN Batch 000/391]	Time 0.317s (0.317s)	Loss 0.7303 (0.7303)	Prec@1  74.2 ( 74.2)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/43174bbe1b2294e6e08f5a3f2b52b62ed248974f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.171s (0.106s)	Loss 0.7746 (0.8027)	Prec@1  73.4 ( 71.6)	Prec@5 100.0 ( 98.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aa795999059bb2bdc6ee310bf89689f4620bc0b7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.7980	Avg Prec@1 71.88 %	Avg Prec@5 98.03 %

[EVAL Batch 000/079]	Time 0.180s (0.180s)	Loss 0.8575 (0.8575)	Prec@1  66.4 ( 66.4)	Prec@5  96.9 ( 96.9)

===============&gt; Total time 2s	Avg loss 0.8913	Avg Prec@1 68.56 %	Avg Prec@5 97.41 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/69a059fead03d343c791d07ddd8b59021b86567f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 10 =====
=================

[TRAIN Batch 000/391]	Time 0.614s (0.614s)	Loss 0.6508 (0.6508)	Prec@1  76.6 ( 76.6)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/315f489f317edf8fa51e4dc2028c0e7697ccc9ea.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.048s (0.111s)	Loss 0.7857 (0.7716)	Prec@1  71.1 ( 72.8)	Prec@5  98.4 ( 98.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6af6c93e0857423973f990f9d53d0a68164cd00b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.7671	Avg Prec@1 72.96 %	Avg Prec@5 98.15 %

[EVAL Batch 000/079]	Time 0.181s (0.181s)	Loss 0.7953 (0.7953)	Prec@1  71.9 ( 71.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.8285	Avg Prec@1 71.02 %	Avg Prec@5 97.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/346043d57b24321c47989aa2e66c0c701910fb31.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 11 =====
=================

[TRAIN Batch 000/391]	Time 0.335s (0.335s)	Loss 0.8870 (0.8870)	Prec@1  69.5 ( 69.5)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e271beafc5e467a7a5fafdda6df95e7cf75d19d3.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.125s (0.106s)	Loss 0.8709 (0.7405)	Prec@1  69.5 ( 74.4)	Prec@5  96.9 ( 98.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2c423213f04342bc420ac8c75d57001226b57d66.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.7404	Avg Prec@1 74.29 %	Avg Prec@5 98.23 %

[EVAL Batch 000/079]	Time 0.176s (0.176s)	Loss 0.6987 (0.6987)	Prec@1  75.0 ( 75.0)	Prec@5 100.0 (100.0)

===============&gt; Total time 2s	Avg loss 0.7628	Avg Prec@1 73.59 %	Avg Prec@5 98.08 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/36fab67bb7c3091c983162cb506d5ed6db1b21dd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 12 =====
=================

[TRAIN Batch 000/391]	Time 0.465s (0.465s)	Loss 0.8333 (0.8333)	Prec@1  65.6 ( 65.6)	Prec@5  97.7 ( 97.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0907fdd1d5733822d92bf2cf134efb985cd88988.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.176s (0.106s)	Loss 0.6967 (0.7196)	Prec@1  76.6 ( 74.8)	Prec@5  99.2 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4dfd2f282248a5aecaa94f2b8143c9de04438e45.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.7180	Avg Prec@1 74.81 %	Avg Prec@5 98.37 %

[EVAL Batch 000/079]	Time 0.196s (0.196s)	Loss 0.7465 (0.7465)	Prec@1  73.4 ( 73.4)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.7663	Avg Prec@1 72.88 %	Avg Prec@5 98.31 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/4cb07648351b9a6e2e6cae799b03aa1e25c9fb6e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 13 =====
=================

[TRAIN Batch 000/391]	Time 0.352s (0.352s)	Loss 0.6065 (0.6065)	Prec@1  78.9 ( 78.9)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dc79f1a616116a8a3698528ad982d9a9c074ef3d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.108s)	Loss 0.8082 (0.6993)	Prec@1  75.8 ( 75.6)	Prec@5  97.7 ( 98.5)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/dd6afe3c5a5d12aa94e1779368ff93ba385d4c8b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6950	Avg Prec@1 75.68 %	Avg Prec@5 98.56 %

[EVAL Batch 000/079]	Time 0.201s (0.201s)	Loss 0.5811 (0.5811)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6773	Avg Prec@1 76.27 %	Avg Prec@5 98.41 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/94c339b28b2530fe6235a010a680e54c08870d4b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 14 =====
=================

[TRAIN Batch 000/391]	Time 0.571s (0.571s)	Loss 0.7074 (0.7074)	Prec@1  77.3 ( 77.3)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cd3b1eac31e2ebc16d51345e6094d259514f9c5a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.073s (0.104s)	Loss 0.5821 (0.6774)	Prec@1  80.5 ( 76.6)	Prec@5  97.7 ( 98.6)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d6eaddf144ee88fa0839efcb00ec6fee1c68d85e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6766	Avg Prec@1 76.41 %	Avg Prec@5 98.62 %

[EVAL Batch 000/079]	Time 0.187s (0.187s)	Loss 0.6516 (0.6516)	Prec@1  75.0 ( 75.0)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6602	Avg Prec@1 76.95 %	Avg Prec@5 98.55 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a994977a47d46d69e9be45ad8db5d051d6b2a93f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 15 =====
=================

[TRAIN Batch 000/391]	Time 0.344s (0.344s)	Loss 0.6754 (0.6754)	Prec@1  74.2 ( 74.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/98b1689a11784faeed2a5a9eac0c2c02121d7b28.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.109s)	Loss 0.6711 (0.6617)	Prec@1  75.0 ( 76.6)	Prec@5  97.7 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/214a1c9a325fe17e943c30fb3f8fd7d6a17d79fe.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.6646	Avg Prec@1 76.60 %	Avg Prec@5 98.65 %

[EVAL Batch 000/079]	Time 0.213s (0.213s)	Loss 0.6768 (0.6768)	Prec@1  77.3 ( 77.3)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.7048	Avg Prec@1 75.59 %	Avg Prec@5 98.24 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0f4cf15c13f62e8c3fc67651309c755832759571.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 16 =====
=================

[TRAIN Batch 000/391]	Time 0.336s (0.336s)	Loss 0.4886 (0.4886)	Prec@1  85.2 ( 85.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f4cc3f8cc58b7a9cd78701e03b3da2bda85eb75c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.101s)	Loss 0.7284 (0.6579)	Prec@1  78.9 ( 76.9)	Prec@5  97.7 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fccb4e41aef22afd630ea4e7ef154aa77c9e0993.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.6463	Avg Prec@1 77.39 %	Avg Prec@5 98.68 %

[EVAL Batch 000/079]	Time 0.194s (0.194s)	Loss 0.5938 (0.5938)	Prec@1  78.9 ( 78.9)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6616	Avg Prec@1 76.95 %	Avg Prec@5 98.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9f1cdf7b3248bc5ffe2bbdf7bb9175b507b1bebf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 17 =====
=================

[TRAIN Batch 000/391]	Time 0.355s (0.355s)	Loss 0.6256 (0.6256)	Prec@1  75.0 ( 75.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b3ee7ddf564eafa47c5fd849a9e64c87b9685e2f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.173s (0.107s)	Loss 0.6687 (0.6362)	Prec@1  78.1 ( 77.8)	Prec@5 100.0 ( 98.7)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b30c3d0677fae1af63d7c5c17f6b3daf8e9c4a02.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.6338	Avg Prec@1 77.89 %	Avg Prec@5 98.75 %

[EVAL Batch 000/079]	Time 0.325s (0.325s)	Loss 0.6784 (0.6784)	Prec@1  76.6 ( 76.6)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 3s	Avg loss 0.7270	Avg Prec@1 74.61 %	Avg Prec@5 98.49 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/414e31b97e6ceefa86cd4e3548cbafeac8511d01.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 18 =====
=================

[TRAIN Batch 000/391]	Time 0.337s (0.337s)	Loss 0.6292 (0.6292)	Prec@1  81.2 ( 81.2)	Prec@5  96.9 ( 96.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3e028c8e091c6be8ea54f0f5318c81538613f970.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.087s (0.102s)	Loss 0.7150 (0.6117)	Prec@1  72.7 ( 78.5)	Prec@5  98.4 ( 98.8)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1c5db30edd7177559905356d74dc10fd0b96a40b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.6173	Avg Prec@1 78.50 %	Avg Prec@5 98.80 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.5764 (0.5764)	Prec@1  78.1 ( 78.1)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.6162	Avg Prec@1 78.45 %	Avg Prec@5 98.73 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f5713aee6d0f5140fc9205674d9292bf63a8a01d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 19 =====
=================

[TRAIN Batch 000/391]	Time 0.367s (0.367s)	Loss 0.5641 (0.5641)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/47687042e38debe48421126eb3b8f1d417b29064.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.109s)	Loss 0.5657 (0.6083)	Prec@1  78.9 ( 78.7)	Prec@5  99.2 ( 98.9)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/bec49b9ade9446c13829047c16e7378f148fdee2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.6060	Avg Prec@1 78.73 %	Avg Prec@5 98.88 %

[EVAL Batch 000/079]	Time 0.186s (0.186s)	Loss 0.6636 (0.6636)	Prec@1  78.9 ( 78.9)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6672	Avg Prec@1 76.41 %	Avg Prec@5 98.66 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/99c2eaf14bae19e91e86e158254321305ff0d6af.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 20 =====
=================

[TRAIN Batch 000/391]	Time 0.366s (0.366s)	Loss 0.5764 (0.5764)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/2edecbea12b5ff9e159cad7e05b19712e34176f8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.018s (0.106s)	Loss 0.5682 (0.5878)	Prec@1  80.5 ( 79.2)	Prec@5  98.4 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f246e9b8102397c2a6b094b97aefc1d7ed6cb63e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.5962	Avg Prec@1 79.02 %	Avg Prec@5 98.95 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.5592 (0.5592)	Prec@1  81.2 ( 81.2)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.6297	Avg Prec@1 77.56 %	Avg Prec@5 98.72 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/49f2ed3b5ec3d35f811ed83045c7df24734bbc8b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 21 =====
=================

[TRAIN Batch 000/391]	Time 0.554s (0.554s)	Loss 0.5579 (0.5579)	Prec@1  80.5 ( 80.5)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8aa76790702cbecd230c4d2a563b0448448d37e1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.023s (0.109s)	Loss 0.5768 (0.5797)	Prec@1  83.6 ( 79.6)	Prec@5  97.7 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/459f8ecf03004febac33f001b71b597fff82f492.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5884	Avg Prec@1 79.37 %	Avg Prec@5 98.99 %

[EVAL Batch 000/079]	Time 0.191s (0.191s)	Loss 0.6760 (0.6760)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.6404	Avg Prec@1 77.81 %	Avg Prec@5 98.57 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/e9d5085d77d41c87b79edd4dccbcd44db75ed362.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 22 =====
=================

[TRAIN Batch 000/391]	Time 0.403s (0.403s)	Loss 0.5788 (0.5788)	Prec@1  76.6 ( 76.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9163ede9cec80d3d1675efcbda824f43bf80acfa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.042s (0.109s)	Loss 0.4975 (0.5721)	Prec@1  85.9 ( 80.0)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/87e3ac6ae394144e9b9102bf6b51ac39a7d8877d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5769	Avg Prec@1 79.84 %	Avg Prec@5 98.99 %

[EVAL Batch 000/079]	Time 0.205s (0.205s)	Loss 0.5931 (0.5931)	Prec@1  78.9 ( 78.9)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 3s	Avg loss 0.6377	Avg Prec@1 78.24 %	Avg Prec@5 98.64 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/9f423747f974d2e6b3b589ed68953d1407c4e768.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 23 =====
=================

[TRAIN Batch 000/391]	Time 0.510s (0.510s)	Loss 0.7434 (0.7434)	Prec@1  74.2 ( 74.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/fa3b85a55ef33d6e6ef180b9afaf8a84a7863724.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.105s)	Loss 0.3976 (0.5670)	Prec@1  85.2 ( 80.1)	Prec@5 100.0 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/cd814128d1edc80672c764a3a03f75894ae06edd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5690	Avg Prec@1 80.15 %	Avg Prec@5 98.97 %

[EVAL Batch 000/079]	Time 0.197s (0.197s)	Loss 0.5548 (0.5548)	Prec@1  82.0 ( 82.0)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.6075	Avg Prec@1 78.84 %	Avg Prec@5 98.72 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ad264bb2874fd0e907061923bedb15289e32db4a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 24 =====
=================

[TRAIN Batch 000/391]	Time 0.379s (0.379s)	Loss 0.5638 (0.5638)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/765f0d62ee5f708b85de4c27dcd20926a42caba7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.180s (0.110s)	Loss 0.5433 (0.5691)	Prec@1  79.7 ( 80.0)	Prec@5  99.2 ( 99.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1ac54211cdbabbe2f39da4729bb8198e6995959a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5642	Avg Prec@1 80.27 %	Avg Prec@5 99.00 %

[EVAL Batch 000/079]	Time 0.217s (0.217s)	Loss 0.6278 (0.6278)	Prec@1  77.3 ( 77.3)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 4s	Avg loss 0.6130	Avg Prec@1 78.67 %	Avg Prec@5 98.83 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d75a73e6204f282aeafa2480109c028efe39f582.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 25 =====
=================

[TRAIN Batch 000/391]	Time 0.372s (0.372s)	Loss 0.5002 (0.5002)	Prec@1  82.8 ( 82.8)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b9d7206a8216dc29f4984a701da8182cbb17aed5.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.164s (0.104s)	Loss 0.6132 (0.5555)	Prec@1  79.7 ( 80.7)	Prec@5 100.0 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/75dfd4b5ee442cb3a1252da565a343af57f9788d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5577	Avg Prec@1 80.49 %	Avg Prec@5 99.09 %

[EVAL Batch 000/079]	Time 0.206s (0.206s)	Loss 0.5627 (0.5627)	Prec@1  78.1 ( 78.1)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5893	Avg Prec@1 79.23 %	Avg Prec@5 98.72 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8e2ebaeced7ec31132c7f5abb82041687d0b4d5d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 26 =====
=================

[TRAIN Batch 000/391]	Time 0.366s (0.366s)	Loss 0.5375 (0.5375)	Prec@1  85.2 ( 85.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d07721a0bdec107b52c3daf9f6dcf5cb125bb841.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.178s (0.108s)	Loss 0.5378 (0.5521)	Prec@1  84.4 ( 80.9)	Prec@5  97.7 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/385732e25280815d7a8c32756d1e20e542f656be.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5508	Avg Prec@1 80.91 %	Avg Prec@5 99.05 %

[EVAL Batch 000/079]	Time 0.231s (0.231s)	Loss 0.5182 (0.5182)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5594	Avg Prec@1 80.58 %	Avg Prec@5 98.91 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8b1a8e273c6cb47832468bb5639a08e0c952633f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 27 =====
=================

[TRAIN Batch 000/391]	Time 0.332s (0.332s)	Loss 0.3803 (0.3803)	Prec@1  88.3 ( 88.3)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/934ec7372ebdaa2206c735f05be15ace6f67af7a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.225s (0.106s)	Loss 0.5301 (0.5379)	Prec@1  79.7 ( 81.3)	Prec@5  99.2 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c59fead7bcf565a5a03b198912cfd253781ba91c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5426	Avg Prec@1 81.07 %	Avg Prec@5 99.09 %

[EVAL Batch 000/079]	Time 0.209s (0.209s)	Loss 0.5235 (0.5235)	Prec@1  83.6 ( 83.6)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.5661	Avg Prec@1 80.13 %	Avg Prec@5 98.87 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c5cbde19a0a81cf77c88d8fd577a192370d98184.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 28 =====
=================

[TRAIN Batch 000/391]	Time 0.385s (0.385s)	Loss 0.7245 (0.7245)	Prec@1  74.2 ( 74.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a8df9849c967bdf0d1a2674c846d5dc6e587cb0e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.172s (0.110s)	Loss 0.4922 (0.5327)	Prec@1  80.5 ( 81.3)	Prec@5 100.0 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/651c1b52b1176085d966e21072c5c8bc1ba6f07c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5302	Avg Prec@1 81.40 %	Avg Prec@5 99.13 %

[EVAL Batch 000/079]	Time 0.217s (0.217s)	Loss 0.5667 (0.5667)	Prec@1  80.5 ( 80.5)	Prec@5  97.7 ( 97.7)

===============&gt; Total time 2s	Avg loss 0.5578	Avg Prec@1 80.31 %	Avg Prec@5 98.96 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/55651fe42d254615f7306aa19245d27bed251aaf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 29 =====
=================

[TRAIN Batch 000/391]	Time 0.351s (0.351s)	Loss 0.4521 (0.4521)	Prec@1  86.7 ( 86.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f247177b1cf1711dcdc820d5b0df019135346691.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.218s (0.107s)	Loss 0.6789 (0.5218)	Prec@1  77.3 ( 81.8)	Prec@5 100.0 ( 99.1)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/a49fe9a5e0cf9f7e26fd987d6d3db069b162c75c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.5219	Avg Prec@1 81.80 %	Avg Prec@5 99.13 %

[EVAL Batch 000/079]	Time 0.198s (0.198s)	Loss 0.5297 (0.5297)	Prec@1  79.7 ( 79.7)	Prec@5  98.4 ( 98.4)

===============&gt; Total time 2s	Avg loss 0.5483	Avg Prec@1 80.43 %	Avg Prec@5 99.01 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/7bc6612288d09ac833985a070b73abcefb59a3a0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 30 =====
=================

[TRAIN Batch 000/391]	Time 0.565s (0.565s)	Loss 0.6187 (0.6187)	Prec@1  76.6 ( 76.6)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1f5ded993abd7e28f22a9ee578ae77c5619f1e68.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.158s (0.109s)	Loss 0.4529 (0.5195)	Prec@1  85.9 ( 81.6)	Prec@5  98.4 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ee21e3bb7bad66ff8e1a9d3d5831ede60182a040.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5212	Avg Prec@1 81.67 %	Avg Prec@5 99.13 %

[EVAL Batch 000/079]	Time 0.204s (0.204s)	Loss 0.5069 (0.5069)	Prec@1  83.6 ( 83.6)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5605	Avg Prec@1 80.41 %	Avg Prec@5 98.99 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/66a5151343b4fc220d4cbfa65d70aa73aabb585e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 31 =====
=================

[TRAIN Batch 000/391]	Time 0.356s (0.356s)	Loss 0.6139 (0.6139)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6a5cc38f5ae385ad538e37ca49af9327942aa83b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.115s)	Loss 0.5413 (0.5189)	Prec@1  85.9 ( 82.1)	Prec@5  96.9 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d3a0c2534d2b15d6f0cce5802b5fb804fce3728a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5160	Avg Prec@1 82.07 %	Avg Prec@5 99.18 %

[EVAL Batch 000/079]	Time 0.206s (0.206s)	Loss 0.4752 (0.4752)	Prec@1  85.2 ( 85.2)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5556	Avg Prec@1 80.44 %	Avg Prec@5 98.91 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/58a75b2890eb9a935db856c1f5839d808c4e458b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 32 =====
=================

[TRAIN Batch 000/391]	Time 0.457s (0.457s)	Loss 0.5110 (0.5110)	Prec@1  82.0 ( 82.0)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3e38e451063d19f97d21aae3846de9dac1000245.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.110s (0.105s)	Loss 0.5470 (0.5032)	Prec@1  81.2 ( 82.3)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8f7671aa28bd314de242132c05ff89fdc962addf.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.5084	Avg Prec@1 82.02 %	Avg Prec@5 99.18 %

[EVAL Batch 000/079]	Time 0.202s (0.202s)	Loss 0.4879 (0.4879)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5442	Avg Prec@1 81.19 %	Avg Prec@5 98.96 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/3a7f3d5f4554248798273c8897c63abaa4649a29.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 33 =====
=================

[TRAIN Batch 000/391]	Time 0.427s (0.427s)	Loss 0.3966 (0.3966)	Prec@1  86.7 ( 86.7)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/0c2099ff2d37c516d91e4b4b6cfdee66857f761c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.151s (0.110s)	Loss 0.5952 (0.5055)	Prec@1  78.1 ( 82.3)	Prec@5 100.0 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/b33ca1fa40d45cf27cd8bb27933ef58e05dd0e71.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.5047	Avg Prec@1 82.31 %	Avg Prec@5 99.18 %

[EVAL Batch 000/079]	Time 0.257s (0.257s)	Loss 0.4644 (0.4644)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5317	Avg Prec@1 81.63 %	Avg Prec@5 99.06 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/61fe7bf0fb766cb3e8b55a2b640a656d4bfae907.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 34 =====
=================

[TRAIN Batch 000/391]	Time 0.395s (0.395s)	Loss 0.4529 (0.4529)	Prec@1  85.2 ( 85.2)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/00d83e1c887825ce3772805b06a69a1bcae3e587.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.217s (0.109s)	Loss 0.3782 (0.4943)	Prec@1  85.2 ( 82.6)	Prec@5 100.0 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/035d9de524eb12fc9fdba4b9507bb79abbcecf70.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4980	Avg Prec@1 82.44 %	Avg Prec@5 99.22 %

[EVAL Batch 000/079]	Time 0.217s (0.217s)	Loss 0.5245 (0.5245)	Prec@1  83.6 ( 83.6)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5394	Avg Prec@1 81.23 %	Avg Prec@5 98.98 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/61c53b016abe32073305e935dcc9eeef01ffebfc.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 35 =====
=================

[TRAIN Batch 000/391]	Time 0.582s (0.582s)	Loss 0.4579 (0.4579)	Prec@1  82.0 ( 82.0)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1c0f2e6d4843c82a7d6f25d430dcaf052de86f85.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.107s)	Loss 0.5468 (0.4989)	Prec@1  78.9 ( 82.6)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/be57c0e66599372562a18e39ba40de3650e083df.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 42s	Avg loss 0.4938	Avg Prec@1 82.72 %	Avg Prec@5 99.27 %

[EVAL Batch 000/079]	Time 0.218s (0.218s)	Loss 0.4696 (0.4696)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5311	Avg Prec@1 81.44 %	Avg Prec@5 99.03 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/c26371e976736f0eaa8826bfda6db308deb45282.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 36 =====
=================

[TRAIN Batch 000/391]	Time 0.479s (0.479s)	Loss 0.5355 (0.5355)	Prec@1  80.5 ( 80.5)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/be7a22275fe5e6282cbc1234486f9c07e4c6af95.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.180s (0.109s)	Loss 0.4731 (0.4859)	Prec@1  81.2 ( 82.9)	Prec@5  99.2 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/1306650996ec0d41c52561da55cee82478f418c2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4875	Avg Prec@1 82.90 %	Avg Prec@5 99.31 %

[EVAL Batch 000/079]	Time 0.222s (0.222s)	Loss 0.4798 (0.4798)	Prec@1  87.5 ( 87.5)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5399	Avg Prec@1 81.52 %	Avg Prec@5 98.96 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d3fa12fc4b65a3f9abb8779b5cc528f7e4d3c1df.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 37 =====
=================

[TRAIN Batch 000/391]	Time 0.478s (0.478s)	Loss 0.5317 (0.5317)	Prec@1  81.2 ( 81.2)	Prec@5  98.4 ( 98.4)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/8a032226c10ddaaacee3e3d106fce9014e33ba65.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.014s (0.104s)	Loss 0.6426 (0.4902)	Prec@1  77.3 ( 82.9)	Prec@5  98.4 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ce82c8c124c21849b7e5c327287aa86b6dc1b659.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4876	Avg Prec@1 83.08 %	Avg Prec@5 99.28 %

[EVAL Batch 000/079]	Time 0.232s (0.232s)	Loss 0.4658 (0.4658)	Prec@1  82.8 ( 82.8)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5581	Avg Prec@1 81.16 %	Avg Prec@5 98.93 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/ec48b90ddacef00c56e4a0a73e88c3bfaa857893.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 38 =====
=================

[TRAIN Batch 000/391]	Time 0.490s (0.490s)	Loss 0.4798 (0.4798)	Prec@1  78.9 ( 78.9)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/29585b302402dcaa2b16d340c343a6672885aabd.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.010s (0.107s)	Loss 0.4862 (0.4802)	Prec@1  85.9 ( 83.0)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f1bef5d9513e3829a17fb62468f36324a4a7d7b9.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.4828	Avg Prec@1 82.95 %	Avg Prec@5 99.27 %

[EVAL Batch 000/079]	Time 0.223s (0.223s)	Loss 0.4300 (0.4300)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5201	Avg Prec@1 81.92 %	Avg Prec@5 99.07 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/07da42b7898274bca479caba3038bf27b47a0425.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 39 =====
=================

[TRAIN Batch 000/391]	Time 0.389s (0.389s)	Loss 0.4639 (0.4639)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/aa2990e21233d766a3f2d6c71b8dec2076f3cf04.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.166s (0.103s)	Loss 0.5169 (0.4785)	Prec@1  82.0 ( 83.4)	Prec@5 100.0 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/6120a1cc05c16a3eff479a1ae78943512315d082.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 41s	Avg loss 0.4777	Avg Prec@1 83.45 %	Avg Prec@5 99.30 %

[EVAL Batch 000/079]	Time 0.223s (0.223s)	Loss 0.4636 (0.4636)	Prec@1  84.4 ( 84.4)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 2s	Avg loss 0.5254	Avg Prec@1 81.62 %	Avg Prec@5 99.07 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d8d6783fddaea19af873ebdc724f6783764e9f40.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>=================
=== EPOCH 40 =====
=================

[TRAIN Batch 000/391]	Time 0.435s (0.435s)	Loss 0.5313 (0.5313)	Prec@1  84.4 ( 84.4)	Prec@5 100.0 (100.0)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/556e66a117fcfbb025de0d792f22b0b229407245.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>[TRAIN Batch 200/391]	Time 0.160s (0.107s)	Loss 0.4072 (0.4713)	Prec@1  88.3 ( 83.4)	Prec@5  98.4 ( 99.3)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/f306b89803feb915ef73ecd52b49fbd51f88ccc8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
===============&gt; Total time 40s	Avg loss 0.4758	Avg Prec@1 83.37 %	Avg Prec@5 99.29 %

[EVAL Batch 000/079]	Time 0.267s (0.267s)	Loss 0.4467 (0.4467)	Prec@1  86.7 ( 86.7)	Prec@5  99.2 ( 99.2)

===============&gt; Total time 3s	Avg loss 0.5144	Avg Prec@1 82.46 %	Avg Prec@5 99.08 %

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0dc98ae5cc0b47c0a64d95d09a65906a/d38930449b650e2ddf2015018db73e27018bccbf.png" /></p>
</div>
</div>
<div class="cell markdown" id="1v0jfhn4e1ah">
<p>resume:</p>
<p>Baseline:</p>
<p>===============&gt; Total time 9s Avg loss 0.0001 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 100.00 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 100.00 %</p>
<p>===============&gt; Total time 1s Avg loss 2.2823 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 72.81 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 97.55 %</p>
<p>standardization of examples:</p>
<ul>
<li><p>Normalzation :</p>
<p>===============&gt; Total time 14s Avg loss 0.0004 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 100.00 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 100.00 %</p>
<p>===============&gt; Total time 3s Avg loss 1.7286 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 76.97 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 98.14 %</p></li>
<li><p>ZCA normalization :</p>
<p>===============&gt; Total time 81s Avg loss 0.0006 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 100.00 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 100.00 %</p>
<p>===============&gt; Total time 16s Avg loss 1.5942 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 77.97 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 98.53 %</p></li>
</ul>
<p>data augmentation:</p>
<p>===============&gt; Total time 18s Avg loss 0.2208 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 92.31 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 99.92 %</p>
<p>===============&gt; Total time 2s Avg loss 0.6978 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 80.80 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 98.56 %</p>
<p>optimizer:</p>
<p>===============&gt; Total time 20s Avg loss 0.2225 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 92.28 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 99.86 %</p>
<p>===============&gt; Total time 2s Avg loss 0.6119 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 82.09 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 98.95 %</p>
<p>dropout:</p>
<p>===============&gt; Total time 20s Avg loss 0.3253 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 88.56 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 99.67 %</p>
<p>===============&gt; Total time 2s Avg loss 0.5146 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 83.26 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 99.15 %</p>
<p>batch normalization:</p>
<p>===============&gt; Total time 19s Avg loss 0.3081 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 89.34 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 99.74 %</p>
<p>===============&gt; Total time 2s Avg loss 0.4519 Avg <a
href="mailto:Prec@1" class="email">Prec@1</a> 84.45 % Avg <a
href="mailto:Prec@5" class="email">Prec@5</a> 99.32 %</p>
</div>
</body>
</html>
